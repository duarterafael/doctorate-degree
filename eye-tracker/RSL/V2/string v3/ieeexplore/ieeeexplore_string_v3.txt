@INPROCEEDINGS{7965634,
author={A. {De Abreu} and C. {Ozcinar} and A. {Smolic}},
booktitle={2017 Ninth International Conference on Quality of Multimedia Experience (QoMEX)},
title={Look around you: Saliency maps for omnidirectional images in VR applications},
year={2017},
volume={},
number={},
pages={1-6},
abstract={Understanding visual attention has always been a topic of great interest in the graphics, image/video processing, robotics and human-computer interaction communities. By understanding salient image regions, the compression, transmission and rendering algorithms can be optimized. This is particularly important in omnidirectional images (ODIs) viewed with a head-mounted display (HMD), where only a fraction of the captured scene is displayed at a time, namely viewport. In order to predict salient image regions, saliency maps are estimated either by using an eye tracker to collect eye fixations during subjective tests or by using computational models of visual attention. However, eye tracking developments for ODIs are still in the early stages and although a large list of saliency models are available, no particular attention has been dedicated to ODIs. Therefore, in this paper, we consider the problem of estimating saliency maps for ODIs viewed with HMDs, when the use of an eye tracker device is not possible. We collected viewport center trajectories (VCTs) of 32 participants for 21 ODIs and propose a method to transform the gathered data into saliency maps. The obtained saliency maps are compared in terms of image exposition time used to display each ODI in the subjective tests. Then, motivated by the equator bias tendency in ODIs, we propose a post-processing method, namely fused saliency maps (FSM), to adapt current saliency models to ODIs requirements. We show that the use of FSM on current models improves their performance by up to 20%. The developed database and testbed are publicly available with this paper.},
keywords={data compression;helmet mounted displays;image coding;image representation;rendering (computer graphics);virtual reality;omnidirectional images;VR applications;graphics;image processing;video processing;robotics;human-computer interaction;salient image regions;compression algorithms;transmission algorithms;rendering algorithms;head-mounted display;HMD;eye tracker;eye fixations;viewport center trajectories;VCTs;image exposition time;fused saliency maps;FSM;ODI representation;Visualization;Computational modeling;Adaptation models;Resists;Three-dimensional displays;Solid modeling;Trajectory;Fixations;head-mounted display (HMD);omnidirectional images (ODIs);saliency maps;viewport;virtual reality (VR)},
doi={10.1109/QoMEX.2017.7965634},
ISSN={2472-7814},
month={May},}
@ARTICLE{8399735,
author={L. {Lévêque} and H. {Bosmans} and L. {Cockmartin} and H. {Liu}},
journal={IEEE Access},
title={State of the Art: Eye-Tracking Studies in Medical Imaging},
year={2018},
volume={6},
number={},
pages={37023-37034},
abstract={Eye-tracking-the process of measuring where people look in a visual field-has been widely used to study how humans process visual information. In medical imaging, eye-tracking has become a popular technique in many applications to reveal how visual search and recognition tasks are performed, providing information that can improve human performance. In this paper, we present a comprehensive review of eye-tracking studies conducted with medical images and videos for diverse research purposes, including the identification of the degree of expertise, development of training, and understanding and modeling of visual search patterns. In addition, we present our recent eye-tracking study that involves a large number of screening mammograms viewed by experienced breast radiologists. Based on the eye-tracking data, we evaluate the plausibility of predicting visual attention by computational models.},
keywords={mammography;medical image processing;humans process visual information;recognition tasks;human performance;medical images;visual search patterns;eye-tracking data;medical videos;mammograms;breast radiologists;Visualization;Computed tomography;Mammography;Radiology;Task analysis;Medical diagnostic imaging;Medical imaging;visual attention;image quality;eye-tracking;saliency},
doi={10.1109/ACCESS.2018.2851451},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{8449636,
author={N. {Peitek}},
booktitle={2018 IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion)},
title={A Neuro-Cognitive Perspective of Program Comprehension},
year={2018},
volume={},
number={},
pages={496-499},
abstract={Program comprehension is the cognitive process of understanding code. Researchers have proposed several models to describe program comprehension. However, because program comprehension is an internal process and difficult to measure, the accuracy of the existing models are limited. Neuro-imaging methods, such as functional magnetic resonance imaging (fMRI), provide a novel neuro-cognitive perspective to program-comprehension research. With my thesis work, we aim at establishing fMRI as a new tool for program-comprehension and software-engineering studies. Furthermore, we seek to refine our existing framework for conducting fMRI studies by extending it with eye tracking and improved control conditions. We describe how we will apply our upgraded framework to extend our understanding of program comprehension. In the long-run, we would like to contribute insights from our fMRI studies into software-engineering practices by providing code-styling guidelines and programming tools, which reduce the required cognitive effort to comprehend code.},
keywords={biomedical MRI;cognition;neurophysiology;software engineering;neuro-cognitive perspective;functional magnetic resonance imaging;fMRI;eye tracking;software-engineering practices;programming tools;code-styling guidelines;program-comprehension research;Functional magnetic resonance imaging;Brain;Gaze tracking;Cognition;Task analysis;Tools;Electroencephalography;program comprehension;top down comprehension;functional magnetic resonance imaging;eye tracking},
doi={},
ISSN={2574-1934},
month={May},}
@INPROCEEDINGS{6912281,
author={R. {Laue} and F. {Hogrebe} and B. {Böttcher} and M. {Nüttgens}},
booktitle={2014 IEEE 22nd International Requirements Engineering Conference (RE)},
title={Efficient visual notations for efficient stakeholder communication},
year={2014},
volume={},
number={},
pages={329-330},
abstract={The visual syntax of modelling languages can support (or impede) the intuitive understandability of a model. We observed the process of problem solving with two notation variants of i* diagrams by means of an eye-tracking device. The number of wrongly answered questions was significantly lower when the alternative i* notation suggested by Moody et al. was used. For the eye-tracking metrics “time to solve a task” and “number of eye fixations”, no such significant result can be given. Furthermore, we identified a deficiency for the “dependency” symbol in the alternative notation.},
keywords={formal specification;object tracking;social aspects of automation;visual languages;visual notations;stakeholder communication;visual syntax;modelling languages;intuitive understandability;eye-tracking device;wrongly answered questions;eye-tracking metrics;eye fixations;dependency symbol;Visualization;Adaptation models;Computational modeling;Educational institutions;Information science;Syntactics;Electronic mail},
doi={10.1109/RE.2014.6912281},
ISSN={2332-6441},
month={Aug},}
@ARTICLE{6525431,
author={C. {Yin} and F. {Kuo}},
journal={IEEE Transactions on Professional Communication},
title={A Study of How Information System Professionals Comprehend Indirect and Direct Speech Acts in Project Communication},
year={2013},
volume={56},
number={3},
pages={226-241},
abstract={Research problem: Indirect communication is prevalent in business communication practices. For information systems (IS) projects that require professionals from multiple disciplines to work together, the use of indirect communication may hinder successful design, implementation, and maintenance of these systems. Drawing on the Speech Act Theory (SAT), this study investigates how direct and indirect speech acts may influence language comprehension in the setting of communication problems inherent in IS projects. Research questions: (1) Do participating subjects, who are IS professionals, differ in their comprehension of indirect and direct speech acts? (2) Do participants display different attention processes in their comprehension of indirect and direct speech acts? (3) Do participants' attention processes influence their comprehension of indirect and direct speech acts? Literature review: We review two relevant areas of theory-polite speech acts in professional communication and SAT. First, a broad review that focuses on literature related to the use of polite speech acts in the workplace and in information system (IS) projects suggests the importance of investigating speech acts by professionals. In addition, the SAT provides the theoretical framework guiding this study and the development of hypotheses. Methodology: The current study uses a quantitative approach. A between-groups experiment design was employed to test how direct and indirect speech acts influence the language comprehension of participants. Forty-three IS professionals participated in the experiment. In addition, through the use of eye-tracking technology, this study captured the attention process and analyzed the relationship between attention and comprehension. Results and discussion: The results show that the directness of speech acts significantly influences participants' attention process, which, in turn, significantly affects their comprehension. In addition, the findings indicate that indirect speech acts, if employed by IS professionals to communicate with others, may easily be distorted or misunderstood. Professionals and managers of organizations should be aware that effective communication in interdisciplinary projects, such as IS development, is not easy, and that reliance on polite or indirect communication may inhibit the generation of valid information.},
keywords={business communication;information systems;linguistics;organisational aspects;professional communication;attention-comprehension relationship analysis;eye-tracking technology;quantitative approach;information system projects;workplace;SAT;professional communication;polite speech acts;attention processes;IS professionals;IS projects;language comprehension;speech act theory;business communication;indirect communication;project communication;direct speech act comprehension;indirect speech act comprehension;information system professionals;Information systems;Speech processing;Indirect speech;Direct speech;Project management;Communication;direct/indirect speech act;eye movement;information system (IS) project;Speech Act Theory (SAT)},
doi={10.1109/TPC.2013.2263648},
ISSN={1558-1500},
month={Sep.},}
@INPROCEEDINGS{8449594,
author={H. {Störrle} and N. {Baltsen} and H. {Christoffersen} and A. M. {Maier}},
booktitle={2018 IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion)},
title={Poster: How Do Modelers Read UML Diagrams? Preliminary Results from an Eye-Tracking Study},
year={2018},
volume={},
number={},
pages={396-397},
abstract={Background: Conceptual diagrams are widely used. Previous research suggested layout quality, diagram size, and expertise level are relevant impact factors on understanding, while diagram type is not. Surprisingly little is known about how diagrams are read. Objective: Eventually, we want to understand the cognitive processes of diagram and model understanding. In this paper, we study the behavior of modelers while reading UML diagrams in terms of reading strategies and how they affect diagram understanding. Method: We conduct an eye tracking study with 28 participants, reusing diagrams and items from previous experiments. We record several objective and subjective performance indicators as well as eye movement and pupil dilation. Results: We discover behavioral regularities and aggregate them into reading strategies which vary with expertise level and diagram type, but not with layout quality. Conclusions: Modelers exhibit specific strategies of diagram understanding. Experts employ different strategies than novices, which explains performance differences irrespective of layout quality.},
keywords={cognition;eye;Unified Modeling Language;behavioral regularity;cognitive process;eye-tracking;pupil dilation;eye movement;diagram understanding;model understanding;expertise level;diagram size;Conceptual diagrams;UML diagrams;modelers;layout quality;Unified modeling language;Layout;Gaze tracking;Visualization;Software engineering;Software;Trajectory},
doi={},
ISSN={2574-1934},
month={May},}
@INPROCEEDINGS{4761352,
author={M. {Milanova} and S. {Rubin} and R. {Kountchev} and V. {Todorov} and R. {Kountcheva}},
booktitle={2008 19th International Conference on Pattern Recognition},
title={Combined visual attention model for video sequences},
year={2008},
volume={},
number={},
pages={1-4},
abstract={The paper presents a model of visual attention combined with eye tracking to drive content-based retrieval of image data in order to facilitate understanding and development of new adaptive eye guided representation of image sequences. The bottom-up component of the proposed visual attention model is based on the extended Itti-Koch saliency model incorporating conjunction search and temporal aspects of sequences of natural images. The top-down component is a gaze-prediction model designed to associate measured eye tracking locations and features extracted from images. This approach permits the detection and separation of attention-driven regions of interest and their processing with the highest accuracy, while the remaining part of the image (the background) is reproduced with lower quality.},
keywords={feature extraction;image retrieval;image sequences;visual attention model;video sequences;eye tracking;image data retrieval;feature extraction;extended Itti-Koch saliency model;gaze-prediction model;image sequences;Video sequences;Feature extraction;Information retrieval;Image retrieval;Content based retrieval;Image sequences;Image coding;Computer science;Nearest neighbor searches;Context modeling},
doi={10.1109/ICPR.2008.4761352},
ISSN={1051-4651},
month={Dec},}
@INPROCEEDINGS{7765561,
author={C. {Gralha}},
booktitle={2016 IEEE 24th International Requirements Engineering Conference (RE)},
title={Evaluation of Requirements Models},
year={2016},
volume={},
number={},
pages={432-437},
abstract={Requirements Engineering (RE) approaches, following paradigms such as goal-oriented [1] or scenario-based [2], provide expressive model elements for requirements elicitation and analysis. However, these approaches are still struggling when it comes to managing the quality of their models. Problems in quality can cause difficulties in anaging and understanding requirements, which in turn leads to increased development costs. The models' quality should then be a permanent concern. We propose a quantitative assessment of the goal-oriented and scenario-based models' quality, namely its complexity, completeness, appropriateness recognizability, understandability and learnability. To this end, we propose a combination of techniques to be applied to the RE models, the modelling process, and the models' notation. We are going to define metrics about the models, through the Goal-Question-Metric (GQM) approach, and incorporate them in a common evaluation framework that helps in the requirements modelling process. The quality of the RE models, the modelling process and the model's notation will be measured by collecting biometric data from stakeholders, by using eye-tracking devices, electroencephalography (EEG) scanners, and electro-dermal activity (EDA) scanners. Furthermore, we will collect metrics about the model during the modelling process, and the subjective opinion of stakeholders about the usage of these models, through questionnaires like NASA TLX [4] (which measures perceived effort while working on tasks). All metrics and biometrics are going to be theoretically and experimentally evaluated, through a set of case studies and experiments with different types of participants (including researchers, practitioners and students).},
keywords={biometrics (access control);formal specification;formal verification;requirements models evaluation;requirements engineering;models notation;goal-question-metric approach;GQM approach;requirements modelling process;eye-tracking devices;electroencephalography;EEG scanners;electro-dermal activity;EDA scanners;biometrics;Biological system modeling;Brain models;Unified modeling language;Analytical models;Measurement;Stakeholders;requirements models;quality evaluation;metrics;biometrics},
doi={10.1109/RE.2016.28},
ISSN={2332-6441},
month={Sep.},}
@ARTICLE{8876860,
author={I. {Martinikorena} and A. {Larumbe-Bergera} and M. {Ariz} and S. {Porta} and R. {Cabeza} and A. {Villanueva}},
journal={IEEE Transactions on Image Processing},
title={Low Cost Gaze Estimation: Knowledge-Based Solutions},
year={2020},
volume={29},
number={},
pages={2328-2343},
abstract={Eye tracking technology in low resolution scenarios is not a completely solved issue to date. The possibility of using eye tracking in a mobile gadget is a challenging objective that would permit to spread this technology to non-explored fields. In this paper, a knowledge based approach is presented to solve gaze estimation in low resolution settings. The understanding of the high resolution paradigm permits to propose alternative models to solve gaze estimation. In this manner, three models are presented: a geometrical model, an interpolation model and a compound model, as solutions for gaze estimation for remote low resolution systems. Since this work considers head position essential to improve gaze accuracy, a method for head pose estimation is also proposed. The methods are validated in an optimal framework, I2Head database, which combines head and gaze data. The experimental validation of the models demonstrates their sensitivity to image processing inaccuracies, critical in the case of the geometrical model. Static and extreme movement scenarios are analyzed showing the higher robustness of compound and geometrical models in the presence of user’s displacement. Accuracy values of about 3° have been obtained, increasing to values close to 5° in extreme displacement settings, results fully comparable with the state-of-the-art.},
keywords={Estimation;Head;Image resolution;Gaze tracking;Databases;Cameras;Gaze estimation methods;low resolution;eye tracking},
doi={10.1109/TIP.2019.2946452},
ISSN={1941-0042},
month={},}
@ARTICLE{8957473,
author={W. {Wang} and J. {Shen} and X. {Lu} and S. C. H. {Hoi} and H. {Ling}},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Paying Attention to Video Object Pattern Understanding},
year={2020},
volume={},
number={},
pages={1-1},
abstract={This paper conducts a systematic study on the role of visual attention in video object pattern understanding. By elaborately annotating three popular video segmentation datasets (DAVIS16, Youtube-Objects and SegTrackV2) with dynamic eye-tracking data in the unsupervised video object segmentation (UVOS) setting, for the first time, we quantitatively verified the high consistency of visual attention behavior among human observers, and found strong correlation between human attention and explicit primary object judgments during dynamic, task-driven viewing. Such novel observations provide an in-depth insight of the underlying rationale behind video object pattens. Inspired by these findings, we decouple UVOS into two sub-tasks: UVOS-driven Dynamic Visual Attention Prediction (DVAP) in spatiotemporal domain, and Attention-Guided Object Segmentation (AGOS) in spatial domain. Our UVOS solution enjoys three major advantages: 1) modular training without using expensive video segmentation annotations, instead, using more affordable dynamic fixation data to train the initial video attention module and using existing fixation-segmentation paired static/image data to train the subsequent segmentation module; 2) comprehensive foreground understanding through multi-source learning; and 3) additional interpretability from the biologically-inspired and assessable attention. Experiments on four popular benchmarks show that, even without using expensive video object mask annotations, our model achieves compelling performance compared with state-of-the-arts and enjoys fast processing speed (10 fps on a single GPU). Our collected eye-tracking data and algorithm implementations have been made publicly available at https://github.com/wenguanwang/AGS.},
keywords={Video Object Pattern Understanding;Unsupervised Video Object Segmentation;Top-Down Visual Attention;Video Salient Object Detection},
doi={10.1109/TPAMI.2020.2966453},
ISSN={1939-3539},
month={},}
@INPROCEEDINGS{5656042,
author={J. {Zhang} and J. {Sun} and J. {Liu} and C. {Yang} and H. {Yan}},
booktitle={IEEE 10th INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING PROCEEDINGS},
title={Visual attention model based on multi-scale local contrast of low-level features},
year={2010},
volume={},
number={},
pages={902-905},
abstract={Salient regions detection is becoming more and more important due to its useful application in image representation and understanding. The accurate detection of salient regions can reduce the complexity and improve the efficiency of image processing. In this paper, a visual attention model based on multi-scale local contrast of low level features is proposed. In the proposed model, a multi-scale transform is used to obtain the original image at different scales, and the local contrast features of intensity, texture and color are calculated at each scale. Then these contrast features are interpolated iteratively to form three feature maps corresponding to intensity, texture and color respectively. Finally, the feature maps are integrated to obtain the final salient regions. In the experiment, a proven eye tracking system is used and verifies the salient region detected by the proposed model consistent with human vision. Furthermore, comparing with another two existing models, the proposed model also shows better performance.},
keywords={image representation;visual attention model;multiscale local contrast;image representation;salient region detection;human vision;eye tracking system;Computational modeling;Visualization;Image color analysis;Feature extraction;Humans;Frequency modulation;Biological system modeling;salient region;interest region;visual attention;local contrast;multi-scale transform},
doi={10.1109/ICOSP.2010.5656042},
ISSN={2164-523X},
month={Oct},}
@INPROCEEDINGS{4839576,
author={Z. {Ma}},
booktitle={2009 IEEE Aerospace conference},
title={Dragonfly preying on flying insects, rendezvous search games, and rendezvous and docking in space explorations},
year={2009},
volume={},
number={},
pages={1-8},
abstract={Dragonfly adults are predatory, and they demonstrate extraordinary capability to prey on flying insects. Significant understanding of this dragonfly behavior has been achieved in last 3 decades. Some species of dragonflies are the so-called ldquoperchersrdquo because they sit-and-wait for preys. Some other species of dragonflies are the so-called ldquohawkersrdquo who forage in flight by swooping up to grab prey insects passing overhead (Olberg et al. 1980, 2007). According to Olberg et al. (2000, 2005), this exceptional feat in intercepting and capturing prey insects (with as high as 97% success) are guided by their large compound eyes (each eye may consists of as many as 28000 facets). This amazingly versatile behavior can be abstracted as three interdependent processes: (1) decision to take off and initiate the pursuit; (2) navigating to intercept the prey; (3) coordinating leg movements in space and time to grab the prey. Although the exact mechanisms is still not well understood, it is clear that dragonflies are able to accurately estimate the distance from the passing object, over surprising long ranges, to make take-off decision in pursuit of a flying prey. In this paper, the author first reviews the state-of-the-art research on this extraordinary feat of the dragonfly and then explore the potential to harness it for precise rendezvous and docking in space explorations. To achieve the objective, the first necessary step is to capture the behavior process with realistic and ideally precise mathematical models. However, it appears that the existing object tracking algorithms alone may be insufficient for the modeling task. The author suggests that the search game (SG), rendezvous search game (RSG), and rendezvous search game with evasion are applied to capture the behavior process, particularly, the cognitive decision-making process. Finally, the author proposes a two-layer modeling architecture in which SG/RSG are used at the strategic level and the traditional object tracking algorithms at the tactical level.},
keywords={aerospace computing;cognition;decision making;object detection;dragonfly preying;flying insect;rendezvous search game;rendezvous and docking;space exploration;dragonfly behavior;behavior process;mathematical model;object tracking;cognitive decision-making;modeling architecture;tactical level;Insects;Space exploration;Game theory;Animals;Aerodynamics;Space technology;Computer science;Eyes;Navigation;Leg},
doi={10.1109/AERO.2009.4839576},
ISSN={1095-323X},
month={March},}
@INPROCEEDINGS{6613831,
author={Z. {Sharafi} and A. {Marchetto} and A. {Susi} and G. {Antoniol} and Y. {Guéhéneuc}},
booktitle={2013 21st International Conference on Program Comprehension (ICPC)},
title={An empirical study on the efficiency of graphical vs. textual representations in requirements comprehension},
year={2013},
volume={},
number={},
pages={33-42},
abstract={Graphical representations are used to visualise, specify, and document software artifacts in all stages of software development process. In contrast with text, graphical representations are presented in two-dimensional form, which seems easy to process. However, few empirical studies investigated the efficiency of graphical representations vs. textual ones in modelling and presenting software requirements. Therefore, in this paper, we report the results of an eye-tracking experiment involving 28 participants to study the impact of structured textual vs. graphical representations on subjects' efficiency while performing requirement comprehension tasks. We measure subjects' efficiency in terms of the percentage of correct answers (accuracy) and of the time and effort spend to perform the tasks. We observe no statistically-significant difference in term of accuracy. However, our subjects spent more time and effort while working with the graphical representation although this extra time and effort does not affect accuracy. Our findings challenge the general assumption that graphical representations are more efficient than the textual ones at least in the case of developers not familiar with the graphical representation. Indeed, our results emphasise that training can significantly improve the efficiency of our subjects working with graphical representations. Moreover, by comparing the visual paths of our subjects, we observe that the spatial structure of the graphical representation leads our subjects to follow two different strategies (top-down vs. bottomup) and subsequently this hierarchical structure helps developers to ease the difficulty of model comprehension tasks.},
keywords={data visualisation;formal specification;graphical representation;textual representation;software requirements comprehension;software development process;visual path;spatial structure;eye-tracking study;software artifacts;Visualization;Accuracy;Software;Unified modeling language;Pragmatics;Monitoring;Time measurement;Graphical representation;Textual representation;Eye-tracking study;Visual path},
doi={10.1109/ICPC.2013.6613831},
ISSN={1092-8138},
month={May},}
@INPROCEEDINGS{6069746,
author={M. {Costin}},
booktitle={2011 5th International Symposium on Computational Intelligence and Intelligent Informatics (ISCIII)},
title={Cognitive aspects of multiple tasks and perception: Overview and discussion on recent studies},
year={2011},
volume={},
number={},
pages={75-80},
abstract={Cognitive aspects implemented in computational processes establish frames to model human actions relative to the human corresponding senses. Vision is one of them. To conceive appropriated artificial architectures, behavior manifestations and brain structural proprieties are intensively studied. Physiology is modeled in simplified schemas in order to be implemented in virtual systems and machine models. Eye tracking models offer clues for implementing artificial vision on robots, to give salient information for marketing, for art, or forensic research. If the task might be relatively simple for following one precise target in an image, the issues are more complicated when more objects have to be identified, in a multitask perception simulation. Another aspect is to take into consideration the semantic content and this depends on the detailed, rule-frames, formulation of the problem. A number of direct and reverse processes are identified: from real image to human vision and understanding, and from observation of the human behavior, to computer modeling and image processing. The paper makes a quick overview on the relevant studies connected to these topics and proposes a new schema to identify salient points in an image, based both on image processing techniques and on cognitive aspects, mainly from vision and multi-task perception studies.},
keywords={cognition;robot vision;cognitive aspect;human vision;behavior manifestation;brain structural propriety;physiology;artificial vision;robots;eye tracking model;multitask perception simulation;salient point identification;image processing technique;computer modeling;Visualization;Image color analysis;Humans;Computational modeling;Entropy;Computer architecture;Robots},
doi={10.1109/ISCIII.2011.6069746},
ISSN={null},
month={Sep.},}
@ARTICLE{6790604,
author={T. K. {Horiuchi} and C. {Koch}},
journal={Neural Computation},
title={Analog VLSI-Based Modeling of the Primate Oculomotor System},
year={1999},
volume={11},
number={1},
pages={243-265},
abstract={One way to understand a neurobiological system is by building a simulacrum that replicates its behavior in real time using similar constraints. Analog very large-scale integrated (VLSI) electronic circuit technology provides such an enabling technology. We here describe a neuromorphic system that is part of a long-term effort to understand the primate oculomotor system. It requires both fast sensory processing and fast motor control to interact with the world. A one-dimensional hardware model of the primate eye has been built that simulates the physical dynamics of the biological system. It is driven by two different analog VLSI chips, one mimicking cortical visual processing for target selection and tracking and another modeling brain stem circuits that drive the eye muscles. Our oculomotor plant demonstrates both smooth pursuit movements, driven by a retinal velocity error signal, and saccadic eye movements, controlled by retinal position error, and can reproduce several behavioral, stimulation, lesion, and adaptation experiments performed on primates.},
keywords={},
doi={10.1162/089976699300016908},
ISSN={0899-7667},
month={Jan},}
@INPROCEEDINGS{7851156,
author={N. {Flad} and J. C. {Ditz} and A. {Schmidt} and H. H. {Bulthoff} and L. L. {Chuang}},
booktitle={2016 IEEE Second Workshop on Eye Tracking and Visualization (ETVIS)},
title={Data-driven approaches to unrestricted gaze-tracking benefit from saccade filtering},
year={2016},
volume={},
number={},
pages={1-5},
abstract={Unrestricted gaze tracking that allows for head and body movements can enable us to understand interactive gaze behavior with large-scale visualizations. Approaches that support this, by simultaneously recording eye- and user-movements, can either be based on geometric or data-driven regression models. A data-driven approach can be implemented more flexibly but its performance can suffer with poor quality training data. In this paper, we introduce a pre-processing procedure to remove training data for periods when the gaze is not fixating the presented target stimuli. Our procedure is based on a velocity-based filter for rapid eye-movements (i.e., saccades). Our results show that this additional procedure improved the accuracy of our unrestricted gaze-tracking model by as much as 56 %. Future improvements to data-driven approaches for unrestricted gaze-tracking are proposed, in order to allow for more complex dynamic visualizations.},
keywords={data visualisation;gaze tracking;geometry;interactive systems;large-scale systems;regression analysis;unrestricted gaze-tracking;saccade filtering;interactive gaze behavior;large-scale visualizations;eye-movements;user-movements;geometric regression models;data-driven regression models;complex dynamic visualizations;Data models;Cameras;Training data;Training;Magnetic heads;Tracking;Biology},
doi={10.1109/ETVIS.2016.7851156},
ISSN={null},
month={Oct},}
@INPROCEEDINGS{756054,
author={T. {Horiuchi} and E. {Niebur}},
booktitle={Proceedings 20th Anniversary Conference on Advanced Research in VLSI},
title={Conjunction search using a 1-D, analog VLSI-based, attentional search/tracking chip},
year={1999},
volume={},
number={},
pages={276-290},
abstract={The ability of animals to select a limited region of sensory space for scrutiny is an important factor in dealing with cluttered or complex sensory environments. Such an "attentional" system in the visual domain is believed to be involved in both the perception of objects and the control of eye movements in primates. While we can intentionally guide our attention to perform a specific task, it is also reflexively drawn to "salient" features in our sensory input space. Understanding how high-level task information and lour-level stimulus information can combine to control our sensory processing is of great interest to both neuroscience and engineering. Towards this end, we have designed and fabricated a one-dimensional, analog VLSI vision chip that models covert attentional search and tracking. We extend previous analog VLSI work (Morris and DeWeerth, 1997) on the delayed onset of inhibition in a winner-take-all network to now use extracted image edges as input to the attentional saliency map and to perform serial search on a particular feature conjunction (spatial derivative and the direction-of-motion). We further demonstrate the ability to modify the circuit's parameters "on-the-fly" to switch between a search mode and a tracking mode.},
keywords={analogue processing circuits;VLSI;CMOS analogue integrated circuits;image processing equipment;tracking;CMOS image sensors;conjunction search;1D analog attentional search/tracking chip;attentional search/tracking chip;one-dimensional analog VLSI vision chip;extracted image edges;attentional saliency map;serial search;feature conjunction;search mode;tracking mode;WTA circuit;Very large scale integration;Switches;Animals;Control systems;Process control;Neuroscience;Switching circuits},
doi={10.1109/ARVLSI.1999.756054},
ISSN={1522-869X},
month={March},}
@ARTICLE{8744328,
author={W. {Wang} and J. {Shen} and J. {Xie} and M. {Cheng} and H. {Ling} and A. {Borji}},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Revisiting Video Saliency Prediction in the Deep Learning Era},
year={2019},
volume={},
number={},
pages={1-1},
abstract={Predicting where people look in static scenes, a.k.a visual saliency, has received significant research interests recently. However, relatively less effort has been spent in understanding visual attention over dynamic scenes. This work makes three contributions to video saliency research. First, we introduce a new benchmark, called DHF1K, for predicting fixations during dynamic scene free-viewing, which is a long-time need in this field. DHF1K consists of 1K high-quality, elaborately selected videos annotated by 17 observers using an eye tracker. The videos span a wide range of scenes, motions, object types and backgrounds. Second, we propose a novel video saliency model, called ACLNet, that augments the CNN-LSTM network with a supervised attention mechanism to enable fast end-to-end learning. The attention mechanism explicitly encodes static saliency information, thus allowing LSTM to focus on learning a more flexible temporal saliency representation. Such a design leverages existing large-scale static fixation datasets, avoids overfitting, and significantly improves training efficiency and testing performance. Third, we perform an extensive evaluation of state-of-the-art saliency models on three current datasets (i.e., DHF1K, Hollywood2, UCF sports). Experimental results over more than 1.2K testing videos containing 400K frames demonstrate that ACLNet outperforms other contenders and has a fast processing speed (40fps).},
keywords={Visualization;Benchmark testing;Sports;Predictive models;Analytical models;Computational modeling;Task analysis;Video saliency;dynamic visual attention prediction;benchmark;deep learning},
doi={10.1109/TPAMI.2019.2924417},
ISSN={1939-3539},
month={},}
@INPROCEEDINGS{5699147,
author={C. {Cavaro-Menard} and L. {Zhang} and P. {Le Callet}},
booktitle={2010 2nd European Workshop on Visual Information Processing (EUVIP)},
title={Diagnostic quality assessment of medical images: Challenges and trends},
year={2010},
volume={},
number={},
pages={277-284},
abstract={With medical imaging technologies growth, the question of their assessment on the impact and benefit on patient care is rising. Development and design of those medical imaging technologies should take into account the concept of image quality as it might impact the ability of practicians while they are using image information. Towards that goal, one should consider several human factors involved in image analysis and interpretation, e.g. image perception issues, decision process, image analysis pipeline (detection, localization, characterization...). While many efforts have been dedicated to objectively assess the value of imaging system in terms of ideal decision process, new trends have recently emerged to deal with human observer perfomances. This task effort is huge considering the variability of imaging acquisition methods and the possible pathologies. This paper proposes a survey of some key issues and results associated to this effort. We first outline the wide range of medical images with their own specific features. Next, we review the main methodologies to evaluate diagnostic quality of medical images from subjective assessment including ROC analysis, and diagnostic criteria quality analysis, to objective assessment including metrics based on the HVS, and model observers. At last, we present another evaluation method: eye-tracking studies to gain basic understanding of the visual search and decision-making process.},
keywords={data acquisition;decision making;medical image processing;patient diagnosis;diagnostic quality assessment;medical imaging technologies;patient care;image quality;image information;image analysis;image interpolation;image perception;imaging acquisition methods;decision-making process;Observers;Medical diagnostic imaging;Pathology;Magnetic resonance imaging;Diagnostic accuracy;medical images;ROC analysis;model observer;eye-tracking},
doi={10.1109/EUVIP.2010.5699147},
ISSN={null},
month={July},}
@ARTICLE{8704995,
author={S. {Wei} and L. {Liao} and J. {Li} and Q. {Zheng} and F. {Yang} and Y. {Zhao}},
journal={IEEE Transactions on Image Processing},
title={Saliency Inside: Learning Attentive CNNs for Content-Based Image Retrieval},
year={2019},
volume={28},
number={9},
pages={4580-4593},
abstract={In content-based image retrieval (CBIR), one of the most challenging and ambiguous tasks is to correctly understand the human query intention and measure its semantic relevance with images in the database. Due to the impressive capability of visual saliency in predicting human visual attention that is closely related to the query intention, this paper attempts to explicitly discover the essential effect of visual saliency in CBIR via qualitative and quantitative experiments. Toward this end, we first generate the fixation density maps of images from a widely used CBIR dataset by using an eye-tracking apparatus. These ground-truth saliency maps are then used to measure the influence of visual saliency to the task of CBIR by exploring several probable ways of incorporating such saliency cues into the retrieval process. We find that visual saliency is indeed beneficial to the CBIR task, and the best saliency involving scheme is possibly different for different image retrieval models. Inspired by the findings, this paper presents two-stream attentive convolutional neural networks (CNNs) with saliency embedded inside for CBIR. The proposed network has two streams that simultaneously handle two tasks. The main stream focuses on extracting discriminative visual features that are tightly related to semantic attributes. Meanwhile, the auxiliary stream aims to facilitate the main stream by redirecting the feature extraction to the salient image content that a human may pay attention to. By fusing these two streams into the Main and Auxiliary CNNs (MAC), image similarity can be computed as the human being does by reserving conspicuous content and suppressing irrelevant regions. Extensive experiments show that the proposed model achieves impressive performance in image retrieval on four public datasets.},
keywords={content-based retrieval;convolutional neural nets;feature extraction;gaze tracking;image fusion;image retrieval;learning (artificial intelligence);eye-tracking apparatus;MAC;main and auxiliary CNN;discriminative visual feature extraction;CBIR;CNN;two-stream attentive convolutional neural networks;ground-truth saliency maps;human visual attention;visual saliency;human query intention;content-based image retrieval;saliency inside;image similarity;salient image content;Feature extraction;Image retrieval;Visualization;Semantics;Streaming media;Task analysis;Reliability;Visual saliency;content-based image retrieval;bag-of-word;convolutional neural networks},
doi={10.1109/TIP.2019.2913513},
ISSN={1941-0042},
month={Sep.},}
@INPROCEEDINGS{6298440,
author={J. {You}},
booktitle={2012 IEEE International Conference on Multimedia and Expo},
title={Video Gaze Prediction: Minimizing Perceptual Information Loss},
year={2012},
volume={},
number={},
pages={438-443},
abstract={Automatic detection of visually interesting regions and gaze points plays an important role in many video applications. Due to limited ability of the human visual system (HVS) when processing visual stimuli at any instant, a natural function of gaze changes is to collect as much information as possible to form an accurate understanding of the visual scene. This paper proposes an automatic gaze prediction algorithm by modeling such function. An improved foveal imaging model is developed by taking visual attention and temporal visual characteristics into account. Gaze changes are predicted based on minimizing perceptual information loss due to the foveated vision mechanism. Experimental results against a video eye-tracking database demonstrate a promising performance of the proposed gaze prediction algorithm.},
keywords={video signal processing;video gaze prediction;perceptual information loss;automatic detection;gaze points;human visual system;HVS;natural function;foveal imaging model;foveated vision mechanism;video eye-tracking database;Visualization;Predictive models;Sensitivity;Retina;Observers;Loss measurement;Brain modeling;eye movement;foveal imaging;gaze detection;perceptual information loss;visual attention},
doi={10.1109/ICME.2012.191},
ISSN={1945-7871},
month={July},}
@INPROCEEDINGS{6909625,
author={K. A. {Funes Mora} and J. {Odobez}},
booktitle={2014 IEEE Conference on Computer Vision and Pattern Recognition},
title={Geometric Generative Gaze Estimation (G3E) for Remote RGB-D Cameras},
year={2014},
volume={},
number={},
pages={1773-1780},
abstract={We propose a head pose invariant gaze estimation model for distant RGB-D cameras. It relies on a geometric understanding of the 3D gaze action and generation of eye images. By introducing a semantic segmentation of the eye region within a generative process, the model (i) avoids the critical feature tracking of geometrical approaches requiring high resolution images, (ii) decouples the person dependent geometry from the ambient conditions, allowing adaptation to different conditions without retraining. Priors in the generative framework are adequate for training from few samples. In addition, the model is capable of gaze extrapolation allowing for less restrictive training schemes. Comparisons with state of the art methods validate these properties which make our method highly valuable for addressing many diverse tasks in sociology, HRI and HCI.},
keywords={cameras;gaze tracking;image resolution;image segmentation;remote RGB-D cameras;geometric generative gaze estimation;G3E;head pose invariant gaze estimation;distant RGB-D cameras;3D gaze action;eye image generation;semantic segmentation;eye region;high resolution images;person dependent geometry;ambient conditions;gaze extrapolation;diverse tasks;sociology;Eyelids;Head;Image color analysis;Three-dimensional displays;Image segmentation;Training;Visualization;gaze estimation;generative model;segmentation;RGB-D;HRI;HCI;HHI},
doi={10.1109/CVPR.2014.229},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{7961497,
author={A. {Begel}},
booktitle={2017 IEEE/ACM 25th International Conference on Program Comprehension (ICPC)},
title={Keynote - The ABCs of Software Engineering: Affect, Biometrics, and Cognition},
year={2017},
volume={},
number={},
pages={xxi-xxi},
abstract={Summary for only given, as follows. Researchers have long investigated how people read, write, and speak about software on their computers to identify the skills, education, and practices needed need to acquire expertise and perform development duties effectively and efficiently. However, until now the methods used to study developer comprehension, expression, and communication has been limited and coarse-grained because there was no way to identify what a developer thought or felt unless it was expressed out loud. The world has changed. With the introduction of low-cost, widely available, high-fidelity biometric sensors, we can now more directly observe a software developer's cognitive and affective (emotional) processes. The ABCs of Software Engineering is a set of techniques that modernize classic approaches to program comprehension and human interaction by combining (A) principles governing the influence of human *affect* on behavior, (B) *biometric* sensors, and (C) models of *cognition* informed by advances in cognitive neuroscience. Technologies like electroencephalography (EEG), electro-dermal activity sensors (EDA), capacitive sensors, and eye trackers can reveal a software developer's internal emotional states, for example identifying when the developer is confused, frustrated, surprised, stressed, fatigued, or in a highly productive flow state. These affective states can be correlated with code quality, software complexity, development productivity, and effective communication - the same software outcomes already correlated with developer activities in other research areas such as mining software repositories (MSR) and cooperative and human aspects of software engineering (CHASE). By developing a better understanding of what programmers think and feel when they create and maintain software, we can design tools and interventions to improve their productivity and reduce the impact of their errors.},
keywords={human factors;software engineering;software engineering;affect;biometrics;cognition;developer comprehension;developer expression;developer communication;code quality;software complexity;development productivity;effective communication;mining software repositories;MSR;cooperative aspect;human aspect},
doi={10.1109/ICPC.2017.44},
ISSN={null},
month={May},}