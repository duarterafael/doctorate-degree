@INPROCEEDINGS{8633162,
author={X. {Wang} and F. {Meng} and X. {Huang}},
booktitle={2018 11th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)},
title={Visual Behaviors Analysis Based on Eye Tracker in Subjective Image Quality Assessment},
year={2018},
volume={},
number={},
pages={1-4},
abstract={The study of the visual behaviors of observers in subjective image quality assessment is helpful in understanding of human visual system. It can also be used to improve the reliability of assessment results. In this paper, we propose a subjective image quality assessment system based on eye tracker. The system can record the viewpoints of observers' eye movements in the process of experiment. The system also can analyze the visual behaviors of observers. In the experiment, we first verify the accuracy of the recorded viewpoints. After, we choose the observers with different professional backgrounds. From the analysis of the assessment results and the corresponding viewpoints, we can see that non-experts are more easily to be attracted by the image contents, and the viewpoints of experts are more overall.},
keywords={eye;image processing;object tracking;visual behaviors analysis;subjective image quality assessment system;human visual system;eye tracker;image contents;observers;Observers;Visualization;Videos;Tracking;Image quality;Bit rate;Calibration;Eye tracker;visual behaviors;subjective image quality assessment},
doi={10.1109/CISP-BMEI.2018.8633162},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6130492,
author={ {Selpi} and T. {Wilhelm} and M. {Jansson} and L. {Hagström} and N. {Brandin} and M. {Andersson} and J. {Grönvall}},
booktitle={2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)},
title={Automatic real-time FACS-coder to anonymise drivers in eye tracker videos},
year={2011},
volume={},
number={},
pages={1986-1993},
abstract={Driver's face is a rich source of information for understanding driver behaviour. From the driver's face, one could get an idea of the driver's emotional state and where s/he looks at. In recent years, naturalistic driving studies and field operational tests have been conducted to collect driver behavioural data, which often includes video of the driver, from many drivers driving for an extended period of time. Due to the Data Privacy Act, it is desirable to make the driver video anonymous, while preserving the original facial expressions. This paper describes our attempt to make a system that could do so. The system is a combination of an automatic Facial Action Coding System (FACS) coder based on Active Appearance Models (AAMs), a classifier that analyses local deformations in the AAM shape mesh and a 3D visualisation. The image acquisition hardware is based on a SmartEye eye tracker installed in a vehicle. The eye tracker we used provides a constant image quality independent of external illumination, which is a precondition for deploying the system in a vehicle environment. While the system uses Action Unit (AU) activations internally, the evaluation was done using the six basic emotions.},
keywords={behavioural sciences computing;data privacy;data visualisation;face recognition;traffic engineering computing;video coding;automatic real-time FACS-coder;eye tracker video;driver face;driver emotional state;naturalistic driving studies;driver behavioural data;Data Privacy Act;driver video;facial expression;facial action coding system;active appearance model;local deformation;AAM shape mesh;3D visualisation;image acquisition hardware;SmartEye eye tracker;image quality;vehicle environment;action unit;Face;Shape;Videos;Databases;Gold;Vehicles},
doi={10.1109/ICCVW.2011.6130492},
ISSN={},
month={Nov},}
@ARTICLE{7329925,
author={S. {Eivazi} and R. {Bednarik} and V. {Leinonen} and M. {von und zu Fraunberg} and J. E. {Jääskeläinen}},
journal={IEEE Sensors Journal},
title={Embedding an Eye Tracker Into a Surgical Microscope: Requirements, Design, and Implementation},
year={2016},
volume={16},
number={7},
pages={2070-2078},
abstract={Eye tracking has long been known as a tool for attention tracking, however, the understanding of gaze in the critical domains such as surgery is still in its infancy. In image-guided surgery, studying the role that visual attention plays in eye-hand coordination, situation awareness, and instrumentation control is critical in order to understand the nature of expertise and explore the possibilities for gaze-based interaction. To date, the eye-tracking technology has not been embedded into an operation room microscope and thus limited knowledge is available about the role of attention in real-life image-guided surgery. To advance the state-of-the-art, we adopted an optical solution for eye tracking and embedded a binocular eye tracker into a surgical microscope. We present the design principles and development evaluation cycles, as well as highlight the technical challenges encountered when embedding an eye tracker for a surgical microscope. The developed solution can be applied for other types of microscopes and ocular-based optical devices, for example, ophthalmology, otolaryngology, plastic and reconstructive surgery, and astronomical devices.},
keywords={biomedical optical imaging;eye;optical microscopy;surgery;surgical microscope;critical domains;image-guided surgery;eye-hand coordination;visual attention;eye-hand coordination;situation awareness;instrumentation control;gaze-based interaction;eye-tracking technology;operation room microscope;real-life image-guided surgery;optical solution;binocular eye tracker;design principles;development evaluation cycles;ocular-based optical devices;ophthalmology;otolaryngology;plastic surgery;reconstructive surgery;astronomical devices;Microscopy;Cameras;Gaze tracking;Optical microscopy;Microsurgery;Tracking;Eye tracking;microsurgery;operating room;Eye tracking;microsurgery;operating room},
doi={10.1109/JSEN.2015.2501237},
ISSN={},
month={April},}
@INPROCEEDINGS{8270330,
author={S. {Ishimaru} and S. {Jacob} and A. {Roy} and S. S. {Bukhari} and C. {Heisel} and N. {Großmann} and M. {Thees} and J. {Kuhn} and A. {Dengel}},
booktitle={2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)},
title={Cognitive State Measurement on Learning Materials by Utilizing Eye Tracker and Thermal Camera},
year={2017},
volume={08},
number={},
pages={32-36},
abstract={We demonstrate how information derived from pervasive sensors can quantify cognitive states of learners while they are reading a textbook. Eye tracking is one of the most effective approaches to measuring reading behavior. For example, high fixation duration represents a reader's attention on a document. However, it is still a challenging task to predict the reason for the attention (i.e., is it because of his/her interest or trouble of understanding?). In this paper, we utilize additional sensing modality to solve the problem. On the dataset of 12 high school students' reading behaviors, we have found that the changing of pupil diameter and nose temperature are highly correlated with their cognitive states including their interests and efforts for reading/solving tasks on learning materials in Physics.},
keywords={cognition;education;eye;gaze tracking;human computer interaction;object tracking;psychology;cognitive state measurement;thermal camera;pervasive sensors;eye tracking;reading behavior;high fixation duration;eye tracker;high school students;learning materials;cognitive behavior;psychological model;digital textbook;Human-Document Interaction;Temperature measurement;Nose;Physics;Task analysis;Temperature sensors;Gaze tracking;eye tracking;thermal image analysis;reading;didactics;Physics;interest;confidence;workload},
doi={10.1109/ICDAR.2017.378},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8813302,
author={J. {Bauer} and J. {Siegmund} and N. {Peitek} and J. C. {Hofmeister} and S. {Apel}},
booktitle={2019 IEEE/ACM 27th International Conference on Program Comprehension (ICPC)},
title={Indentation: Simply a Matter of Style or Support for Program Comprehension?},
year={2019},
volume={},
number={},
pages={154-164},
abstract={An early study showed that indentation is not a matter of style, but provides actual support for program comprehension. In this paper, we present a non-exact replication of this study. Our aim is to provide empirical evidence for the suggested level of indentation made by many style guides. Following Miara and others, we also included the perceived difficulty, and we extended the original design to gain additional insights into the influence of indentation on visual effort by employing an eye-tracker. In the course of our study, we asked 22 participants to calculate the output of Java code snippets with different levels of indentation, while we recorded their gaze behavior. We did not find any indication that the indentation levels affect program comprehension or visual effort, so we could not replicate the findings of Miara and others. Nevertheless, our modernization of the original experiment design is a promising starting point for future studies in this field.},
keywords={gaze tracking;Java;reverse engineering;program comprehension;style guides;visual effort;indentation levels;gaze behavior;Java code snippets;Miara;eye tracker;Code Indentation, Program Comprehension, Visual Effort},
doi={10.1109/ICPC.2019.00033},
ISSN={},
month={May},}
@INPROCEEDINGS{5970175,
author={Z. {Sharafi}},
booktitle={2011 IEEE 19th International Conference on Program Comprehension},
title={A Systematic Analysis of Software Architecture Visualization Techniques},
year={2011},
volume={},
number={},
pages={254-257},
abstract={The visualization of software systems allows a software developer to build a mental model of the program supporting her to better understand its design and functionality. This research aims at studying current visualization techniques and practices to propose a set of principles for designing effective software architecture visualization techniques, focusing on their support for program comprehension. The research will be carried out in three main phases. First, we will complement current works by proposing a taxonomy of visualization techniques. The second contribution will be to identify different requirements and characteristics of architecture visualization techniques. Finally, to evaluate software visualization's usability and effectiveness in practice, we will measure the performance of developers in terms of their percentage of the correct answers and effort developers spend to answer given questions. To compute the developer's effort, we will use eye-tracker's data.},
keywords={data visualisation;reverse engineering;software architecture;software quality;systematic analysis;software architecture visualization technique;program comprehension;Visualization;Usability;Software architecture;Computer architecture;Software systems;Measurement;Software visualization;Software architecture;program comprehension;usability;eye tracking;usability},
doi={10.1109/ICPC.2011.40},
ISSN={},
month={June},}
@INPROCEEDINGS{8270329,
author={C. L. {Sanches} and O. {Augereau} and K. {Kise}},
booktitle={2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)},
title={Using the Eye Gaze to Predict Document Reading Subjective Understanding},
year={2017},
volume={08},
number={},
pages={28-31},
abstract={The traditional way to analyze the content of a document is to perform document image analysis. However, analyzing how the user perceives a document is another way to get information about the documents but also about the users. By using sensors such as eye tracker, it is possible analyze the reader's skill or the document comprehensibility. In this paper, we focus on predicting the user's understanding as it can be used as a feedback either for the author (to improve his document) or the user (to review the parts he did not understand). The eye movements of the readers are recorded by an eye tracker while they read several documents, then several features are extracted and a support vector regression system is used to predict the readers' understanding. In our experiment, 17 subjects were asked to read 19 documents for a total of 323 recordings. As a first result, we prove that the subjective understanding of the reader can be predicted more accurately by using the eye gaze than by asking a multiple choice question.},
keywords={eye;image motion analysis;literature;object tracking;regression analysis;support vector machines;eye gaze;document image analysis;eye tracker;document comprehensibility;document reading subjective understanding prediction;reader skill;reader eye movements;support vector regression system;Feature extraction;Estimation;Text analysis;Tracking;Electronic mail;Image analysis},
doi={10.1109/ICDAR.2017.377},
ISSN={},
month={Nov},}
@INPROCEEDINGS{1247017,
author={B. {Allier} and H. {Emptoz}},
booktitle={Proceedings 2003 International Conference on Image Processing (Cat. No.03CH37429)},
title={Character prototyping in document images using Gabor filters},
year={2003},
volume={1},
number={},
pages={I-537},
abstract={In this article we present a particular application of Gabor filtering for machine-printed document image understanding. To do so, we assume that the text can be seen as texture, characters being the smallest texture elements, and we verify this hypothesis by a series of experiments over different sets of character images. We first apply a bank of 24 Gabor filters (4 frequencies and 6 orientations) on each set, then we extract texture features further used to classify character images without a priori knowledge using a Bayesian classifier. Results are shown for different characters written in a same font, and for different font types given a character.},
keywords={filters;document image processing;feature extraction;image classification;belief networks;character prototyping;Gabor filters;machine-printed document image;feature extraction;character image classification;Bayesian classifier;Prototypes;Gabor filters;Frequency;Testing;Filtering;Image segmentation;Feature extraction;Image analysis;Humans;Reconnaissance},
doi={10.1109/ICIP.2003.1247017},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8706573,
author={D. {Zhou} and J. {Huang} and J. {Dang}},
booktitle={2018 11th International Symposium on Chinese Spoken Language Processing (ISCSLP)},
title={Investigation of the Comprehension Process during Silent Reading based on Eye Movements},
year={2018},
volume={},
number={},
pages={165-169},
abstract={One of the important issues in silent reading comprehension is whether comprehension is a bottom-up or top-down process. Another issue is what the role of orthography and phonology is on semantic retrieval. Previous researches investigated these issues by using eye movements to measure the reading behaviors. A number of studies attempted to address the issues by controlling the frequency of the words in a sentence, but obtained contradictory results. To reexamine these issues, we manipulated the orthography and phonology of a target word in a sentence and observed readers' behavior in their silent reading comprehension. During the silent reading, subjects looked through the words surround the target word a number of rounds. The fixation time of the first round implied that the manipulation did not affect the reading speed of either the preceding word or the following word. In the following rounds, more fixation time took place in both the preceding and following words. Accordingly, the silent comprehension process in the early stage seems to be a bottom-up process, while it more likes a top-down process in the latter stage. It is also found that the comprehension process took less look-back counts and shorter reading time in the case of phonology manipulation than that in the orthography manipulation. This phenomenon indicates that phonological information plays a more important role on semantic retrieval for the silent reading comprehension.},
keywords={behavioural sciences computing;eye;gaze tracking;image motion analysis;object tracking;silent reading comprehension;eye movements;orthography;phonology;reading behaviors;fixation time;reading speed;reader behavior;Eyelink 1000 plus system;eye tracker;Semantics;Computational modeling;Visualization;Linguistics;Tracking;Standards;Frequency control;silent reading comprehension;eye movements;orthography and phonology;bottom-up and top-down process},
doi={10.1109/ISCSLP.2018.8706573},
ISSN={},
month={Nov},}
@INPROCEEDINGS{7961502,
author={J. {Melo} and F. B. {Narcizo} and D. W. {Hansen} and C. {Brabrand} and A. {Wasowski}},
booktitle={2017 IEEE/ACM 25th International Conference on Program Comprehension (ICPC)},
title={Variability through the Eyes of the Programmer},
year={2017},
volume={},
number={},
pages={34-44},
abstract={Preprocessor directives (#ifdefs) are often used to implement compile-time variability, despite the critique that they increase complexity, hamper maintainability, and impair code comprehensibility. Previous studies have shown that the time of bug finding increases linearly with variability. However, little is known about the cognitive process of debugging programs with variability. We carry out an experiment to understand how developers debug programs with variability. We ask developers to debug programs with and without variability, while recording their eye movements using an eye tracker. The results indicate that debugging time increases for code fragments containing variability. Interestingly, debugging time also seems to increase for code fragments without variability in the proximity of fragments that do contain variability. The presence of variability correlates with increase in the number of gaze transitions between definitions and usages for fields and methods. Variability also appears to prolong the "initial scan" of the entire program that most developers initiate debugging with.},
keywords={cognition;gaze tracking;human factors;program compilers;program debugging;software maintenance;gaze transitions;code fragments;debugging time;eye tracker;eye movements;program debugging;cognitive process;code comprehensibility;software maintainability;compile-time variability;preprocessor directives;Debugging;Computer bugs;Complexity theory;Tracking;Tools;Games;Open source software;Variability;Preprocessors;Debugging;Eye Tracking;Highly-Configurable Systems},
doi={10.1109/ICPC.2017.34},
ISSN={},
month={May},}
@INPROCEEDINGS{8695957,
author={K. {Baskoro} and A. {Widyanti}},
booktitle={2018 International Conference on Information Technology Systems and Innovation (ICITSI)},
title={Usability Evaluation on an Indonesian Mobile Application for Small Business Lending},
year={2018},
volume={},
number={},
pages={148-153},
abstract={This study aims to evaluate the usability of Indonesian mobile application for small business lending regarding its effectiveness, efficiency, and satisfaction, as well as through the understanding of users' comments gathered through retrospective think aloud. The study was conducted in two different experiments, one using smartphones and the other using an eye tracker device, to operate the application. Data was obtained from a total of 60 participants who were asked to do a series of tasks on the application and executed retrospective think aloud. Participants for the smartphone experiment also filled out a questionnaire on user interface satisfaction (QUIS).Based on the acquired findings, the application was found to have good effectiveness and satisfaction, but still has problems regarding efficiency and users' grievances found through retrospective think aloud.},
keywords={business data processing;customer satisfaction;financial data processing;human factors;mobile computing;smart phones;user interfaces;usability evaluation;Indonesian mobile application;small business lending;smartphone experiment;QUIS;questionnaire on user interface satisfaction;eye tracker device experiment;Task analysis;Usability;Smart phones;Mobile applications;User interfaces;Correlation;usability;usability evaluation;eye tracker;retrospective think aloud;mobile application;small business lending},
doi={10.1109/ICITSI.2018.8695957},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7521498,
author={I. G. {Silva} and C. T. {Lopes} and M. {Ellison}},
booktitle={2016 11th Iberian Conference on Information Systems and Technologies (CISTI)},
title={Can we detect English proficiency through reading behavior? A preliminary study},
year={2016},
volume={},
number={},
pages={1-6},
abstract={If it were possible to automatically detect proficiency in languages using data from eye movements, new levels of customizing computer applications could possibly be achieved. An example in case is web searches where suggestions and results could be adjusted to the user's knowledge of the language. The objective of this study is to compare the reading habits of users with high and low English language proficiency, having in mind the possible automatic detection of the English proficiency level through reading. For this purpose, a study was conducted with two types of user, those with a high level of proficiency (Proficient Users), and those with low proficiency (Basic Users) in the English language. An eye-tracker was used to collect users' eye movements while reading a text in English. Results show that users with high proficiency engage in more careful reading. In contrast, low English proficiency users take more time to read, revisit sentences and paragraphs more often, have more and longer fixations and also a higher number of saccades. As expected, these users have more difficulties in understanding the text.},
keywords={gaze tracking;human computer interaction;linguistics;natural language processing;reading behavior;eye movements;reading habits;English language proficiency;English proficiency level automatic detection;proficient users;basic users;eye-tracker;English text reading;text understanding;human computer interaction;Frequency measurement;Position measurement;Computers;Dispersion;Computer applications;Knowledge engineering;human-computer interaction;eye-tracker;English proficiency;user study},
doi={10.1109/CISTI.2016.7521498},
ISSN={},
month={June},}
@INPROCEEDINGS{8846978,
author={H. {Sari} and F. {Mutaqin} and A. P. {Setiaboedi}},
booktitle={2018 Thirteenth International Conference on Digital Information Management (ICDIM)},
title={The Effect of Different Type of Information on Trust in Facebook Page},
year={2018},
volume={},
number={},
pages={191-195},
abstract={The use of social media has grown as an essential communication tool for e-commerce. To be effective, a clear understanding of the impact of information on viewers should be reached. This paper investigates the impact of the different type of information on Facebook, i.e., detailed information, interactivity information and persuasive information, on the level of trust. Based on experiment design using an eye tracker devices, we find that detailed information and interactivity information have a positive and significant effect on trust. Therefore, information about the product and promise for quick response to the question are critical to be included on a Facebook page.},
keywords={Facebook;Business;Atmospheric measurements;Particle measurements;Heating systems;Receivers;eye tracker;Facebook;information;trust},
doi={10.1109/ICDIM.2018.8846978},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{5316015,
author={S. {Jeanmart} and Y. {Gueheneuc} and H. {Sahraoui} and N. {Habra}},
booktitle={2009 3rd International Symposium on Empirical Software Engineering and Measurement},
title={Impact of the visitor pattern on program comprehension and maintenance},
year={2009},
volume={},
number={},
pages={69-78},
abstract={In the software engineering literature, many works claim that the use of design patterns improves the comprehensibility of programs and, more generally, their maintainability. Yet, little work attempted to study the impact of design patterns on the developers' tasks of program comprehension and modification. We design and perform an experiment to collect data on the impact of the visitor pattern on comprehension and modification tasks with class diagrams. We use an eye-tracker to register saccades and fixations, the latter representing the focus of the developers' attention. Collected data show that the visitor pattern plays a role in maintenance tasks: class diagrams with its canonical representation requires less efforts from developers.},
keywords={object-oriented programming;reverse engineering;software maintenance;visitor pattern;program comprehension;program maintenance;software engineering literature;design patterns;Unified modeling language;Software engineering;Software measurement;Level measurement;Electronic mail;Pattern analysis;Visualization;Performance evaluation;Standards development;Design for experiments},
doi={10.1109/ESEM.2009.5316015},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8239082,
author={F. E. {Gunawan} and O. {Wijaya} and B. {Soewito} and S. {Candra} and {Diana} and C. E. {Suharyanto} and N. {Sekishita}},
booktitle={2017 4th International Conference on Electrical Engineering, Computer Science and Informatics (EECSI)},
title={An analysis of concentration region on powerpoint slides using eye tracking},
year={2017},
volume={},
number={},
pages={1-5},
abstract={Powerpoint slides have become one of the essential teaching tools in academic for both offline and online modes. It may play a useful role to facilitate discussion and information exchange. However, in our teaching experience, we find many students utilizing Powerpoint slides beyond their traditional functions. Many students fully rely on the slides as the main learning materials and, in some cases, substituting textbooks. This study intends to understand how students interact with the learning materials presented on Powerpoint slides. The interaction is measured using an eye tracker device called the Eye Tribe Tracker. Thirty sophomore and junior students are asked to participate. They are instructed to learn a topic in the subject of Introduction to Algorithm and Programming, a basic course in the computer science field. During the process, their fixation points are monitored and are related to the contents on the slides. The results are rather surprising. Many students read the slides in unexpected manners that may compromise their understanding and may lead to inaccurate interpretations.},
keywords={computer aided instruction;computer science education;educational courses;teaching;powerpoint slides;main learning materials;eye tracker device;Eye Tribe Tracker;teaching tools;concentration region analysis;eye tracking;students interaction;basic course;computer science field;Image color analysis;Gaze tracking;Monitoring;Programming;Education;Atmospheric measurements;Particle measurements},
doi={10.1109/EECSI.2017.8239082},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7889231,
author={D. {Pappusetty} and V. V. R. {Chinta} and H. {Kalva}},
booktitle={2017 IEEE International Conference on Consumer Electronics (ICCE)},
title={Using pupillary response to assess video quality},
year={2017},
volume={},
number={},
pages={64-65},
abstract={Pupil response can be measured non-intrusively using an eye tracker and offers a potentially new approach to understanding video structure and content. An analysis of pupil response to quality variations in a video is reported in this paper. Experiments were conducted under free viewing conditions and pupillary response of subjects was analyzed. Video clip encoded with AVC/H.264 at various qualities and durations were used to assess user response. Results show pupillary constrictions at points of quality transitions.},
keywords={eye;object tracking;video coding;pupillary response;video quality;eye tracker;video structure;quality variations;free viewing conditions;video clip;AVC-H.264;user response;quality transitions;Quality assessment;Streaming media;Video recording;Brightness;Bit rate;Distortion},
doi={10.1109/ICCE.2017.7889231},
ISSN={},
month={Jan},}
@ARTICLE{8666160,
author={A. {Morando} and T. {Victor} and M. {Dozza}},
journal={IEEE Transactions on Intelligent Transportation Systems},
title={A Bayesian Reference Model for Visual Time-Sharing Behaviour in Manual and Automated Naturalistic Driving},
year={2019},
volume={},
number={},
pages={1-12},
abstract={Visual time-sharing (VTS) behavior characterizes an inattentive driver. Because inattention has been identified as the major contributing factor in traffic crashes, understanding the relationship between VTS and crash risk could help reduce the crash risk through the development of inattention countermeasures. The aims of this paper are: 1) to develop a reference model of the VTS behavior and 2) reveal if vehicle automation influences the VTS behavior. The reference model was based on naturalistic eye-tracking data. The VTS sequences were extracted from routine driving data (including manual and automated driving). We used Bayesian generalized linear mixed models for a range of on- and off-path glance-based metrics. Each parameter was estimated with a probability distribution and summarized with credible intervals containing the model parameters with 95% probability. The reference model not only corroborates previous findings from the driving simulator experiments and on-road studies, but also captures the characteristics of on-path and off-path glance behavior in greater detail. The model demonstrated that: 1) there was minimal change in the VTS behavior due to automation and 2) the percentage of time that glances fell on-path (PRC) was greater for all routine driving (~80%) than for the VTS sequences (~50%). The PRC was the only metric that was sensitive to the VTS, but it did not differentiate between manual and automated driving. Our model, by describing a measure of inattention (VTS behavior), can be used in future driver models to improve the computer simulations used to design ADASs and evaluate their safety benefits. In addition, the model could serve as a detailed reference for inattention guidelines.},
keywords={Vehicles;Task analysis;Bayes methods;Manuals;Measurement;Biological system modeling;Visualization;ADAS;attention;eye tracker;glance distribution;vehicle automation;visual behavior.},
doi={10.1109/TITS.2019.2900436},
ISSN={},
month={},}
@INPROCEEDINGS{7965634,
author={A. {De Abreu} and C. {Ozcinar} and A. {Smolic}},
booktitle={2017 Ninth International Conference on Quality of Multimedia Experience (QoMEX)},
title={Look around you: Saliency maps for omnidirectional images in VR applications},
year={2017},
volume={},
number={},
pages={1-6},
abstract={Understanding visual attention has always been a topic of great interest in the graphics, image/video processing, robotics and human-computer interaction communities. By understanding salient image regions, the compression, transmission and rendering algorithms can be optimized. This is particularly important in omnidirectional images (ODIs) viewed with a head-mounted display (HMD), where only a fraction of the captured scene is displayed at a time, namely viewport. In order to predict salient image regions, saliency maps are estimated either by using an eye tracker to collect eye fixations during subjective tests or by using computational models of visual attention. However, eye tracking developments for ODIs are still in the early stages and although a large list of saliency models are available, no particular attention has been dedicated to ODIs. Therefore, in this paper, we consider the problem of estimating saliency maps for ODIs viewed with HMDs, when the use of an eye tracker device is not possible. We collected viewport center trajectories (VCTs) of 32 participants for 21 ODIs and propose a method to transform the gathered data into saliency maps. The obtained saliency maps are compared in terms of image exposition time used to display each ODI in the subjective tests. Then, motivated by the equator bias tendency in ODIs, we propose a post-processing method, namely fused saliency maps (FSM), to adapt current saliency models to ODIs requirements. We show that the use of FSM on current models improves their performance by up to 20%. The developed database and testbed are publicly available with this paper.},
keywords={data compression;helmet mounted displays;image coding;image representation;rendering (computer graphics);virtual reality;omnidirectional images;VR applications;graphics;image processing;video processing;robotics;human-computer interaction;salient image regions;compression algorithms;transmission algorithms;rendering algorithms;head-mounted display;HMD;eye tracker;eye fixations;viewport center trajectories;VCTs;image exposition time;fused saliency maps;FSM;ODI representation;Visualization;Computational modeling;Adaptation models;Resists;Three-dimensional displays;Solid modeling;Trajectory;Fixations;head-mounted display (HMD);omnidirectional images (ODIs);saliency maps;viewport;virtual reality (VR)},
doi={10.1109/QoMEX.2017.7965634},
ISSN={},
month={May},}
@INPROCEEDINGS{6650519,
author={B. {Sharif} and G. {Jetty} and J. {Aponte} and E. {Parra}},
booktitle={2013 First IEEE Working Conference on Software Visualization (VISSOFT)},
title={An empirical study assessing the effect of seeit 3D on comprehension},
year={2013},
volume={},
number={},
pages={1-10},
abstract={A study to assess the effect of SeeIT 3D, a software visualization tool is presented. Six different tasks in three different task categories are assessed in the context of a large open-source system. Ninety-seven subjects were recruited from three different universities to participate in the study. Two methods of data collection: traditional questionnaires and an eye-tracker were used. The main goal was to determine the impact and added benefit of SeeIT 3D while performing typical software tasks within the Eclipse IDE. Results indicate that SeeIT 3D performs significantly better in one task category namely overview tasks but takes significantly longer when completing bug fixing tasks. Scores obtained by the subjects in the SeeIT 3D group are 13% better and 45% faster for overview tasks.},
keywords={program debugging;program visualisation;public domain software;Eclipse IDE;task category;bug fixing tasks;software tasks;eye-tracking;data collection;open-source system;software visualization tool;SeeIT 3D;comprehension;Three-dimensional displays;Accuracy;Educational institutions;Software;Visualization;Tracking;Data visualization;software visualization;eye tracking study;SeeIT 3D;empirical study},
doi={10.1109/VISSOFT.2013.6650519},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{5521745,
author={B. {Sharif} and J. I. {Maletic}},
booktitle={2010 IEEE 18th International Conference on Program Comprehension},
title={An Eye Tracking Study on camelCase and under_score Identifier Styles},
year={2010},
volume={},
number={},
pages={196-205},
abstract={An empirical study to determine if identifier-naming conventions (i.e., camelCase and under_score) affect code comprehension is presented. An eye tracker is used to capture quantitative data from human subjects during an experiment. The intent of this study is to replicate a previous study published at ICPC 2009 (Binkley et al.) that used a timed response test method to acquire data. The use of eye-tracking equipment gives additional insight and overcomes some limitations of traditional data gathering techniques. Similarities and differences between the two studies are discussed. One main difference is that subjects were trained mainly in the underscore style and were all programmers. While results indicate no difference in accuracy between the two styles, subjects recognize identifiers in the underscore style more quickly.},
keywords={programming;camelCase;under_score identifier styles;code comprehension;eye-tracking equipment;Programming profession;Computer science;Humans;Testing;Cognitive science;Computer languages;Java;Keyboards;Writing;Software maintenance;identifier styles;eye-tracking study;code readability},
doi={10.1109/ICPC.2010.41},
ISSN={},
month={June},}
@ARTICLE{7513425,
author={E. {Bekele} and D. {Bian} and J. {Peterman} and S. {Park} and N. {Sarkar}},
journal={IEEE Transactions on Neural Systems and Rehabilitation Engineering},
title={Design of a Virtual Reality System for Affect Analysis in Facial Expressions (VR-SAAFE); Application to Schizophrenia},
year={2017},
volume={25},
number={6},
pages={739-749},
abstract={Schizophrenia is a life-long, debilitating psychotic disorder with poor outcome that affects about 1% of the population. Although pharmacotherapy can alleviate some of the acute psychotic symptoms, residual social impairments present a significant barrier that prevents successful rehabilitation. With limited resources and access to social skills training opportunities, innovative technology has emerged as a potentially powerful tool for intervention. In this paper, we present a novel virtual reality (VR)-based system for understanding facial emotion processing impairments that may lead to poor social outcome in schizophrenia. We henceforth call it a VR System for Affect Analysis in Facial Expressions (VR-SAAFE). This system integrates a VR-based task presentation platform that can minutely control facial expressions of an avatar with or without accompanying verbal interaction, with an eye-tracker to quantitatively measure a participants real-time gaze and a set of physiological sensors to infer his/her affective states to allow in-depth understanding of the emotion recognition mechanism of patients with schizophrenia based on quantitative metrics. A usability study with 12 patients with schizophrenia and 12 healthy controls was conducted to examine processing of the emotional faces. Preliminary results indicated that there were significant differences in the way patients with schizophrenia processed and responded towards the emotional faces presented in the VR environment compared with healthy control participants. The preliminary results underscore the utility of such a VR-based system that enables precise and quantitative assessment of social skill deficits in patients with schizophrenia.},
keywords={biomedical optical imaging;emotion recognition;face recognition;medical disorders;medical image processing;virtual reality;Virtual Reality System for Affect Analysis in Facial Expressions;VR-SAAFE;schizophrenia;psychotic disorder;pharmacotherapy;psychotic symptoms;facial emotion processing;VR-based task presentation platform;eye-tracker;physiological sensors;emotion recognition mechanism;emotional face processing;social skill deficit assessment;Biomedical monitoring;Emotion recognition;Training;Monitoring;Avatars;Atmospheric measurements;Affective computing;eye gaze;physiology;schizophrenia therapy;virtual reality (VR);Adult;Affect;Diagnosis, Computer-Assisted;Facial Expression;Female;Humans;Male;Middle Aged;Photic Stimulation;Reproducibility of Results;Schizophrenia;Schizophrenic Psychology;Sensitivity and Specificity;User-Computer Interface},
doi={10.1109/TNSRE.2016.2591556},
ISSN={},
month={June},}
@INPROCEEDINGS{7145116,
author={M. {Radecký} and J. {Vykopal} and P. {Smutný}},
booktitle={Proceedings of the 2015 16th International Carpathian Control Conference (ICCC)},
title={Analysis of syntactic elements and structure of web pages using eye-tracking technology},
year={2015},
volume={},
number={},
pages={420-425},
abstract={As web pages are becoming increasingly complex, with images and different page layouts, understanding how users examine the page is important. We used eye-tracking analysis of user behavior and performance in the use of web. Using GP3 Eye Tracker we created tracking system, which take into account syntactic elements and content structure of the tested web pages.},
keywords={computer vision;gaze tracking;user interfaces;Web sites;syntactic elements;Web pages;eye-tracking technology;user behavior;GP3 eye tracker;Tracking;IP networks;Heating;Extraterrestrial measurements;eye-tracking;user interface;syntactic analysis},
doi={10.1109/CarpathianCC.2015.7145116},
ISSN={},
month={May},}
@ARTICLE{8248746,
author={A. H. {Memar} and E. T. {Esfahani}},
journal={IEEE Access},
title={Physiological Measures for Human Performance Analysis in Human-Robot Teamwork: Case of Tele-Exploration},
year={2018},
volume={6},
number={},
pages={3694-3705},
abstract={Continuous monitoring of mental workload and situation awareness in operational environments is useful for understanding and prediction of human performance. Such information can be used to develop real-time adaptive systems to enhance human performance. In this paper, we investigate the use of work load- and attention-related physiological measures to predict operator performance and situation awareness in the context of tele-exploration with a small team of robots. A user study is conducted based on a simulated scenario involving visual scanning and manual control tasks with varying levels of task-load. Brain activity and eye movements of the participants are monitored across the experimental tasks using electroencephalogram and eye tracker sensors. The performances of the subjects are evaluated in terms of target detection and situation awareness (primary metrics) as well as reaction time and false detection (secondary metrics). Moreover, individual differences in two specific visual skills, visual search (VS) and multi-object tracking (MOT) are considered as between-subject factors in the experimental design. The main effects of task type and individual differences reveal that VS and MOT skill have significant effects on target detection and situation awareness, respectively. The correlations of physiological measures with the task performance and situation awareness are analyzed. The results suggest that brain-based features (mental workload and distraction) which represent the covert aspect of attention are better suited to predict the secondary performance metrics. On the other hand, glance-based features which represent the overt aspect of attention are shown to be the best predictors of the primary performance metrics.},
keywords={brain;cognition;electroencephalography;gaze tracking;human factors;human-robot interaction;mobile robots;multi-robot systems;object detection;object tracking;physiology;robot vision;team working;real-time adaptive systems;situation awareness;tele-exploration;visual scanning;manual control tasks;eye movements;experimental tasks;electroencephalogram;eye tracker sensors;target detection;reaction time;false detection;specific visual skills;visual search;task type;task performance;mental workload;secondary performance metrics;primary performance metrics;human performance analysis;continuous monitoring;operational environments;human-robot teamwork;human performance enhancement;human performance prediction;operator performance prediction;brain activity;task-load level;multiobject tracking;MOT skill;physiological measure correlation;brain-based features;glance-based features;attention -related physiological measures;workload-related physiological measures;Robots;Biomedical monitoring;Brain modeling;Visualization;Electroencephalography;Human-robot interaction;human performance;individual differences;mental workload;physiological measures;situation awareness},
doi={10.1109/ACCESS.2018.2790838},
ISSN={},
month={},}
@INPROCEEDINGS{6251696,
author={C. {Torrey} and A. {Powers} and S. R. {Fussell} and S. {Kiesler}},
booktitle={2007 2nd ACM/IEEE International Conference on Human-Robot Interaction (HRI)},
title={Exploring adaptive dialogue based on a robot's awareness of human gaze and task progress},
year={2007},
volume={},
number={},
pages={247-254},
abstract={When a robot provides direction-as a guide, an assistant, or as an instructor-the robot may have to interact with people of different backgrounds and skill sets. Different people require information adapted to their level of understanding. In this paper, we explore the use of two simple forms of awareness that a robot might use to infer that a person needs further verbal elaboration during a tool selection task. First, the robot could use an eye tracker for inferring whether the person is looking at the robot and thus in need of further elaboration. Second, the robot could monitor delays in the individual's task progress, indicating that he or she could use further elaboration. We investigated the effects of these two types of awareness on performance time, selection mistakes, and the number of questions people asked the robot. We did not observe any obvious benefits of our gaze awareness manipulation. Awareness of task delays did reduce the number of questions participants' asked compared to our control condition but did not significantly reduce the number of selection mistakes. The mixed results of our investigation suggest that more research is necessary before we can understand how awareness of gaze and awareness of task delay can be successfully implemented in human-robot dialogue.},
keywords={delays;eye;human-robot interaction;interactive systems;object tracking;service robots;task analysis;robot awareness;human gaze;task progress;human robot interaction;eye tracker;delay monitoring;human-robot dialogue;Robots;Delay;Atmospheric measurements;Particle measurements;Grounding;Humans;Abstracts;Human-robot interaction;human-robot dialogue;adaptive dialogue;social robots},
doi={10.1145/1228716.1228750},
ISSN={},
month={March},}
@INPROCEEDINGS{7107603,
author={M. {Malčík} and E. {Mechlová} and Z. {Sikorová} and A. {Mentel}},
booktitle={2014 IEEE 12th IEEE International Conference on Emerging eLearning Technologies and Applications (ICETA)},
title={Analysis of visual perceptual patterns on screen using eyetracker technology},
year={2014},
volume={},
number={},
pages={311-317},
abstract={According to current researches, individual differences in eye movement patterns may be related to different ways of text processing, to comprehension difficulties, etc. They also depend on the nature of the text. Particular question is how reading takes place in case of using figurative (visual) language, e.g. in case of metaphors, irony, or fixed phrases (phraseologisms). Based on other researches, it is evident that the development level of executive functions of each individual is strongly tied to their sociocultural background, especially to the parenting style of their family; on the other hand, differences in executive function significantly affect a person's performance at school. The notable part for the aim of this paper, however, is the direct link between executive functions and eye movement, i.e. on the basis of eye tracking it is possible to make direct, unaltered conclusions on executive functions [1]. Electronic screens on laptop and tablet computers and smartphones are being used for reading text, often while multitasking [14]. The question arises whether reading from a screen is as effective as the reading from textbooks. One of the aims of the paper is to verify the visual patterns of information retrieval from screen for problem comprehension and solving using eyetracking technology.},
keywords={computer aided instruction;computer literacy;gaze tracking;information retrieval;screens (display);reading literacy;information retrieval;smartphones;tablet computers;laptop;electronic screens;executive functions;sociocultural background;figurative language;text processing;eye movement patterns;eye tracker technology;visual perceptual pattern analysis;Visualization;Tracking;Art;Testing;Conferences;Electronic learning;eyetracking;reading literacy;learning style;perceptual patterns},
doi={10.1109/ICETA.2014.7107603},
ISSN={},
month={Dec},}