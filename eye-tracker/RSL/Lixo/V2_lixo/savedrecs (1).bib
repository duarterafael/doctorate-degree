
@article{ ISI:000489469200004,
Author = {Feng, Yuanyuan and McGowan, Hannah and Semsar, Azin and Zahiri, Hamid R.
   and George, Ivan M. and Park, Adrian and Kleinsmith, Andrea and Mentis,
   Helena},
Title = {{Virtual pointer for gaze guidance in laparoscopic surgery}},
Journal = {{SURGICAL ENDOSCOPY AND OTHER INTERVENTIONAL TECHNIQUES}},
Abstract = {{Background A challenge of laparoscopic surgery is learning how to
   interpret the indirect view of the operative field. Acquiring
   professional vision-understanding what to see and which information to
   attend to, is thereby an essential part of laparoscopic training and one
   in which trainers exert great effort to convey. We designed a virtual
   pointer (VP) that enables experts to point or draw free-hand sketches
   over an intraoperative laparoscopic video for a novice to see. This
   study aimed to investigate the efficacy of the virtual pointer in
   guiding novices' gaze patterns. Methods We conducted a counter-balanced,
   within-subject trial to compare the novices' gaze behaviors in
   laparoscopic training with the virtual pointer compared to a standard
   training condition, i.e., verbal instruction with un-mediated gestures.
   In the study, seven trainees performed four simulated laparoscopic tasks
   guided by an experienced surgeon as the trainer. A Tobii Pro X3-120
   eye-tracker was used to capture the trainees' eye movements. The
   measures include fixation rate, i.e., the frequency of trainees'
   fixations, saccade amplitude, and fixation concentration, i.e., the
   closeness of trainees' fixations. Results No significant difference in
   fixation rate or saccade amplitude was found between the virtual pointer
   condition and the standard condition. In the virtual pointer condition,
   trainees' fixations were more concentrated (p = 0.039) and longer
   fixations were more clustered, compared to the Standard condition (p =
   0.008). Conclusions The virtual pointer effectively improved surgical
   trainees' in-the-moment gaze focus during the laparoscopic training by
   reducing their gaze dispersion and concentrating their attention on the
   anatomical target. These results suggest that technologies which support
   gaze training should be expert-driven and intraoperative to efficiently
   modify novices' gaze behaviors.}},
Publisher = {{SPRINGER}},
Address = {{233 SPRING ST, NEW YORK, NY 10013 USA}},
Type = {{Article; Early Access}},
Language = {{English}},
Affiliation = {{Feng, YY (Reprint Author), Univ Maryland Baltimore Cty, Dept Informat Syst, Baltimore, MD 21228 USA.
   Feng, Yuanyuan; McGowan, Hannah; Semsar, Azin; Kleinsmith, Andrea; Mentis, Helena, Univ Maryland Baltimore Cty, Dept Informat Syst, Baltimore, MD 21228 USA.
   Zahiri, Hamid R.; Park, Adrian, Anna Arundel Med Ctr, Dept Surg, Annapolis, MD USA.
   George, Ivan M., Johns Hopkins Med, Dept Surg, Baltimore, MD USA.}},
DOI = {{10.1007/s00464-019-07141-x}},
Early Access Date = {{OCT 2019}},
ISSN = {{0930-2794}},
EISSN = {{1432-2218}},
Keywords = {{Laparoscopic training; Gaze guidance; Intraoperative video annotation;
   Eye tracking; Fixation concentration}},
Research-Areas = {{Surgery}},
Web-of-Science-Categories  = {{Surgery}},
Author-Email = {{fengy1@umbc.edu}},
Funding-Acknowledgement = {{National Science FoundationNational Science Foundation (NSF) {[}IIS
   \#1422671, 1552837]}},
Funding-Text = {{This study is sponsored by National Science Foundation Grant IIS
   \#1422671 and \#1552837.}},
Number-of-Cited-References = {{25}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{1}},
Journal-ISO = {{Surg. Endosc.}},
Doc-Delivery-Number = {{JC7OY}},
Unique-ID = {{ISI:000489469200004}},
DA = {{2019-10-28}},
}

@article{ ISI:000484567800001,
Author = {Cutumisu, Maria and Turgeon, Krystle-Lee and Saiyera, Tasbire and
   Chuong, Steven and Esparza, Lydia Marion Gonzalez and MacDonald, Rob and
   Kokhan, Vasyl},
Title = {{Eye Tracking the Feedback Assigned to Undergraduate Students in a
   Digital Assessment Game}},
Journal = {{FRONTIERS IN PSYCHOLOGY}},
Year = {{2019}},
Volume = {{10}},
Month = {{SEP 6}},
Abstract = {{High-quality feedback exerts a crucial influence on learning new skills
   and it is one of the most common psychological interventions. However,
   knowing how to deliver feedback effectively is challenging for educators
   in both traditional and online classroom environments. This study uses
   psychophysiological methodology to investigate attention allocation to
   different feedback valences (i.e., positive and negative feedback), as
   the eye tracker provides accurate information about individuals' locus
   of attention when they process feedback. We collected learning analytics
   via a behavioral assessment game and eye-movement measures via an eye
   tracker to infer undergraduate students' cognitive processing of
   feedback that is assigned to them after completing a task. The eye
   movements of n = 30 undergraduates at a university in Western Canada
   were tracked by the EyeLink 1000 Plus eye tracker while they played
   Posterlet, a digital game-based assessment. In Posterlet, students
   designed three posters and received critical (negative) or confirmatory
   (positive) feedback from virtual characters in the game after completing
   each poster. Analyses showed that, overall, students attended to
   critical feedback more than to confirmatory feedback, as measured by the
   time spent on feedback in total, per word, and per letter, and by the
   number of feedback fixations and revisits. However, there was no
   difference in dwell time between valences prior to any feedback
   revisits, suggesting that returning to read critical feedback more often
   than confirmatory feedback accounts for the overall dwell time
   difference between valences when feedback is assigned to students. The
   study summarizes the eye movement record on critical and confirmatory
   feedback, respectively. Implications of this research include enhancing
   our understanding of the differential temporal cognitive processing of
   feedback valences that may ultimately improve the delivery of feedback.}},
Publisher = {{FRONTIERS MEDIA SA}},
Address = {{AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Cutumisu, M (Reprint Author), Univ Alberta, Ctr Res Appl Measurement \& Evaluat, Dept Educ Psychol, Edmonton, AB, Canada.
   Cutumisu, Maria; Turgeon, Krystle-Lee; Saiyera, Tasbire; Chuong, Steven; Esparza, Lydia Marion Gonzalez; MacDonald, Rob; Kokhan, Vasyl, Univ Alberta, Ctr Res Appl Measurement \& Evaluat, Dept Educ Psychol, Edmonton, AB, Canada.}},
DOI = {{10.3389/fpsyg.2019.01931}},
Article-Number = {{1931}},
ISSN = {{1664-1078}},
Keywords = {{eye tracking; eye movement; error processing; feedback; game-based
   assessment}},
Keywords-Plus = {{DESIGN; ATTENTION; MOVEMENTS}},
Research-Areas = {{Psychology}},
Web-of-Science-Categories  = {{Psychology, Multidisciplinary}},
Author-Email = {{cutumisu@ualberta.ca}},
Funding-Acknowledgement = {{Centre for Mathematics, Science, and Technology Education (CMASTE) at
   the University of Alberta; Social Sciences and Humanities Research
   Council of Canada - Insight Development Grant (SSHRC IDG)
   {[}RES0034954]; Natural Sciences and Engineering Research Council (NSERC
   DG)Natural Sciences and Engineering Research Council of Canada
   {[}RES0043209]; Killam Cornerstone Operating Grant {[}RES0043207]}},
Funding-Text = {{We would like to thank the Centre for Mathematics, Science, and
   Technology Education (CMASTE) at the University of Alberta, the Social
   Sciences and Humanities Research Council of Canada - Insight Development
   Grant (SSHRC IDG) RES0034954, the Natural Sciences and Engineering
   Research Council (NSERC DG) RES0043209, and the Killam Cornerstone
   Operating Grant RES0043207 for supporting this research.}},
Number-of-Cited-References = {{38}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Journal-ISO = {{Front. Psychol.}},
Doc-Delivery-Number = {{IV9EX}},
Unique-ID = {{ISI:000484567800001}},
OA = {{DOAJ Gold}},
DA = {{2019-10-28}},
}

@article{ ISI:000487964600203,
Author = {Steinhauser, Johann and Janssen, Meike and Hamm, Ulrich},
Title = {{Who Buys Products with Nutrition and Health Claims? A Purchase
   Simulation with Eye Tracking on the Influence of Consumers' Nutrition
   Knowledge and Health Motivation}},
Journal = {{NUTRIENTS}},
Year = {{2019}},
Volume = {{11}},
Number = {{9}},
Month = {{SEP}},
Abstract = {{Nutrition and health claims are seen as a way of promoting healthy
   aspects of food. However, the results of previous studies have been
   contradictory regarding the effect of these claims on purchase. This
   study aims to achieve a better understanding of how the consumer
   characteristics `nutrition knowledge' and `health motivation' influence
   the purchase of products with nutrition and health claims and what role
   gaze behavior plays. We included gaze behavior in our analysis, as
   visual attention on the claims is a precondition to its influence on the
   purchase decision. In a close-to-realistic shopping situation, consumers
   could choose from three-dimensional orange juice packages labeled with
   nutrition, health, and taste claims. In total, the sample consisted of
   156 consumers. The data were analyzed with a structural equation model
   (SEM), linking the purchase decision for products with claims to gaze
   data recorded with a mobile eye tracker and consumer and product-related
   variables collected via the questionnaire. Results showed that the
   variables in the SEM explained 31\% (8\%) of the variance observed in
   the purchase of products with a nutrition (health) claim. The longer a
   consumer looked at a specific claim, the more likely the consumer would
   purchase the respective product. The lower the price and the higher the
   perceived healthiness and tastiness of the product further heightened
   its likelihood of being purchased. Interestingly, consumers with higher
   nutrition knowledge and/or higher health motivation looked longer at the
   nutrition and health claims; however, these consumer characteristics did
   not show an effect on the purchase decision. Implications for policy
   makers and marketers are given.}},
Publisher = {{MDPI}},
Address = {{ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Janssen, M (Reprint Author), Univ Kassel, Fac Organ Agr Sci, Dept Agr \& Food Mkt, Steinstr 19, D-37213 Witzenhausen, Germany.
   Janssen, M (Reprint Author), Copenhagen Business Sch, Dept Management Soc \& Commun, Dalgas Have 15, DK-2000 Frederiksberg, Denmark.
   Steinhauser, Johann; Janssen, Meike; Hamm, Ulrich, Univ Kassel, Fac Organ Agr Sci, Dept Agr \& Food Mkt, Steinstr 19, D-37213 Witzenhausen, Germany.
   Janssen, Meike, Copenhagen Business Sch, Dept Management Soc \& Commun, Dalgas Have 15, DK-2000 Frederiksberg, Denmark.}},
DOI = {{10.3390/nu11092199}},
Article-Number = {{2199}},
EISSN = {{2072-6643}},
Keywords = {{health claims; nutrition knowledge; eye tracking; visual attention;
   consumer behavior; purchase decision}},
Keywords-Plus = {{NUTRIENT-CONTENT CLAIMS; WILLINGNESS-TO-PAY; VISUAL-ATTENTION;
   CONJOINT-ANALYSIS; FUNCTIONAL FOODS; PACKAGE CLAIMS; NEW-ZEALAND;
   INFORMATION; CHOICE; LABELS}},
Research-Areas = {{Nutrition \& Dietetics}},
Web-of-Science-Categories  = {{Nutrition \& Dietetics}},
Author-Email = {{j.steinhauser@uni-kassel.de
   mj.msc@cbs.dk
   hamm@uni-kassel.de}},
Number-of-Cited-References = {{139}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Journal-ISO = {{Nutrients}},
Doc-Delivery-Number = {{JA6PP}},
Unique-ID = {{ISI:000487964600203}},
OA = {{DOAJ Gold}},
DA = {{2019-10-28}},
}

@article{ ISI:000483806400005,
Author = {Arias-Trejo, Natalia and Angulo-Chavira, Armando Q. and Barron-Martinez,
   Julia B.},
Title = {{Verb-mediated anticipatory eye movements in people with Down syndrome}},
Journal = {{INTERNATIONAL JOURNAL OF LANGUAGE \& COMMUNICATION DISORDERS}},
Year = {{2019}},
Volume = {{54}},
Number = {{5}},
Pages = {{756-766}},
Month = {{SEP}},
Abstract = {{Background Children and adults with neurotypical development employ
   linguistic information to predict and anticipate information.
   Individuals with Down syndrome (DS) have weaknesses in language
   production and the domain of grammar but relative strengths in language
   comprehension and the domain of semantics. What is not clear is the
   extent to which they can use linguistic information, as it unfolds in
   real time, to anticipate upcoming information correctly. Aims To
   investigate whether children and young people with DS employ verb
   information to predict and anticipate upcoming linguistic information.
   Methods \& Procedures A preferential looking task was performed, using
   an eye-tracker, with children and teenagers with DS and a typically
   developing (TD) control group matched by sex and mental age (average =
   5.48 years). In each of 10 trials, two images were presented, a target
   and a distractor, while participants heard a phrase that contained a
   semantically informative verb (e.g., `eat') or an uninformative verb
   (e.g., `see'). Outcomes \& Results Both DS and TD control participants
   could anticipate the target upon hearing an informative verb, and
   prediction skills were positively correlated with mental age in those
   with DS. Conclusions \& Implications This work demonstrates for the
   first time that children and teenagers with DS can predict linguistic
   information based on semantic cues from verbs, and that sentence
   processing is driven by predictive relationships between verbs and
   arguments, as in children with typical development. Clinicians can take
   advantage of these prediction skills, using them in therapy to support
   weaker areas.}},
Publisher = {{WILEY}},
Address = {{111 RIVER ST, HOBOKEN 07030-5774, NJ USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Arias-Trejo, N (Reprint Author), Fac Psicol, Lab Psicolinguist, Av Univ 3000,Sotano Edificio C, Mexico City 04510, DF, Mexico.
   Arias-Trejo, Natalia; Barron-Martinez, Julia B., Univ Nacl Autonoma Mexico, Fac Psicol, Lab Psicolinguist, Mexico City, DF, Mexico.
   Angulo-Chavira, Armando Q., Univ Guadalajara, Inst Neurociencias, Guadalajara, Jalisco, Mexico.}},
DOI = {{10.1111/1460-6984.12473}},
ISSN = {{1368-2822}},
EISSN = {{1460-6984}},
Keywords = {{prediction; Down syndrome; verb comprehension; anticipation}},
Keywords-Plus = {{LANGUAGE-SKILLS; CHILDREN; MEMORY; BRAIN; PREDICTION; COMPREHENSION;
   ADOLESCENTS; VOCABULARY; ADULTS}},
Research-Areas = {{Audiology \& Speech-Language Pathology; Linguistics; Rehabilitation}},
Web-of-Science-Categories  = {{Audiology \& Speech-Language Pathology; Linguistics; Rehabilitation}},
Author-Email = {{nariast@unam.mx}},
Number-of-Cited-References = {{45}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{2}},
Usage-Count-Since-2013 = {{2}},
Journal-ISO = {{Int. J. Lang. Commun. Disord.}},
Doc-Delivery-Number = {{IU8BG}},
Unique-ID = {{ISI:000483806400005}},
DA = {{2019-10-28}},
}

@article{ ISI:000482697800001,
Author = {Robertson, Erin K. and Gallant, Jennifer E.},
Title = {{Eye tracking reveals subtle spoken sentence comprehension problems in
   children with dyslexia}},
Journal = {{LINGUA}},
Year = {{2019}},
Volume = {{228}},
Month = {{SEP}},
Abstract = {{Children with dyslexia who did not have SLI (n = 31) and
   typically-developing (TD, n = 31) children with similar oral language
   and nonverbal skills completed a spoken sentence-picture matching task
   while an eye tracker recorded fixations and dwell time. No group
   differences were found on accuracy - which was very high across both
   groups. However, there were online processing differences. The TD group
   made more target fixations and the dyslexic group made more fixations to
   syntax distractors. Time course analyses revealed that compared to the
   dyslexic group, the TD group looked longer at the target when sentences
   became unambiguous. Across groups, sentence accuracy, target fixations,
   and cumulative target dwell time were correlated. Word reading was
   correlated with sentence accuracy and both online sentence processing
   measures, but only in the dyslexic group. In conclusion, the eye
   tracking data uncovered more than the behavioral measures alone, and
   children with dyslexia showed subtle sentence comprehension difficulties
   compared to TD peers. (C) 2019 Elsevier B.V. All rights reserved.}},
Publisher = {{ELSEVIER}},
Address = {{RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Robertson, EK (Reprint Author), Cape Breton Univ, 1250 Grand Lake Rd, Sydney, NSW B1P 6L2, Australia.
   Robertson, Erin K., Cape Breton Univ, 1250 Grand Lake Rd, Sydney, NSW B1P 6L2, Australia.
   Gallant, Jennifer E., Univ New Brunswick, Fredericton, NB, Canada.}},
DOI = {{10.1016/j.lingua.2019.06.009}},
Article-Number = {{UNSP 102708}},
ISSN = {{0024-3841}},
EISSN = {{1872-6135}},
Keywords = {{Dyslexia; Children; Spoken sentence comprehension; Eye tracking}},
Keywords-Plus = {{SUBJECT-VERB AGREEMENT; LANGUAGE IMPAIRMENT; SYNTACTIC COMPREHENSION;
   DEVELOPMENTAL DYSLEXIA; INDIVIDUAL-DIFFERENCES; SPEECH-PERCEPTION;
   MEMORY; MORPHOLOGY; PHONOLOGY; DEFICITS}},
Research-Areas = {{Linguistics}},
Web-of-Science-Categories  = {{Linguistics; Language \& Linguistics}},
Author-Email = {{erin\_robertson@cbu.ca}},
Funding-Acknowledgement = {{Natural Sciences and Engineering Research Council of CanadaNatural
   Sciences and Engineering Research Council of Canada {[}402,411-2011];
   Canadian Foundation for InnovationCanada Foundation for Innovation; Nova
   Scotia Research Trust; Cape Breton University {[}30,088]}},
Funding-Text = {{Funding for this project was provided to the first author from the
   Natural Sciences and Engineering Research Council of Canada (Grant \#
   402,411-2011), the Canadian Foundation for Innovation, Nova Scotia
   Research Trust, and Cape Breton University (Grant \# 30,088).}},
Number-of-Cited-References = {{40}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{5}},
Usage-Count-Since-2013 = {{5}},
Journal-ISO = {{Lingua}},
Doc-Delivery-Number = {{IT2QZ}},
Unique-ID = {{ISI:000482697800001}},
DA = {{2019-10-28}},
}

@article{ ISI:000477965800001,
Author = {Strnadelova, Bronislava and Halamova, Julia and Kanovsky, Martin},
Title = {{Eye-tracking of Facial Emotions in Relation to Self-criticism and
   Self-reassurance}},
Journal = {{APPLIED ARTIFICIAL INTELLIGENCE}},
Year = {{2019}},
Volume = {{33}},
Number = {{10}},
Pages = {{839-862}},
Month = {{AUG 24}},
Abstract = {{The study explores the relation between participants' level of
   self-criticism, self-reassurance, and eye gaze when looking at
   photographs of primary emotions. Participants completed The Forms of
   Self-Criticising/Attacking \& Self-Reassuring Scale (FSCRS) and then a
   facial-emotion expression task while their eye movements were being
   recorded by an eye-tracker. The results indicate differences in people's
   eye-gaze patterns when viewing facial-emotion expressions in relation to
   the level of self-criticism and self-reassurance. Specifically,
   participants with higher self-reassurance look more frequently at the
   eye region and less frequently at other facial areas and beyond the
   emotional faces. However, individuals with higher self-hatred look at
   the outside of the face more frequently than at the eyes area, and
   higher self-inadequacy predicted the individual would look more
   frequently at the eyes than at other facial areas. The results are
   important for understanding the role of self-criticism in relation to
   facial-emotion expressions and gazing, as self-criticism is a key
   underlying factor of all kinds of psychopathologies. Following further
   research, the results could be used to develop more objective
   diagnostics for self-criticism screening than the existing self-rating
   scales.}},
Publisher = {{TAYLOR \& FRANCIS INC}},
Address = {{530 WALNUT STREET, STE 850, PHILADELPHIA, PA 19106 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Halamova, J (Reprint Author), Comenius Univ, Inst Appl Psychol, Fac Social \& Econ Sci, Mlynske Luhy 4, Bratislava 82105, Slovakia.
   Strnadelova, Bronislava; Halamova, Julia, Comenius Univ, Inst Appl Psychol, Fac Social \& Econ Sci, Mlynske Luhy 4, Bratislava 82105, Slovakia.
   Kanovsky, Martin, Comenius Univ, Inst Social Anthropol, Fac Social \& Econ Sci, Bratislava, Slovakia.}},
DOI = {{10.1080/08839514.2019.1646004}},
ISSN = {{0883-9514}},
EISSN = {{1087-6545}},
Keywords-Plus = {{CONFIRMATORY FACTOR-ANALYSIS; SOCIAL ANXIETY; R PACKAGE; FACE; GAZE;
   RECOGNITION; COMPASSION; FORMS; ADOLESCENTS; PERSONALITY}},
Research-Areas = {{Computer Science; Engineering}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Engineering, Electrical \&
   Electronic}},
Author-Email = {{julia.halamova@gmail.com}},
Number-of-Cited-References = {{73}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{4}},
Usage-Count-Since-2013 = {{4}},
Journal-ISO = {{Appl. Artif. Intell.}},
Doc-Delivery-Number = {{IM4LJ}},
Unique-ID = {{ISI:000477965800001}},
DA = {{2019-10-28}},
}

@article{ ISI:000487186800014,
Author = {Monica, Tichindelean (Beca) and Iuliana, Cetina and Mihai, Tichindelean},
Title = {{STUDYING THE USER EXPERIENCE IN ONLINE BANKING SERVICES: AN EYE-TRACKING
   APPLICATION}},
Journal = {{STUDIES IN BUSINESS AND ECONOMICS}},
Year = {{2019}},
Volume = {{14}},
Number = {{2}},
Pages = {{193-208}},
Month = {{AUG}},
Abstract = {{Neuromarketing as research method contrbutes to understanding consumer
   behavior a step further than traditional marketing research. The aim of
   the current study is to explore the usability and cognitive
   understanding of banking services webpages. In this regard, the
   theoretical part of the article reviews the relevant literature related
   to neuromarketing as research method and the use of eye-tracker as
   research technique. Further on, a two-step research design was developed
   for studying the consumers' attention and memory during and after
   viewing two Romanian banking services websites. The results showed that
   the way information is structured and presented on the webpages
   influence their usabiliy and cognitive understanding.}},
Publisher = {{SCIENDO}},
Address = {{DE GRUYTER POLAND SP Z O O, BOGUMILA ZUGA 32A STR, 01-811 WARSAW, POLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Monica, T (Reprint Author), Bucharest Univ Econ Studies, Bucharest, Romania.
   Monica, Tichindelean (Beca); Iuliana, Cetina, Bucharest Univ Econ Studies, Bucharest, Romania.
   Mihai, Tichindelean, Lucian Blaga Univ Sibiu, Sibiu, Romania.}},
DOI = {{10.2478/sbe-2019-0034}},
ISSN = {{1842-4120}},
EISSN = {{2344-5416}},
Keywords = {{consumer attention; neuromarketing; eye-tracker; banking services;
   consumer memory}},
Research-Areas = {{Business \& Economics}},
Web-of-Science-Categories  = {{Economics}},
Number-of-Cited-References = {{14}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{2}},
Usage-Count-Since-2013 = {{2}},
Journal-ISO = {{Stud. Bus. Econ.}},
Doc-Delivery-Number = {{IZ6JE}},
Unique-ID = {{ISI:000487186800014}},
OA = {{DOAJ Gold}},
DA = {{2019-10-28}},
}

@article{ ISI:000475694200012,
Author = {Ballenghein, Ugo and Baccino, Thierry},
Title = {{Referential processing during reading: concurrent recordings of eye
   movements and head motion}},
Journal = {{COGNITIVE PROCESSING}},
Year = {{2019}},
Volume = {{20}},
Number = {{3}},
Pages = {{371-384}},
Month = {{AUG}},
Abstract = {{The present study utilized a new experimental set-up synchronizing eye
   movements and head motion for investigating referential change occurring
   in a reading task. We examined the effects of a change in narrative
   perspective during reading on eye movements and head motion. Forty-four
   participants read texts on a digital tablet, and both participants' eye
   movements and head movements were recorded using eye tracker and motion
   capture. The results showed longer eye fixation duration, longer reading
   time and decreasing head motions when perspective changed. Recent
   studies have supported the dynamic engagement hypothesis suggesting that
   there is a fluctuation in cognitive engagement reflected by postural
   movements. Our findings on head movements seem to validate this
   hypothesis of a bodily engagement in reading. The results provided by
   our study showed that the novel methodology of eye and head movement
   recordings we used was proven to be informative in studying the reader's
   embodiment during reading.}},
Publisher = {{SPRINGER HEIDELBERG}},
Address = {{TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Ballenghein, U; Baccino, T (Reprint Author), Univ Paris 08, Dept Psychol, 2 Rue Liberte, F-93526 St Denis, France.
   Ballenghein, U; Baccino, T (Reprint Author), CHART LUTIN Lab, Paris, France.
   Ballenghein, Ugo; Baccino, Thierry, Univ Paris 08, Dept Psychol, 2 Rue Liberte, F-93526 St Denis, France.
   Ballenghein, Ugo; Baccino, Thierry, CHART LUTIN Lab, Paris, France.}},
DOI = {{10.1007/s10339-018-0894-1}},
ISSN = {{1612-4782}},
EISSN = {{1612-4790}},
Keywords = {{Eye movements; Head motion; Referential processing; Narrative
   perspective}},
Keywords-Plus = {{POSTURAL CONTROL; MENTAL MODELS; INFORMATION; MEMORY; COMPREHENSION;
   TASK; REPRESENTATION; EMBODIMENT; DIFFICULTY; ENGAGEMENT}},
Research-Areas = {{Psychology}},
Web-of-Science-Categories  = {{Psychology, Experimental}},
Author-Email = {{ugo.ballenghein@univ-paris8.fr
   thierry.baccino@univ-paris8.fr}},
ORCID-Numbers = {{Baccino, Thierry/0000-0002-2406-5306}},
Number-of-Cited-References = {{44}},
Times-Cited = {{1}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Journal-ISO = {{Cogn. Process.}},
Doc-Delivery-Number = {{IJ1YD}},
Unique-ID = {{ISI:000475694200012}},
DA = {{2019-10-28}},
}

@article{ ISI:000478364100001,
Author = {Strnadelova, Bronislava and Halamova, Julia and Kanovsky, Martin},
Title = {{Eye-tracking of Facial Emotions in Relation to Self-criticism and
   Self-reassurance}},
Journal = {{APPLIED ARTIFICIAL INTELLIGENCE}},
Abstract = {{The study explores the relation between participants' level of
   self-criticism, self-reassurance, and eye gaze when looking at
   photographs of primary emotions. Participants completed The Forms of
   Self-Criticising/Attacking \& Self-Reassuring Scale (FSCRS) and then a
   facial-emotion expression task while their eye movements were being
   recorded by an eye-tracker. The results indicate differences in people's
   eye-gaze patterns when viewing facial-emotion expressions in relation to
   the level of self-criticism and self-reassurance. Specifically,
   participants with higher self-reassurance look more frequently at the
   eye region and less frequently at other facial areas and beyond the
   emotional faces. However, individuals with higher self-hatred look at
   the outside of the face more frequently than at the eyes area, and
   higher self-inadequacy predicted the individual would look more
   frequently at the eyes than at other facial areas. The results are
   important for understanding the role of self-criticism in relation to
   facial-emotion expressions and gazing, as self-criticism is a key
   underlying factor of all kinds of psychopathologies. Following further
   research, the results could be used to develop more objective
   diagnostics for self-criticism screening than the existing self-rating
   scales.}},
Publisher = {{TAYLOR \& FRANCIS INC}},
Address = {{530 WALNUT STREET, STE 850, PHILADELPHIA, PA 19106 USA}},
Type = {{Article; Early Access}},
Language = {{English}},
Affiliation = {{Halamova, J (Reprint Author), Comenius Univ, Fac Social \& Econ Sci, Inst Appl Psychol, Mlynske Iuhy 4, Bratislava 82105, Slovakia.
   Strnadelova, Bronislava; Halamova, Julia, Comenius Univ, Fac Social \& Econ Sci, Inst Appl Psychol, Mlynske Iuhy 4, Bratislava 82105, Slovakia.
   Kanovsky, Martin, Comenius Univ, Fac Social \& Econ Sci, Inst Social Anthropol, Bratislava, Slovakia.}},
DOI = {{10.1080/08839514.2019.1646004}},
Early Access Date = {{JUL 2019}},
ISSN = {{0883-9514}},
EISSN = {{1087-6545}},
Keywords-Plus = {{CONFIRMATORY FACTOR-ANALYSIS; SOCIAL ANXIETY; R PACKAGE; FACE; GAZE;
   RECOGNITION; COMPASSION; FORMS; ADOLESCENTS; PERSONALITY}},
Research-Areas = {{Computer Science; Engineering}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Engineering, Electrical \&
   Electronic}},
Author-Email = {{julia.halamova@gmail.com}},
Funding-Acknowledgement = {{Vedecka grantova agentura VEGA {[}1/0075/19]}},
Funding-Text = {{Writing this work was supported by the Vedecka grantova agentura VEGA
   under Grant 1/0075/19.}},
Number-of-Cited-References = {{73}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{4}},
Usage-Count-Since-2013 = {{4}},
Journal-ISO = {{Appl. Artif. Intell.}},
Doc-Delivery-Number = {{IN0HJ}},
Unique-ID = {{ISI:000478364100001}},
DA = {{2019-10-28}},
}

@article{ ISI:000484979500014,
Author = {Nayar, Kritika and McKinney, Walker and Hogan, Abigail L. and Martin,
   Gary E. and La Valle, Chelsea and Sharp, Kevin and Berry-Kravis,
   Elizabeth and Norton, Elizabeth S. and Gordon, Peter C. and Losh, Molly},
Title = {{Language processing skills linked to FMR1 variation: A study of
   gaze-language coordination during rapid automatized naming among women
   with the FMR1 premutation}},
Journal = {{PLOS ONE}},
Year = {{2019}},
Volume = {{14}},
Number = {{7}},
Month = {{JUL 26}},
Abstract = {{The FMR1 premutation (PM) is relatively common in the general
   population. Evidence suggests that PM carriers may exhibit subtle
   differences in specific cognitive and language abilities. This study
   examined potential mechanisms underlying such differences through the
   study of gaze and language coordination during a language processing
   task (rapid automatized naming; RAN) among female carriers of the FMR1
   PM. RAN taps a complex set of underlying neuropsychological mechanisms,
   with breakdowns implicating processing disruptions in fundamental skills
   that support higher order language and executive functions, making RAN
   (and analysis of gaze/language coordination during RAN) a potentially
   powerful paradigm for revealing the phenotypic expression of the FMR1
   PM. Forty-eight PM carriers and 56 controls completed RAN on an eye
   tracker, where they serially named arrays of numbers, letters, colors,
   and objects. Findings revealed a pattern of inefficient language
   processing in the PM group, including a greater number of eye fixations
   (namely, visual regressions) and reduced eye-voice span (i.e., the eyes'
   lead over the voice) relative to controls. Differences were driven by
   performance in the latter half of the RAN arrays, when working memory
   and processing load are the greatest, implicating executive skills. RAN
   deficits were associated with broader social-communicative difficulties
   among PM carriers, and with FMR1-related molecular genetic variation
   (higher CGG repeat length, lower activation ratio, and increased levels
   of the fragile X mental retardation protein; FMRP). Findings contribute
   to an understanding of the neurocognitive profile of PM carriers and
   indicate specific gene-behavior associations that implicate the role of
   the FMR1 gene in language-related processes.}},
Publisher = {{PUBLIC LIBRARY SCIENCE}},
Address = {{1160 BATTERY STREET, STE 100, SAN FRANCISCO, CA 94111 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Losh, M (Reprint Author), Northwestern Univ, Roxelyn \& Richard Pepper Dept Commun Sci \& Disord, Evanston, IL 60208 USA.
   Nayar, Kritika; McKinney, Walker; Hogan, Abigail L.; La Valle, Chelsea; Norton, Elizabeth S.; Losh, Molly, Northwestern Univ, Roxelyn \& Richard Pepper Dept Commun Sci \& Disord, Evanston, IL 60208 USA.
   McKinney, Walker, Univ Kansas, Clin Child Psychol Program, Lawrence, KS 66045 USA.
   Hogan, Abigail L., Univ South Carolina, Psychol, Columbia, SC 29208 USA.
   Martin, Gary E., St Johns Univ, Commun Sci \& Disorders, Queens, NY USA.
   La Valle, Chelsea, Boston Univ, Psychol, Boston, MA 02215 USA.
   Sharp, Kevin; Berry-Kravis, Elizabeth, Rush Univ, Pediat, Med Ctr, Chicago, IL 60612 USA.
   Gordon, Peter C., Univ North Carolina Chapel Hill, Psychol \& Neurosci, Chapel Hill, NC USA.}},
DOI = {{10.1371/journal.pone.0219924}},
Article-Number = {{e0219924}},
ISSN = {{1932-6203}},
Keywords-Plus = {{FRAGILE-X-SYNDROME; WORKING-MEMORY CAPACITY; BROAD AUTISM PHENOTYPE;
   TREMOR/ATAXIA-SYNDROME; EYE-MOVEMENTS; CURVILINEAR ASSOCIATION;
   EXECUTIVE FUNCTIONS; PRAGMATIC LANGUAGE; SPEED PROCESSES; CGG EXPANSIONS}},
Research-Areas = {{Science \& Technology - Other Topics}},
Web-of-Science-Categories  = {{Multidisciplinary Sciences}},
Author-Email = {{m-losh@northwestern.edu}},
Funding-Acknowledgement = {{National Institutes of HealthUnited States Department of Health \& Human
   ServicesNational Institutes of Health (NIH) - USA {[}R01DC010191,
   R01MH091131, P30 HD03110]}},
Funding-Text = {{This research was supported by grants from the National Institutes of
   Health (R01DC010191, R01MH091131, to ML; and P30 HD03110). The funders
   had no role in study design, data collection and analysis, decision to
   publish, or preparation of the manuscript.}},
Number-of-Cited-References = {{117}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Journal-ISO = {{PLoS One}},
Doc-Delivery-Number = {{IW4WC}},
Unique-ID = {{ISI:000484979500014}},
OA = {{DOAJ Gold, Green Published}},
DA = {{2019-10-28}},
}

@article{ ISI:000487796600001,
Author = {Yow, W. Quin and Priyashri, Sridhar},
Title = {{Computerized Electronic Features Direct Children's Attention to Print in
   Single- and Dual-Language e-Books}},
Journal = {{AERA OPEN}},
Year = {{2019}},
Volume = {{5}},
Number = {{3}},
Month = {{JUL}},
Abstract = {{There has been a rapid proliferation of electronic books in recent
   years. Given that dual-language books may impose extra cognitive load on
   children's information processing capacity, we investigated whether
   multimedia features in e-books (i.e., audio narration and tracking
   animation) were effective in directing preschoolers' attention to print
   in the target language. Seventy-one English-and-Mandarin speaking 4- to
   6-year-olds were presented with single-language and dual-language
   e-books (with and without enhancing features) on a computer fitted with
   an eye tracker to read independently. Children attended more to their
   dominant language text than the other when reading dual-language texts
   silently but comparably in single-language texts. Most importantly,
   enhancing electronic features with synchronized dual-channel (visual and
   audio) inputs and attention-guiding cues effectively directed children's
   attention to print in both their dominant and nondominant languages.
   These findings provide important implications on how computerized
   electronic books affect children's attention to print and in turn
   support the development of children's emergent literacy.}},
Publisher = {{SAGE PUBLICATIONS INC}},
Address = {{2455 TELLER RD, THOUSAND OAKS, CA 91320 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Yow, WQ (Reprint Author), Singapore Univ Technol \& Design, Singapore, Singapore.
   Yow, W. Quin; Priyashri, Sridhar, Singapore Univ Technol \& Design, Singapore, Singapore.}},
DOI = {{10.1177/2332858419878126}},
Article-Number = {{UNSP 2332858419878126}},
EISSN = {{2332-8584}},
Keywords = {{reading; electronic books; bilingual education; attention to print}},
Keywords-Plus = {{EMERGENT LITERACY; CD-ROM; EYE-TRACKING; INTERACTIVE STORYBOOK;
   READING-COMPREHENSION; VISUAL-ATTENTION; COGNITIVE LOAD; PUPILS RECALL;
   SOCIALIZATION; KINDERGARTEN}},
Research-Areas = {{Education \& Educational Research}},
Web-of-Science-Categories  = {{Education \& Educational Research}},
Funding-Acknowledgement = {{SUTD-MIT IDC grant {[}IDG31100106, IDD41100104]}},
Funding-Text = {{We are grateful to the children and parents who participated and to the
   teachers and staff members of the Creative O Preschoolers' Bay and Red
   SchoolHouse. We thank Ferninda Patrycia for her help in experiment
   preparation and data collection, and Jessica Tan for her help in the
   early analysis and manuscript preparation. Portions of this work were
   previously presented at the American Education Research Association
   Conference 2016. This work was partially supported by the SUTD-MIT IDC
   grant (IDG31100106 and IDD41100104) awarded to the first author.}},
Number-of-Cited-References = {{76}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{1}},
Journal-ISO = {{AERA Open}},
Doc-Delivery-Number = {{JA4KG}},
Unique-ID = {{ISI:000487796600001}},
OA = {{DOAJ Gold}},
DA = {{2019-10-28}},
}

@article{ ISI:000472692800027,
Author = {Nelson, Nicole L. and Mondloch, Catherine J.},
Title = {{Children's perception of emotions in the context of live interactions:
   Eye movements and emotion judgements}},
Journal = {{BEHAVIOURAL PROCESSES}},
Year = {{2019}},
Volume = {{164}},
Pages = {{193-200}},
Month = {{JUL}},
Abstract = {{Research examining children's understanding of emotional expressions has
   generally used static, isolated facial expressions presented in a
   non-interactive context. However, these tasks do not resemble children's
   experiences with expressions in daily life, where they must attend to a
   range of information, including others' facial expressions, movements,
   and the situation surrounding the expression. In this research, we
   examine the development of visual attention to another's emotional
   expressions during a live interaction. Via an eye-tracker, children
   (4-11 years old) and adults viewed an experimenter open a series of
   opaque boxes and make an expression (happiness, sadness, fear, or
   disgust) based on the object inside. Participants determined which of
   four possible objects (stickers, a broken toy, a spider, or dog poop)
   was in the box. We examined the proportion of the trial in which
   participants looked to three areas of the face (the eyes, mouth, and
   nose area), and the available contextual information (the box held by
   the experimenter, the four objects). Although children spent less time
   looking to the face than adults did, their pattern of visual attention
   within the face and to object AOIs did not differ from that of adults.
   Finally, for adults, increased accuracy was linked to spending less time
   looking to the objects whereas increased accuracy for children was not
   strongly linked to any emotion cue. These data indicate that although
   children spend less time looking to the face during live interactions
   than adults do, the proportion of time spent looking to areas of the
   face and context are generally adult-like.}},
Publisher = {{ELSEVIER SCIENCE BV}},
Address = {{PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Mondloch, CJ (Reprint Author), Brock Univ, Dept Psychol, 1812 Sir Isaac Brock Way, St Catharines, ON L2S 3A1, Canada.
   Nelson, Nicole L.; Mondloch, Catherine J., Brock Univ, Dept Psychol, 1812 Sir Isaac Brock Way, St Catharines, ON L2S 3A1, Canada.
   Nelson, Nicole L., Univ Queensland, Sch Psychol, Brisbane, Qld, Australia.}},
DOI = {{10.1016/j.beproc.2019.05.006}},
ISSN = {{0376-6357}},
EISSN = {{1872-8308}},
Keywords = {{Emotion; Development; Facial expressions; Eye-tracking; Interactive;
   Social cognition}},
Keywords-Plus = {{BEHAVIOR SYSTEMS VIEW; FACIAL EXPRESSIONS; RECOGNITION; FACES;
   DIFFERENTIATION; ATTENTION; RESPONSES; TRACKING; LOOKING; LONG}},
Research-Areas = {{Psychology; Behavioral Sciences; Zoology}},
Web-of-Science-Categories  = {{Psychology, Biological; Behavioral Sciences; Zoology}},
Author-Email = {{cmondloch@brocku.ca}},
ResearcherID-Numbers = {{Nelson, Nicole/P-1078-2014}},
ORCID-Numbers = {{Nelson, Nicole/0000-0001-8299-4798}},
Funding-Acknowledgement = {{SSHRCSocial Sciences and Humanities Research Council of Canada (SSHRC);
   Canada Foundation for InnovationCanada Foundation for Innovation; NSF
   IRFPNational Science Foundation (NSF)}},
Funding-Text = {{This research was funded by a SSHRC Insight grant and Canada Foundation
   for Innovation grant to CJM and an NSF IRFP awarded to NN. We thank
   Breanna Elliott and Jasmine Bender for helping to pose faces, Jasmine
   Bender, Harmonie Chan, Lewis Lau, Allison Mondloch, and Gabriela
   Saldago, for helping to process data, Bryce Hunt for technical
   assistance, and the parents and children who participated.}},
Number-of-Cited-References = {{61}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{6}},
Usage-Count-Since-2013 = {{6}},
Journal-ISO = {{Behav. Processes}},
Doc-Delivery-Number = {{IE9KH}},
Unique-ID = {{ISI:000472692800027}},
DA = {{2019-10-28}},
}

@article{ ISI:000479241000011,
Author = {Michael, Ian and Ramsoy, Thomas and Stephens, Melodena and Kotsi,
   Filareti},
Title = {{A study of unconscious emotional and cognitive responses to tourism
   images using a neuroscience method}},
Journal = {{JOURNAL OF ISLAMIC MARKETING}},
Year = {{2019}},
Volume = {{10}},
Number = {{2}},
Pages = {{543-564}},
Month = {{JUN 10}},
Abstract = {{Purpose This applied neuroscience study aims to understand how direct
   and unconscious emotional and cognitive responses underlie travel
   destination preferences. State-of-the-art neuroscience tools and methods
   were used, including stationary eye tracking and brain scanning
   electroencephalography (EEG) to assess emotional and cognitive responses
   to destination images and assets. To the researchers' knowledge, this
   study is the first applied neuroscience study in tourism research and
   thus opens a new path of research and enquiry to this area. This paper
   is an attempt to understand specific mental processes in human tourism
   behaviours, and it is suggest that unconscious emotional and cognitive
   responses are natural processes that need to be studied and understood,
   not as special cases, but embedded as natural parts of tourism research.
   Design/methodology/approach To better understand consumers' unconscious
   responses to possible travel destinations, a 3 x 5 factorial design was
   run with the factors being stimulus type (images, printed names and
   videos) and travel destination (Dubai, Abu Dhabi, Hong Kong, New York
   and London). Eye-tracking calibration was done with a nine-point
   fixation test and the EEG calibration was done using functional
   localizer tests based on the ABM B-ALERT calibration process. This
   calibration procedure allows reliable tracking of emotional and
   cognitive responses over time. Thirty Emirati (nationals of the UAE)
   participants, consisting of equal numbers of males and females (15) were
   recruited from the UAE and signed informed consent. Each participant was
   positioned in front of an eye tracker and computer screen, and
   brain-scanning equipment was mounted; then, each participant underwent
   eye-tracking and neuroimaging calibration procedures. A Tobii T60XL eye
   tracker and an ABM X-10 EEG brain scanner, both running iMotions v5.1 in
   a Windows 7 environment, were used. Findings General emotional and
   cognitive differences were identified between the channels through which
   travel destinations are presented. Words about and names of travel
   destinations cause higher cognitive loads, which may not be surprising,
   given the greater associative load that words have than images. Of
   particular interest is the hypothesis that images evoke stronger
   affective responses than verbal representations. However, as previously
   noted (Holmes and Mathews, 2005), empirical evidence for this assumption
   seems surprisingly sparse. The present study and the context provided
   here suggest that decisions on travel destination have an unconscious
   component and a direct component that may drive or affect overt
   preference and actual choice. Originality/value Thus, tourism research
   may indeed be a suitable field for understanding the brain bases of
   complex preference formation and choice. Various researchers have found
   that a destination image is typically measured using cognitive,
   affective and behavioural components, and further stated that the
   cognitive image component of a destination was found to have a
   significant positive effect on the affective image component and overall
   destination image (Stylidis et al., 2017). Therefore, this research
   which has introduced brain scanning can be used to better understand the
   underlying unconscious emotional and cognitive processes that affect
   consumer thought and action. An understanding of what goes on in the
   human unconscious mind is very important for destination marketers, this
   can help in the integrated marketing communication process to create a
   destination image and brand.}},
Publisher = {{EMERALD GROUP PUBLISHING LTD}},
Address = {{HOWARD HOUSE, WAGON LANE, BINGLEY BD16 1WA, W YORKSHIRE, ENGLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Michael, I (Reprint Author), Zayed Univ, Coll Business, Dubai, U Arab Emirates.
   Michael, I (Reprint Author), Univ South Pacific, Fac Business \& Econ, Grad Sch Business, Suva, Fiji.
   Michael, Ian, Zayed Univ, Coll Business, Dubai, U Arab Emirates.
   Michael, Ian, Univ South Pacific, Fac Business \& Econ, Grad Sch Business, Suva, Fiji.
   Ramsoy, Thomas, Neurons Inc, Copenhagen, Denmark.
   Stephens, Melodena, Mohammed Bin Rashid Sch Govt, Dubai, U Arab Emirates.
   Kotsi, Filareti, Zayed Univ, Coll Commun \& Media Sci, Dubai, U Arab Emirates.}},
DOI = {{10.1108/JIMA-09-2017-0098}},
ISSN = {{1759-0833}},
EISSN = {{1759-0841}},
Keywords = {{Motivation; Neuroscience; Neuromarketing; Cognitive; Destination image;
   Emotional}},
Keywords-Plus = {{DESTINATION IMAGE; MENTAL-IMAGERY; BRAND; MODEL; EXPERIENCES;
   PSYCHOLOGY; ENGAGEMENT; ATTENTION; ATTITUDE; AROUSAL}},
Research-Areas = {{Business \& Economics}},
Web-of-Science-Categories  = {{Business}},
Author-Email = {{ian.michael@zu.ac.ae}},
Funding-Acknowledgement = {{Research Incentive Fund (RIF) {[}R14067]; Zayed University, UAE}},
Funding-Text = {{This study was supported by the Research Incentive Fund (RIF) Grant
   R14067 `Destination Branding: Perception Mapping Elements of Visual and
   Auditory Communication Elements' provided by Zayed University, UAE.}},
Number-of-Cited-References = {{97}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{6}},
Usage-Count-Since-2013 = {{6}},
Journal-ISO = {{J. Islamic Mark.}},
Doc-Delivery-Number = {{IO2VX}},
Unique-ID = {{ISI:000479241000011}},
DA = {{2019-10-28}},
}

@article{ ISI:000477009600039,
Author = {Ismael, Diana and Ploeger, Angelika},
Title = {{Development of a Sensory Method to Detect Food-Elicited Emotions Using
   Emotion-Color Association and Eye-Tracking}},
Journal = {{FOODS}},
Year = {{2019}},
Volume = {{8}},
Number = {{6}},
Month = {{JUN}},
Abstract = {{Studying consumers' implicit emotions has been always described as a
   difficult and a complicated mission due to the emotions being of a
   non-cognitive nature. This research aims to develop a new method based
   on emotion-color association (ECA) to detect consumer's implicit
   food-elicited emotions using an eye-tracker tool. The study was
   accomplished in two experiments. The first experiment intended to build
   a new color scale based on the emotion-color association using the
   eye-tracking method and a self-reported questionnaire (SRQ). The results
   showed that people tend to express their evoked positive emotions by
   choosing mostly the light colors, and favor to choose dark colors to
   reveal their evoked negative emotions. In the second experiment, a
   sensory evaluation was conducted employing the developed color scale in
   addition to verbal emotion-based questionnaire (VEQ) to detect the
   participants' food-elicited emotions with different samples. The sensory
   evaluation consisted of taste, smell, and vision tests. The study
   demonstrated a consistency between the results of the verbal emotion
   questionnaire and the new color scale method. This consistency may refer
   to the capability of the developed scale, as a non-intrusive method that
   obtains prompt responses and avoids deliberate action, to rapidly detect
   the implicit emotions in a sensory evaluation for a better understanding
   of the consumer's behavior.}},
Publisher = {{MDPI}},
Address = {{ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Ismael, D (Reprint Author), Univ Kassel, Specialized Partnerships Sustainable Food Syst \&, D-34125 Kassel, Germany.
   Ismael, Diana; Ploeger, Angelika, Univ Kassel, Specialized Partnerships Sustainable Food Syst \&, D-34125 Kassel, Germany.}},
DOI = {{10.3390/foods8060217}},
Article-Number = {{217}},
ISSN = {{2304-8158}},
Keywords = {{the color scale; emotion-color association (ECA); color-emotion
   association; eye-tracking; light colors; dark colors; general positive
   emotions (GPE); general negative emotions (GNE); sensory evaluation;
   self-reported questionnaire (SRQ); verbal emotion-based questionnaire
   (VEQ)}},
Keywords-Plus = {{PICTURES; CONSUMPTION; RESPONSES}},
Research-Areas = {{Food Science \& Technology}},
Web-of-Science-Categories  = {{Food Science \& Technology}},
Author-Email = {{diana.ismael@uni-kassel.de
   a.ploeger@uni-kassel.de}},
ORCID-Numbers = {{Ismael, Diana/0000-0002-1796-9972}},
Funding-Acknowledgement = {{University of Kassel - German Research Foundation (DFG)German Research
   Foundation (DFG); Library of the University of Kassel}},
Funding-Text = {{The costs of this open access publication were covered by open access
   publications funds from the University of Kassel, which is financed by
   the German Research Foundation (DFG) and the Library of the University
   of Kassel.}},
Number-of-Cited-References = {{45}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{2}},
Usage-Count-Since-2013 = {{2}},
Journal-ISO = {{Foods}},
Doc-Delivery-Number = {{IL0SG}},
Unique-ID = {{ISI:000477009600039}},
OA = {{DOAJ Gold, Green Published}},
DA = {{2019-10-28}},
}

@article{ ISI:000475751500004,
Author = {Yim, Dongsun and Park, Wonjeong and Kim, Shinyoung and Han, Jiyun and
   Song, Eun and Son, Jinkyeong},
Title = {{An Eye-Tracking Study of Picture Book Reading in Preschool Children with
   and without Language Delay}},
Journal = {{COMMUNICATION SCIENCES AND DISORDERS-CSD}},
Year = {{2019}},
Volume = {{24}},
Number = {{2}},
Pages = {{299-316}},
Month = {{JUN}},
Abstract = {{Objectives: A child's information processing and language skills may be
   important factors that influence story comprehension during book
   reading. Using an eye-tracking method, the present study investigated
   how age, gender, and language skills influence young children's book
   reading by measuring online eye-movement. Methods: A total of 18
   children (4-6 years old) participated in the study. One picture book
   without text was modified for the study. A total of 6 pages presented on
   a computer screen while children listened to a pre-recorded story. After
   reading, children were asked to answer story comprehension questions.
   The dependent variables were total fixation duration, average fixation
   duration, fixation time, fixation count, and first fixation duration.
   Correlation analyses were conducted among the variables age, story
   comprehension, and eye movement. Gender differences in the eye movement
   and story comprehension variables were analyzed. In addition, eye
   movement patterns and story comprehension performances were compared
   between children with typical development (N=5) and children with
   language delay (N=5). Results: First, significant correlation was found
   only between story comprehension and first fixation duration. Second,
   gender differences were significant only on the fixation count. Lastly,
   group differences were significant on both story comprehension and some
   eye movement variables. Conclusion: An eye tracking method is useful to
   reveal online processing during book reading. Children's eye movement
   patterns differed depending on gender and language abilities. The
   findings indicate that depending on a child's profile, different reading
   strategies should be applied for better learning outcomes during book
   reading.}},
Publisher = {{KOREAN ACAD SPEECH-LANGUAGE PATHOLOGY \& AUDIOLOGY}},
Address = {{KOREA NAZARENE UNIV, DEPT COMMUNICATION DISORDERS, CHUONGNAM, 331-718,
   SOUTH KOREA}},
Type = {{Article}},
Language = {{Korean}},
Affiliation = {{Yim, D (Reprint Author), Ewha Womans Univ, Dept Commun Disorders, 52 Ewhayeodae Gil, Seoul 03760, South Korea.
   Yim, Dongsun; Park, Wonjeong; Kim, Shinyoung; Han, Jiyun; Song, Eun; Son, Jinkyeong, Ewha Womans Univ, Dept Commun Disorders, 52 Ewhayeodae Gil, Seoul 03760, South Korea.}},
DOI = {{10.12963/csd.19621}},
ISSN = {{2288-1328}},
EISSN = {{2288-0917}},
Keywords = {{tracking; Eye tracker; Eye movement; Book reading; Children with
   vocabulary delay}},
Keywords-Plus = {{VISUAL-ATTENTION; COMPREHENSION; VOCABULARY; STORYBOOKS; IMPAIRMENT;
   METAANALYSIS; SCHOOL; SPEED; AGE}},
Research-Areas = {{Audiology \& Speech-Language Pathology}},
Web-of-Science-Categories  = {{Audiology \& Speech-Language Pathology}},
Author-Email = {{sunyim@ewha.ac.kr}},
Funding-Acknowledgement = {{National Research Foundation of Korea - Korean GovernmentNational
   Research Foundation of KoreaKorean Government
   {[}NRF-2016R1D1A1B03935703]}},
Funding-Text = {{This work was supported by National Research Foundation of Korea grant
   funded by the Korean Government (No. NRF-2016R1D1A1B03935703).}},
Number-of-Cited-References = {{57}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Journal-ISO = {{Commun. Sci. Disord.-CSD}},
Doc-Delivery-Number = {{IJ2RB}},
Unique-ID = {{ISI:000475751500004}},
OA = {{Other Gold}},
DA = {{2019-10-28}},
}

@article{ ISI:000462926100011,
Author = {Del Punta, J. A. and Rodriguez, V, K. and Gasaneo, G. and Bouzat, S.},
Title = {{Models for saccadic motion and postsaccadic oscillations}},
Journal = {{PHYSICAL REVIEW E}},
Year = {{2019}},
Volume = {{99}},
Number = {{3}},
Month = {{MAR 29}},
Abstract = {{In a recent letter {[}S. Bouzat et al., Phys. Rev. Lett. 120, 178101
   (2018)], a mathematical model for eyeball and pupil motion was developed
   allowing for the understanding of the postsaccadic oscillations (PSO) as
   inertial effects. The model assumes that the inner part of the iris,
   which defines the pupil, moves driven by inertial forces induced by the
   eyeball rotation, in addition to viscous and elastic forces. Among other
   achievements, the model correctly reproduces eye-tracking experiments
   concerning PSO profiles and their dependence on the saccade size. In
   this paper we propose various extensions of the mentioned model, we
   provide analytical solutions, and we perform an exhaustive analysis of
   the dynamics. In particular, we consider a more general time dependence
   for the eyeball velocity enabling the description of saccades with
   vanishing initial acceleration. Moreover, we give the analytical
   solution in terms of hypergeometric functions for the constant parameter
   version of the model and we provide particular expressions for some
   cases of interest. We also introduce a new version of the model with
   inhomogeneous viscosity that can improve the fitting of the experimental
   results. Our analysis of the solutions explores the dependence of the
   PSO profiles on the system parameters for varying saccade sizes. We show
   that the PSO emerge in critical-like ways when parameters such as the
   elasticity of the iris, the global eyeball velocity, or the saccade size
   vary. Moreover, we find that the PSO profiles with the first overshoot
   smaller than the second one, which are usually observed in experiments,
   can be associated to parameter regions close to criticality.}},
Publisher = {{AMER PHYSICAL SOC}},
Address = {{ONE PHYSICS ELLIPSE, COLLEGE PK, MD 20740-3844 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Bouzat, S (Reprint Author), Consejo Nacl Invest Cient \& Tecn, Ctr Atom Bariloche CNEA, Av E Bustillo 9500 R8402AGP, San Carlos De Bariloche, Rio Negro, Argentina.
   Del Punta, J. A.; Rodriguez, K., V; Gasaneo, G., Univ Nacl Sur, IFISUR, Dept Fis, Neufisur, RA-8000 Bahia Blanca, Buenos Aires, Argentina.
   Del Punta, J. A., Univ Nacl Sur, Dept Matemat, RA-8000 Bahia Blanca, Buenos Aires, Argentina.
   Rodriguez, K., V; Gasaneo, G., CINA, RA-8000 Bahia Blanca, Buenos Aires, Argentina.
   Bouzat, S., Consejo Nacl Invest Cient \& Tecn, Ctr Atom Bariloche CNEA, Av E Bustillo 9500 R8402AGP, San Carlos De Bariloche, Rio Negro, Argentina.}},
DOI = {{10.1103/PhysRevE.99.032422}},
Article-Number = {{032422}},
ISSN = {{2470-0045}},
EISSN = {{2470-0053}},
Keywords-Plus = {{EYE-TRACKER SIGNAL; PUPIL; MOVEMENT}},
Research-Areas = {{Physics}},
Web-of-Science-Categories  = {{Physics, Fluids \& Plasmas; Physics, Mathematical}},
Author-Email = {{bouzat@cab.cnea.gov.ar}},
Funding-Acknowledgement = {{CONICET through the PIP program; CNEA; Universidad Nacional del Sur
   {[}PGI 24/F065]}},
Funding-Text = {{The authors acknowledge support from CONICET through the PIP program,
   and from CNEA, both Argentinian agencies. We also acknowledge funding
   from Universidad Nacional del Sur, through PGI 24/F065. We thank Marcos
   M. Meo for a careful reading of the manuscript.}},
Number-of-Cited-References = {{30}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Journal-ISO = {{Phys. Rev. E}},
Doc-Delivery-Number = {{HR1WG}},
Unique-ID = {{ISI:000462926100011}},
DA = {{2019-10-28}},
}

@article{ ISI:000463543600001,
Author = {DeDe, Gayle},
Title = {{Perceptual span in individuals with aphasia}},
Journal = {{APHASIOLOGY}},
Abstract = {{Background: Perceptual span refers to the field of effective vision
   during reading comprehension. It is determined by many factors,
   including reading proficiency. No studies have investigated the
   perceptual span in people with reading comprehension impairments due to
   aphasia. Aims: The present study examined whether perceptual span is
   smaller in individuals with aphasia than controls. Methods and
   Procedures: The task was a gaze-contingent moving windows paradigm
   during silent reading using an eye tracker. Data from 11 individuals
   with aphasia and 15 neurotypical controls were analyzed. Outcomes and
   Results: Perceptual span in individuals with aphasia was the fixated
   word plus one word to the right of fixation, whereas perceptual span in
   controls was the fixated word plus two words to the right of fixation.
   Conclusion: Individuals with aphasia have a smaller perceptual span than
   controls, which likely reflects increased effort during reading
   comprehension.}},
Publisher = {{ROUTLEDGE JOURNALS, TAYLOR \& FRANCIS LTD}},
Address = {{2-4 PARK SQUARE, MILTON PARK, ABINGDON OX14 4RN, OXON, ENGLAND}},
Type = {{Article; Early Access}},
Language = {{English}},
Affiliation = {{DeDe, G (Reprint Author), Temple Univ, Dept Commun Sci \& Disorders, 110 Weiss Hall,1701 N 13th St, Philadelphia, PA 19122 USA.
   DeDe, Gayle, Temple Univ, Dept Commun Sci \& Disorders, 110 Weiss Hall,1701 N 13th St, Philadelphia, PA 19122 USA.}},
DOI = {{10.1080/02687038.2019.1591612}},
Early Access Date = {{MAR 2019}},
ISSN = {{0268-7038}},
EISSN = {{1464-5041}},
Keywords = {{Perceptual span; aphasia; reading comprehension; eye tracking; sentence
   comprehension}},
Keywords-Plus = {{EYE-MOVEMENT CONTROL; WORD-FREQUENCY; VISUAL-ATTENTION; PREDICTABILITY;
   COMPREHENSION; INFORMATION; PEOPLE; MODEL; DIFFICULTY; FIXATION}},
Research-Areas = {{Audiology \& Speech-Language Pathology; Neurosciences \& Neurology;
   Rehabilitation}},
Web-of-Science-Categories  = {{Audiology \& Speech-Language Pathology; Clinical Neurology;
   Rehabilitation}},
Author-Email = {{gayle.dede@temple.edu}},
Funding-Acknowledgement = {{National Institute on Deafness and Other Communication DisordersUnited
   States Department of Health \& Human ServicesNational Institutes of
   Health (NIH) - USANIH National Institute on Deafness \& Other
   Communication Disorders (NIDCD) {[}K23DC010808]}},
Funding-Text = {{This work was supported by the National Institute on Deafness and Other
   Communication Disorders {[}K23DC010808].}},
Number-of-Cited-References = {{43}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{2}},
Usage-Count-Since-2013 = {{2}},
Journal-ISO = {{Aphasiology}},
Doc-Delivery-Number = {{HS0IF}},
Unique-ID = {{ISI:000463543600001}},
DA = {{2019-10-28}},
}

@article{ ISI:000464565700015,
Author = {Jo, Hye Lyun and Sung, Jee Eun},
Title = {{Age-Related Differences in Word Recognition Task according to the
   Interference Types: Evidence from Eye-Tracking}},
Journal = {{COMMUNICATION SCIENCES AND DISORDERS-CSD}},
Year = {{2019}},
Volume = {{24}},
Number = {{1}},
Pages = {{186-204}},
Month = {{MAR}},
Abstract = {{Objectives:There is considerable controversy with respect to language
   processing in the elderly. The purpose of the study was to investigate
   age-related differences in word recognition tasks according to
   interference types with Eye-tracker. Methods: A total of 46 participants
   (24 young and 22 elderly adults) participated in the study. A word
   recognition task and an online eye-tracking analysis were used. The
   stimuli consisted of 80 Korean nouns with four types: target,
   phonologically related objects to the target, semantically related
   objects to the target, and unrelated objects. Each critical trial
   display included four types. Participants were asked to select one of
   the pictures after the target was auditorily presented. Results: The
   elderly group had lower accuracy and slower reaction time than the young
   group. There were significant interactions between the group and
   time-window. The young group fixated the target longer than the elderly
   group in the time window of 1,000-1,800 ms. According to the proportion
   data of the interference type, there were significant interactions
   between types and time-window. Both groups fixated the phonologically
   related picture longer than semantically related picture or unrelated
   picture in 1,0001,400 ms time-window. Conclusion: Elderly adults
   demonstrated delayed processing when compared with younger adults,
   whereas age-group differences did not emerge as a function of the
   interference types.There was aging-related decline in online processing
   of the word recognition abilities, but differential effects by the
   interference types did not efficiently discriminate the groups,
   indicating that their online processing abilities to inhibit
   phonologically and semantically related foils may be intact, although
   elderly adults suffered from slowed processing.}},
Publisher = {{KOREAN ACAD SPEECH-LANGUAGE PATHOLOGY \& AUDIOLOGY}},
Address = {{KOREA NAZARENE UNIV, DEPT COMMUNICATION DISORDERS, CHUONGNAM, 331-718,
   SOUTH KOREA}},
Type = {{Article}},
Language = {{Korean}},
Affiliation = {{Sung, JE (Reprint Author), Ewha Womans Univ, Dept Commun Disorders, 52 Ewhayeodae Gil, Seoul 03760, South Korea.
   Jo, Hye Lyun; Sung, Jee Eun, Ewha Womans Univ, Dept Commun Disorders, 52 Ewhayeodae Gil, Seoul 03760, South Korea.}},
DOI = {{10.12963/csd.19584}},
ISSN = {{2288-1328}},
EISSN = {{2288-0917}},
Keywords = {{Word recognition; Eye-tracking; Proportion of fixation; Aging}},
Keywords-Plus = {{VERBAL WORKING-MEMORY; LEXICAL ACCESS; OLDER-ADULTS; LANGUAGE
   COMPREHENSION; DISCOURSE PRODUCTION; SEMANTIC ACTIVATION;
   SPOKEN-LANGUAGE; TONGUE; TIP; RETRIEVAL}},
Research-Areas = {{Audiology \& Speech-Language Pathology}},
Web-of-Science-Categories  = {{Audiology \& Speech-Language Pathology}},
Author-Email = {{jeesung@ewha.ac.kr}},
Funding-Acknowledgement = {{National Research Foundation of Korea - Korean GovernmentNational
   Research Foundation of KoreaKorean Government {[}NRF-2017S1A2A2038375]}},
Funding-Text = {{This work was supported by the National Research Foundation of Korea
   Grant funded by the Korean Government (NRF-2017S1A2A2038375)}},
Number-of-Cited-References = {{102}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{1}},
Journal-ISO = {{Commun. Sci. Disord.-CSD}},
Doc-Delivery-Number = {{HT4XD}},
Unique-ID = {{ISI:000464565700015}},
OA = {{Other Gold}},
DA = {{2019-10-28}},
}

@article{ ISI:000462909500032,
Author = {Rizzo, John-Ross and Beheshti, Mahya and Fung, James and Rucker, Janet
   C. and Hudson, Todd E.},
Title = {{Efficiently Recording The Eye-Hand Coordination To Incoordination
   Spectrum}},
Journal = {{JOVE-JOURNAL OF VISUALIZED EXPERIMENTS}},
Year = {{2019}},
Number = {{145}},
Month = {{MAR}},
Abstract = {{The objective analysis of eye movements has a significant history and
   has been long proven to be an important research tool in the setting of
   brain injury. Quantitative recordings have a strong capacity to screen
   diagnostically. Concurrent examinations of the eye and upper limb
   movements directed toward shared functional goals (e.g., eye-hand
   coordination) serve as an additional robust biomarker-laden path to
   capture and interrogate neural injury, including acquired brain injury
   (ABI). While quantitative dual-effector recordings in 3-D afford ample
   opportunities within ocular-manual motor investigations in the setting
   of ABI, the feasibility of such dual recordings for both eye and hand is
   challenging in pathological settings, particularly when approached with
   research-grade rigor. Here we describe the integration of an eye
   tracking system with a motion tracking system intended primarily for
   limb control research to study a natural behavior. The protocol enables
   the investigation of unrestricted, three-dimensional (3D) eye-hand
   coordination tasks. More specifically, we review a method to assess
   eye-hand coordination in visually guided saccade-to-reach tasks in
   subjects with chronic middle cerebral artery (MCA) stroke and compare
   them to healthy controls. Special attention is paid to the specific
   eye-and limb-tracking system properties in order to obtain high fidelity
   data from participants post-injury. Sampling rate, accuracy, permissible
   head movement range given anticipated tolerance and the feasibility of
   use were several of the critical properties considered when selecting an
   eye tracker and an approach. The limb tracker was selected based on a
   similar rubric but included the need for 3-D recording, dynamic
   interaction and a miniaturized physical footprint. The quantitative data
   provided by this method and the overall approach when executed correctly
   has tremendous potential to further refine our mechanistic understanding
   of eye-hand control and help inform feasible diagnostic and pragmatic
   interventions within the neurological and rehabilitative practice.}},
Publisher = {{JOURNAL OF VISUALIZED EXPERIMENTS}},
Address = {{1 ALEWIFE CENTER, STE 200, CAMBRIDGE, MA 02140 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Rizzo, JR (Reprint Author), New York Univ Langone Hlth, Dept Rehabil Med, New York, NY 10016 USA.
   Rizzo, JR (Reprint Author), New York Univ Langone Hlth, Dept Neurol, New York, NY 10016 USA.
   Rizzo, John-Ross; Beheshti, Mahya; Fung, James; Hudson, Todd E., New York Univ Langone Hlth, Dept Rehabil Med, New York, NY 10016 USA.
   Rizzo, John-Ross; Rucker, Janet C.; Hudson, Todd E., New York Univ Langone Hlth, Dept Neurol, New York, NY 10016 USA.
   Rucker, Janet C., New York Univ Langone Hlth, Dept Ophthalmol, New York, NY USA.}},
DOI = {{10.3791/58885}},
Article-Number = {{e58885}},
ISSN = {{1940-087X}},
Keywords = {{Behavior; Issue 145; Brain injuries; eye movements; eye tracker; limb
   motion tracker; Stroke; Ocular motor coordination}},
Keywords-Plus = {{TRAUMATIC BRAIN-INJURY; SCLERAL SEARCH COILS; VIDEO-OCULOGRAPHY; OPTIMAL
   RESPONSE; VISUAL TARGET; ARM MOVEMENTS; MOTOR SYSTEMS; DEFICITS; STROKE;
   SACCADES}},
Research-Areas = {{Science \& Technology - Other Topics}},
Web-of-Science-Categories  = {{Multidisciplinary Sciences}},
Author-Email = {{JohnRoss.Rizzo@nyumc.org}},
ORCID-Numbers = {{Rizzo, John-Ross/0000-0002-4084-0085}},
Funding-Acknowledgement = {{NYULMC Rusk Research Team {[}5K12 HD001097]}},
Funding-Text = {{We would like to thank Dr. Tamara Bushnik and the NYULMC Rusk Research
   Team for their thoughts, suggestions, and contributions. This research
   was supported by 5K12 HD001097 (to J-RR, MSL, and PR).}},
Number-of-Cited-References = {{60}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{8}},
Usage-Count-Since-2013 = {{9}},
Journal-ISO = {{J. Vis. Exp.}},
Doc-Delivery-Number = {{HR1QV}},
Unique-ID = {{ISI:000462909500032}},
DA = {{2019-10-28}},
}

@article{ ISI:000458393400024,
Author = {Barone, Rita and Spampinato, Concetto and Pino, Carmelo and Palermo,
   Filippo and Scuderi, Anna and Zavattieri, Anna and Gulisano, Mariangela
   and Giordano, Daniela and Rizzo, Renata},
Title = {{Online comprehension across different semantic categories in preschool
   children with autism spectrum disorder}},
Journal = {{PLOS ONE}},
Year = {{2019}},
Volume = {{14}},
Number = {{2}},
Month = {{FEB 11}},
Abstract = {{Background
   Word comprehension across semantic categories is a key area of language
   development. Using online automated eye-tracking technology to reduce
   response demands during a word comprehension test may be advantageous in
   children with autism spectrum disorder (ASD). Objectives To measure
   online accuracy of word recognition across eleven semantic categories in
   preschool children with ASD and in typically developing (TD) children
   matched for gender and developmental age.
   Methods
   Using eye-tracker methodology we measured the relative number of
   fixations on a target image as compared to a foil of the same category
   shown simultaneously on screen. This online accuracy measure was
   considered a measure of word understanding. We tested the relationship
   between online accuracy and offline word recognition and the effects of
   clinical variables on online accuracy. Twenty-four children with ASD and
   21 TD control children underwent the eye-tracking task.
   Results
   On average, children with ASD were significantly less accurate at
   fixating on the target image than the TD children. After multiple
   comparison correction, no significant differences were found across the
   eleven semantic categories of the experiment between preschool children
   with ASD and younger TD children matched for developmental age. The ASD
   group showed higher intragroup variability consistent with greater
   variation in vocabulary growth rates. Direct effects of non-verbal
   cognitive levels, vocabulary levels and gesture productions on online
   word recognition in both groups support a dimensional view of language
   abilities in ASD.
   Conclusions
   Online measures of word comprehension across different semantic
   categories show higher interindividual variability in children with ASD
   and may be useful for objectively monitor gains on targeted language
   interventions.}},
Publisher = {{PUBLIC LIBRARY SCIENCE}},
Address = {{1160 BATTERY STREET, STE 100, SAN FRANCISCO, CA 94111 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Barone, R (Reprint Author), Univ Catania, Dept Clin \& Expt Med, Child Neurol \& Psychiat Unit, Catania, Italy.
   Barone, R (Reprint Author), IPCB, CNR, Catania, Italy.
   Barone, Rita; Zavattieri, Anna; Gulisano, Mariangela; Rizzo, Renata, Univ Catania, Dept Clin \& Expt Med, Child Neurol \& Psychiat Unit, Catania, Italy.
   Barone, Rita, IPCB, CNR, Catania, Italy.
   Spampinato, Concetto; Pino, Carmelo; Giordano, Daniela, Univ Catania, Dept Elect Elect \& Comp Engn, Catania, Italy.
   Palermo, Filippo, Univ Catania, Dept Clin \& Expt Med, Biostat, Catania, Italy.
   Scuderi, Anna, Univ Catania, Sch Med, Catania, Italy.}},
DOI = {{10.1371/journal.pone.0211802}},
Article-Number = {{e0211802}},
ISSN = {{1932-6203}},
Keywords-Plus = {{AGE-OF-ACQUISITION; INDIVIDUAL-DIFFERENCES; VOCABULARY GROWTH; WORD
   RECOGNITION; LANGUAGE; SKILLS; SEVERITY; TODDLERS; INFANTS; ADOLESCENTS}},
Research-Areas = {{Science \& Technology - Other Topics}},
Web-of-Science-Categories  = {{Multidisciplinary Sciences}},
Author-Email = {{rbarone@unict.it}},
ResearcherID-Numbers = {{Rizzo, Renata/AAB-8772-2019
   }},
ORCID-Numbers = {{Barone, Rita/0000-0001-6302-2686}},
Funding-Acknowledgement = {{University of Catania {[}FIR-2014 ED99F1]}},
Funding-Text = {{Partial financial support from the University of Catania FIR-2014 ED99F1
   to RB is gratefully acknowledged. The funder did not play any role in
   the study design, data collection and analysis, decision to publish or
   preparation of the manuscript. There was no additional external funding
   received for this study.}},
Number-of-Cited-References = {{46}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Journal-ISO = {{PLoS One}},
Doc-Delivery-Number = {{HL0OI}},
Unique-ID = {{ISI:000458393400024}},
OA = {{DOAJ Gold, Green Published}},
DA = {{2019-10-28}},
}

@article{ ISI:000459941200189,
Author = {Harezlak, Katarzyna and Kasprowski, Pawel},
Title = {{Understanding Eye Movement Signal Characteristics Based on Their
   Dynamical and Fractal Features}},
Journal = {{SENSORS}},
Year = {{2019}},
Volume = {{19}},
Number = {{3}},
Month = {{FEB 1}},
Abstract = {{Eye movement is one of the biological signals whose exploration may
   reveal substantial information, enabling greater understanding of the
   biology of the brain and its mechanisms. In this research, eye movement
   dynamics were studied in terms of chaotic behavior and self-similarity
   assessment to provide a description of young, healthy, oculomotor system
   characteristics. The first of the investigated features is present and
   advantageous for many biological objects or physiological phenomena, and
   its vanishing or diminishment may indicate a system pathology.
   Similarly, exposed self-similarity may prove useful for indicating a
   young and healthy system characterized by adaptability. For this
   research, 24 young people with normal vision were involved. Their eye
   movements were registered with the usage of a head-mounted eye tracker,
   using infrared oculography, embedded in the sensor, measuring the
   rotations of the left and the right eye. The influence of the
   preprocessing step in the form of the application of various filtering
   methods on the assessment of the final dynamics was also explored. The
   obtained results confirmed the existence of chaotic behavior in some
   parts of eye movement signal; however, its strength turned out to be
   dependent on the filter used. They also exposed the long-range
   correlation representing self-similarity, although the influence of the
   applied filters on these outcomes was not unveiled.}},
Publisher = {{MDPI}},
Address = {{ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Harezlak, K (Reprint Author), Silesian Tech Univ, Inst Informat, Akad 16, PL-44100 Gliwice, Poland.
   Harezlak, Katarzyna; Kasprowski, Pawel, Silesian Tech Univ, Inst Informat, Akad 16, PL-44100 Gliwice, Poland.}},
DOI = {{10.3390/s19030626}},
Article-Number = {{626}},
ISSN = {{1424-8220}},
Keywords = {{eye tracking; infrared oculography; chaotic dynamics; fractal theory;
   filtering methods}},
Keywords-Plus = {{TIME-SERIES ANALYSIS; DIMENSION}},
Research-Areas = {{Chemistry; Engineering; Instruments \& Instrumentation}},
Web-of-Science-Categories  = {{Chemistry, Analytical; Engineering, Electrical \& Electronic;
   Instruments \& Instrumentation}},
Author-Email = {{katarzyna.harezlak@polsl.pl
   pawel.kasprowski@polsl.pl}},
ORCID-Numbers = {{Harezlak, Katarzyna/0000-0003-3573-9772}},
Funding-Acknowledgement = {{Rector Habilitation Grant; Silesian University of Technology
   {[}02/020/RGH18/0149]; Statutory Research funds of Institute of
   Informatics, Silesian University of Technology, Gliwice, Poland
   {[}BK-213/RAU2/2018]}},
Funding-Text = {{This research was funded by the Rector Habilitation Grant, Silesian
   University of Technology, Grant Number: 02/020/RGH18/0149 and by
   Statutory Research funds of Institute of Informatics, Silesian
   University of Technology, Gliwice, Poland, Grant Number
   BK-213/RAU2/2018.}},
Number-of-Cited-References = {{46}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{1}},
Journal-ISO = {{Sensors}},
Doc-Delivery-Number = {{HN1IR}},
Unique-ID = {{ISI:000459941200189}},
OA = {{DOAJ Gold, Green Published}},
DA = {{2019-10-28}},
}

@article{ ISI:000456787500005,
Author = {Nazareth, Alina and Killick, Rebecca and Dick, Anthony S. and Pruden,
   Shannon M.},
Title = {{Strategy Selection Versus Flexibility: Using Eye-Trackers to Investigate
   Strategy Use During Mental Rotation}},
Journal = {{JOURNAL OF EXPERIMENTAL PSYCHOLOGY-LEARNING MEMORY AND COGNITION}},
Year = {{2019}},
Volume = {{45}},
Number = {{2}},
Pages = {{232-245}},
Month = {{FEB}},
Abstract = {{Spatial researchers have been arguing over the optimum cognitive
   strategy for spatial problem-solving for several decades. The current
   article aims to shift this debate from strategy dichotomies to strategy
   flexibility-a cognitive process, which although alluded to in spatial
   research, presents practical methodological challenges to empirical
   testing. In the current study, participants' eye movements were tracked
   during a mental rotation task (MRT) using the Tobii x60 eye-tracker.
   Results of a latent profile analysis, combining different eye movement
   parameters, indicated two distinct eye-patterns-fixating and switching
   patterns. The switching eye-pattern was associated with high mental
   rotation performance. There were no sex differences in eye-patterns. To
   investigate strategy flexibility, we used a novel application of the
   changepoint detection algorithm on eye movement data. Strategy
   flexibility significantly predicted mental rotation performance. Male
   participants demonstrated higher strategy flexibility than did female
   participants. Our findings highlight the importance of strategy
   flexibility in spatial thinking and have implications for designing
   spatial training techniques. The novel approaches to analyzing eye
   movement data in the current paper can be extended to research beyond
   the spatial domain.}},
Publisher = {{AMER PSYCHOLOGICAL ASSOC}},
Address = {{750 FIRST ST NE, WASHINGTON, DC 20002-4242 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Nazareth, A (Reprint Author), Temple Univ, Dept Psychol, 1701 North 13th St, Philadelphia, PA 19122 USA.
   Nazareth, Alina, Temple Univ, Dept Psychol, 1701 North 13th St, Philadelphia, PA 19122 USA.
   Killick, Rebecca, Univ Lancaster, Dept Math \& Stat, Lancaster, England.
   Dick, Anthony S.; Pruden, Shannon M., Florida Int Univ, Dept Psychol, Miami, FL 33199 USA.}},
DOI = {{10.1037/xlm0000574}},
ISSN = {{0278-7393}},
EISSN = {{1939-1285}},
Keywords = {{cognitive; sex differences; spatial; eye-movement; changepoint analysis}},
Keywords-Plus = {{LATENT CLASS ANALYSIS; SEX-DIFFERENCES; INDIVIDUAL-DIFFERENCES; SPATIAL
   ABILITY; VISUAL WORLD; COMPREHENSION; METAANALYSIS; INFORMATION;
   COMPONENTS; MOVEMENTS}},
Research-Areas = {{Psychology}},
Web-of-Science-Categories  = {{Psychology; Psychology, Experimental}},
Author-Email = {{alina.nazareth@temple.edu}},
ResearcherID-Numbers = {{Killick, Rebecca/A-9790-2010}},
ORCID-Numbers = {{Killick, Rebecca/0000-0003-0583-3960}},
Funding-Acknowledgement = {{Florida International University Graduate School Dissertation Year
   Fellowship (UGS DYF); National Science Foundation, Division of Research
   on Learning grant {[}1420627]}},
Funding-Text = {{This research was supported in part by the Florida International
   University Graduate School Dissertation Year Fellowship (UGS DYF)
   awarded to Alina Nazareth. Writing of this article was supported by a
   National Science Foundation, Division of Research on Learning grant
   (1420627) to Shannon M. Pruden.}},
Number-of-Cited-References = {{73}},
Times-Cited = {{1}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{6}},
Journal-ISO = {{J. Exp. Psychol.-Learn. Mem. Cogn.}},
Doc-Delivery-Number = {{HI9OX}},
Unique-ID = {{ISI:000456787500005}},
DA = {{2019-10-28}},
}

@article{ ISI:000480433800011,
Author = {Julio, Cristobal and Parodi, Giovanni and Loureda, Oscar},
Title = {{Congruency between semiotic systems: A study on words and graphs with
   the use of eye tracker}},
Journal = {{ESTUDIOS FILOLOGICOS}},
Year = {{2019}},
Number = {{63}},
Pages = {{237-259}},
Abstract = {{In the present study we seek to determine if the
   congruence/non-congruence between two semiotic systems is a factor that
   affects the reading patterns of a group of students in the area of
   economics. Specifically, we are interested in identifying reading routes
   that reveal whether readers identify the incongruence between two
   semiotic systems: the verbal system and the graph system. To achieve
   this objective, eye movements were recorded from 16 students in the
   economics area of a Chilean university, employing an eye tracker. The
   texts read by the students were constructed from specific rhetorical
   textual segments of the discourse genre Monetary Policy Report (IPoM).
   The general results indicate that: 1) readers process all texts with
   equal attention without the factor congruence between semiotic systems
   revealing statistically significant differences, 2) no differences were
   observed in the integrative transitions between congruency and
   non-congruency, and 3) the verbal system of the texts is processed
   mainly, both in the dependent variables time of the first reading (first
   pass) and time of the second reading (second pass). These findings can
   be explained in terms of certain logocentrism or preeminence of the
   words, even though, as students of the economics area, the students have
   received instruction in the reading of graphs.}},
Publisher = {{UNIV AUSTRAL CHILE}},
Address = {{FAC FILOSOFIA HUMANIDADES CASILLA 142 ESTUDIOS FILOLOGICOS, VALDIVIA,
   CHILE}},
Type = {{Article}},
Language = {{Spanish}},
Affiliation = {{Julio, C (Reprint Author), Pontificia Univ Catolica Valparaiso, Fac Filosofia \& Educ, Valparaiso, Chile.
   Julio, Cristobal; Parodi, Giovanni, Pontificia Univ Catolica Valparaiso, Fac Filosofia \& Educ, Valparaiso, Chile.
   Loureda, Oscar, Heidelberg Univ, Heidelberg, Germany.}},
DOI = {{10.4067/S0071-17132019000100237}},
ISSN = {{0071-1713}},
EISSN = {{0717-6171}},
Keywords = {{multisemiotic comprehension; monetary policy report; line charts; eye
   tracking; multimodality}},
Keywords-Plus = {{TEXT; INFORMATION; MOVEMENTS; DISCIPLINES; CORPUS; GENRES}},
Research-Areas = {{Linguistics; Literature}},
Web-of-Science-Categories  = {{Linguistics; Language \& Linguistics; Literature, Romance}},
Author-Email = {{cristobal.julio@pucv.cl
   giovanni.parodi@pucv.cl
   oscarloureda@iued.uni-heidelberg.de}},
Number-of-Cited-References = {{76}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Journal-ISO = {{Estud. Filol.}},
Doc-Delivery-Number = {{IQ0IH}},
Unique-ID = {{ISI:000480433800011}},
OA = {{Other Gold, Bronze}},
DA = {{2019-10-28}},
}

@article{ ISI:000480425500001,
Author = {Krueger, Eva and Schneider, Andrea and Sawyer, Ben D. and Chavaillaz,
   Alain and Sonderegger, Andreas and Groner, Rudolf and Hancock, P. A.},
Title = {{Microsaccades Distinguish Looking From Seeing}},
Journal = {{JOURNAL OF EYE MOVEMENT RESEARCH}},
Year = {{2019}},
Volume = {{12}},
Number = {{6}},
Abstract = {{Understanding our visual world requires both looking and seeing.
   Dissociation of these processes can result in the phenomenon of
   inattentional blindness or `looking without seeing'. Concomitant errors
   in applied settings can be serious, and even deadly. Current visual data
   analysis cannot differentiate between just `looking' and actual
   processing of visual information, i.e., `seeing'. Differentiation may be
   possible through the examination of microsaccades; the involuntary,
   small-magnitude saccadic eye movements that occur during processed
   visual fixation. Recent work has suggested that microsaccades are
   post-attentional biosignals, potentially modulated by task.
   Specifically, microsaccade rates decrease with increased mental task
   demand, and increase with growing visual task difficulty. Such findings
   imply that there are fundamental differences in microsaccadic activity
   between visual and nonvisual tasks. To evaluate this proposition, we
   used a high-speed eye tracker to record participants in looking for
   differences between two images or, doing mental arithmetic, or both
   tasks in combination. Results showed that microsaccade rate was
   significantly increased in conditions that require high visual
   attention, and decreased in conditions that require less visual
   attention. The results support microsaccadic rate reflecting visual
   attention, and level of visual information processing. A measure that
   reflects to what extent and how an operator is processing visual
   information represents a critical step for the application of
   sophisticated visual assessment to real world tasks.}},
Publisher = {{INT GROUP EYE MOVEMENT RESEARCH}},
Address = {{C/O RUDOLF GRONER, AL NASEEM ARABIAN STUD, IFFWIL, CH-3305, SWITZERLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Krueger, E (Reprint Author), Univ Cent Florida, Orlando, FL 32816 USA.
   Krueger, Eva; Hancock, P. A., Univ Cent Florida, Orlando, FL 32816 USA.
   Schneider, Andrea; Groner, Rudolf, Univ Bern, Bern, Switzerland.
   Sawyer, Ben D., MIT, Cambridge, MA 02139 USA.
   Chavaillaz, Alain, Univ Fribourg, Fribourg, Switzerland.
   Sonderegger, Andreas, Ecole Polytech Fed Lausanne, Lausanne, Switzerland.}},
DOI = {{10.16910/jemr.12.6.2}},
Article-Number = {{2}},
ISSN = {{1995-8692}},
Keywords = {{Fixational eye movements; eye tracking; microsaccades; visual load;
   visual attention}},
Keywords-Plus = {{ATTENTION; DYNAMICS}},
Research-Areas = {{Ophthalmology}},
Web-of-Science-Categories  = {{Ophthalmology}},
Number-of-Cited-References = {{44}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Journal-ISO = {{J. Eye Mov. Res.}},
Doc-Delivery-Number = {{IQ0FF}},
Unique-ID = {{ISI:000480425500001}},
OA = {{DOAJ Gold}},
DA = {{2019-10-28}},
}

@article{ ISI:000469466700006,
Author = {Martin-Arnal, Lorena A. and Leon, Jose A. and van den Broek, Paul and
   Olmos, Ricardo},
Title = {{Understanding Comics. A Comparison between Children and Adults through a
   Coherence/Incoherence Paradigm in an Eye-tracking Study}},
Journal = {{PSICOLOGIA EDUCATIVA}},
Year = {{2019}},
Volume = {{25}},
Number = {{2}},
Pages = {{127-137}},
Abstract = {{Theories about visual narrative understanding accentuate the difference
   between patterns of reading comprehension in children and adults when
   they read text and images. This study was conducted to explore the
   differences in eye movement patterns when children and adults read
   different comic stories using a coherence/incoherence paradigm. A total
   of 63 participants, 31 children (10-12 years old) and 32 undergraduate
   university students from the Universidad Autonoma de Madrid, read 20
   comic stories, each of them with both coherent and incoherent versions,
   for the two ending frames. Fixation durations, number of fixations, and
   number of regressions were recorded by an eye-tracker, Tobii x-120. A
   crossed random effects model was applied. Results showed that even
   though children reach a similar level of understanding than adults they
   spend more time and have longer fixations than adults, showing more
   effort to reach the whole comprehension of the stories. Besides, results
   do not detect significant differences between eye movements' patterns in
   peak and release for the two groups studied, and therefore both
   components of the visual narrative grammar are considered equally
   relevant in the understanding of comics.}},
Publisher = {{COLEGIO OFICIAL PSICOLOGOS MADRID}},
Address = {{C/CUESTA SAN VICENTE, NO 4, 6 PLANTA, MADRID, 28008, SPAIN}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Martin-Arnal, LA (Reprint Author), Univ Autonoma Madrid, Madrid, Spain.
   Martin-Arnal, Lorena A.; Leon, Jose A.; Olmos, Ricardo, Univ Autonoma Madrid, Madrid, Spain.
   van den Broek, Paul, Leiden Univ, Leiden, Netherlands.}},
DOI = {{10.5093/psed2019a7}},
ISSN = {{1135-755X}},
EISSN = {{2174-0526}},
Keywords = {{Reading comprehension; Visual narrative grammar; Comic comprehension;
   Eye movements; Eye tracking; Coherence/incoherence paradigm; Children
   and adults}},
Keywords-Plus = {{READING EVIDENCE; CONSTITUENT STRUCTURE; WORD-FREQUENCY; TIME-COURSE;
   MOVEMENTS; COMPREHENSION; MEMORY; REPRESENTATIONS; CONSTRUCTION;
   CATEGORIES}},
Research-Areas = {{Education \& Educational Research; Psychology}},
Web-of-Science-Categories  = {{Education \& Educational Research; Psychology, Educational; Psychology,
   Multidisciplinary}},
Author-Email = {{lorena.martin@uam.es}},
Funding-Acknowledgement = {{Ministry of Economic and Competitivity (MINECO) of Spain
   {[}PSI2013-47219-P]; European UnionEuropean Union (EU)}},
Funding-Text = {{This study was supported by Grant PSI2013-47219-P from the Ministry of
   Economic and Competitivity (MINECO) of Spain, and European Union.}},
Number-of-Cited-References = {{60}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Journal-ISO = {{Psicol. Educ.}},
Doc-Delivery-Number = {{IA3MF}},
Unique-ID = {{ISI:000469466700006}},
OA = {{DOAJ Gold}},
DA = {{2019-10-28}},
}

@article{ ISI:000457360800008,
Author = {Screen, Benjamin},
Title = {{What effect does post-editing have on the translation product from an
   end-user's perspective?}},
Journal = {{JOURNAL OF SPECIALISED TRANSLATION}},
Year = {{2019}},
Number = {{31}},
Pages = {{133-157}},
Month = {{JAN}},
Abstract = {{This article details a triangulated eye-tracking experiment carried out
   at Cardiff University, UK. The experiment sought to compare the quality
   of final texts, from an end-user's perspective, when different
   translation modalities (translating and post-editing Machine
   Translation) were used to translate the same source text. The language
   pair investigated is English and Welsh. An eye tracker was used to
   record fixations in a between-groups experimental design as participants
   read two texts, one post-edited and one translated using the same source
   text, as well as two subjective Likert-type scales where each
   participant rated the texts for readability and comprehensibility.
   Following an analysis of fixation duration, the gaze data of the two
   groups was found to be statistically identical, and there was no
   statistically significant difference found between the readability and
   comprehensibility scores gleaned from subjective Likert-type scales. It
   is argued following this that post-editing machine translated texts does
   not necessarily lead to translations of inferior quality in the context
   of the final end-user, and that this further supports the use of Machine
   Translation in a professional context.}},
Publisher = {{ROEHAMPTON UNIV SCH ARTS}},
Address = {{DIGBY STUART COLL, ROEHAMPTON LA, LONDON, SW15 5PH, ENGLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Screen, B (Reprint Author), Cardiff Univ, Sch Welsh, Cardiff, S Glam, Wales.
   Screen, Benjamin, Cardiff Univ, Sch Welsh, Cardiff, S Glam, Wales.}},
ISSN = {{1740-357X}},
Keywords = {{Eye tracking; translation quality; translation end-users; Welsh
   translation}},
Keywords-Plus = {{MACHINE TRANSLATION; MEMORY}},
Research-Areas = {{Linguistics}},
Web-of-Science-Categories  = {{Linguistics; Language \& Linguistics}},
Author-Email = {{screenb@cardiff.ac.uk}},
Number-of-Cited-References = {{85}},
Times-Cited = {{1}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{1}},
Journal-ISO = {{J. Spec. Transl.}},
Doc-Delivery-Number = {{HJ7FO}},
Unique-ID = {{ISI:000457360800008}},
OA = {{DOAJ Gold}},
DA = {{2019-10-28}},
}

@article{ ISI:000455819800008,
Author = {Kang, Si-Nae and Yim, Dongsun},
Title = {{Reading Comprehension and Reading Processing of School-Aged Children
   with Specific Language Impairment Using Eye Tracker}},
Journal = {{COMMUNICATION SCIENCES AND DISORDERS-CSD}},
Year = {{2018}},
Volume = {{23}},
Number = {{4}},
Pages = {{914-928}},
Month = {{DEC}},
Abstract = {{Objectives:This study investigated the characteristics of reading
   comprehension and reading process in children with specific language
   impairment (SLI) compared with typically developing children using an
   eye tracker. Additionally, the study examined how reading ability
   correlates with vocabulary, personal background knowledge, and working
   memory, which is known as the basis of language development. Methods:The
   participants were 12 children with SLI and 15 typically developing
   children between the 2nd and 4th grades of elementary school. In order
   to investigate the difference of reading comprehension between groups,
   three kinds of questions were used and an eye tracker was used to
   analyze the characteristics of the reading process. Also receptive \&
   expressive vocabulary and two working memory tasks were performed to
   investigate correlation with reading comprehension. Results: The results
   showed that children with SLI need significantly longer reading time and
   on average need more fixation time than typically developing children.
   The total time of fixation was significantly higher than that of
   typically developing children. Secondly, children with SLI showed
   significantly lower performance in reading comprehension in all three
   types of questions tasks. Thirdly, reading comprehension was
   significantly correlated with the receptive \& expressive vocabulary.
   Conclusion: Children with SLI showed longer reading time and longer mean
   of fixation time. This study design allowed us to better understand the
   limitations of previous studies in word or sentence by using the story
   at the level of discourse appropriate to the level of school age
   children in the task.}},
Publisher = {{KOREAN ACAD SPEECH-LANGUAGE PATHOLOGY \& AUDIOLOGY}},
Address = {{KOREA NAZARENE UNIV, DEPT COMMUNICATION DISORDERS, CHUONGNAM, 331-718,
   SOUTH KOREA}},
Type = {{Article}},
Language = {{Korean}},
Affiliation = {{Yim, D (Reprint Author), Ewha Womans Univ, Dept Commun Disorders, 52 Ewhayeodae Gil, Seoul 03760, South Korea.
   Kang, Si-Nae, Ewha Womans Univ, Grad Sch, Dept Commun Disorders, Seoul, South Korea.
   Yim, Dongsun, Ewha Womans Univ, Dept Commun Disorders, 52 Ewhayeodae Gil, Seoul 03760, South Korea.}},
DOI = {{10.12963/csd.18551}},
ISSN = {{2288-1328}},
EISSN = {{2288-0917}},
Keywords = {{Eye-tracker; Eye movement; Reading comprehension; Specific language
   impairment}},
Keywords-Plus = {{WORKING-MEMORY; INDIVIDUAL-DIFFERENCES; STORY RECALL; MOVEMENTS;
   ABILITY; INFERENCE; WORDS; GENERATION; ATTENTION; PATTERNS}},
Research-Areas = {{Audiology \& Speech-Language Pathology}},
Web-of-Science-Categories  = {{Audiology \& Speech-Language Pathology}},
Author-Email = {{sunyim@ewha.ac.kr}},
Funding-Acknowledgement = {{National Research Foundation of Korea - Korean GovernmentNational
   Research Foundation of KoreaKorean Government
   {[}NRF-2016R1D1A1B03935703]}},
Funding-Text = {{This work was supported by the National Research Foundation of Korea
   grant funded by the Korean Government (NRF-2016R1D1A1B03935703).}},
Number-of-Cited-References = {{68}},
Times-Cited = {{2}},
Usage-Count-Last-180-days = {{3}},
Usage-Count-Since-2013 = {{4}},
Journal-ISO = {{Commun. Sci. Disord.-CSD}},
Doc-Delivery-Number = {{HH6DT}},
Unique-ID = {{ISI:000455819800008}},
DA = {{2019-10-28}},
}

@article{ ISI:000453531000012,
Author = {Catak, Esra Nur and Acik, Alper and Goksun, Tilbe},
Title = {{The relationship between handedness and valence: A gesture study}},
Journal = {{QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY}},
Year = {{2018}},
Volume = {{71}},
Number = {{12}},
Pages = {{2615-2626}},
Month = {{DEC}},
Abstract = {{People with different hand preferences assign positive and negative
   emotions to different sides of their bodies and produce co-speech
   gestures with their dominant hand when the content is positive. In this
   study, we investigated this side preference by handedness in both
   gesture comprehension and production. Participants watched faceless
   gesture videos with negative and positive content on eye tracker and
   were asked to retell the stories after each video. Results indicated no
   difference in looking preferences regarding being right- or left-handed.
   Yet, an effect of emotional valence was observed. Participants spent
   more time looking to the right (actor's left) when the information was
   positive and to the left (actor's right) when the information was
   negative. Participants' retelling of stories revealed a handedness
   effect only for different types of gestures (representational vs beat).
   Individuals used their dominant hands for beat gestures. For
   representational gestures, while the right-handers used their right
   hands more, the left-handers gestured using both hands equally. Overall,
   the lack of significant difference between handedness and emotional
   content in both comprehension and production levels suggests that
   body-specific mental representations may not extend to the
   conversational level.}},
Publisher = {{ROUTLEDGE JOURNALS, TAYLOR \& FRANCIS LTD}},
Address = {{2-4 PARK SQUARE, MILTON PARK, ABINGDON OX14 4RN, OXON, ENGLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Goksun, T (Reprint Author), Koc Univ, Dept Psychol, Rumelifeneri Yolu, TR-34450 Istanbul, Turkey.
   Catak, Esra Nur; Goksun, Tilbe, Koc Univ, Dept Psychol, Rumelifeneri Yolu, TR-34450 Istanbul, Turkey.
   Acik, Alper, Ozyegin Univ, Istanbul, Turkey.}},
DOI = {{10.1177/1747021817750110}},
ISSN = {{1747-0218}},
EISSN = {{1747-0226}},
Keywords = {{Handedness; body-specificity; gesture comprehension; gesture production}},
Keywords-Plus = {{BODY-SPECIFIC ASSOCIATIONS; CO-SPEECH GESTURES; SPACE; HAND; LATERALITY;
   SEX}},
Research-Areas = {{Psychology; Physiology}},
Web-of-Science-Categories  = {{Psychology, Biological; Physiology; Psychology; Psychology, Experimental}},
Author-Email = {{tgoksun@ku.edu.tr}},
ORCID-Numbers = {{Goksun, Tilbe/0000-0002-0190-7988}},
Funding-Acknowledgement = {{BAGEP Award (Turkish Science Academy Young Investigator Award)}},
Funding-Text = {{This research was partially supported by BAGEP Award (Turkish Science
   Academy Young Investigator Award) given to Tilbe Goksun. We would like
   to thank everyone in the Language and Cognition Lab at Koc University
   for their helpful comments in this research and Aylin Kuntay and Cala
   Aydn for their constructive feedback on the findings. We are grateful to
   Ayenur Karaduman for her help in stimulus preparation and Benay Bakurt,
   Atakan Kaya, Melike Hazr, and Bura zgi for their help in data collection
   and coding.}},
Number-of-Cited-References = {{28}},
Times-Cited = {{1}},
Usage-Count-Last-180-days = {{4}},
Usage-Count-Since-2013 = {{5}},
Journal-ISO = {{Q. J. Exp. Psychol.}},
Doc-Delivery-Number = {{HE6PE}},
Unique-ID = {{ISI:000453531000012}},
DA = {{2019-10-28}},
}

@article{ ISI:000449983900009,
Author = {Wu, Di and Gao, Yuntao and Miao, Danmin},
Title = {{USING AN EYE TRACKER TO MEASURE INFORMATION PROCESSING ACCORDING TO NEED
   FOR COGNITION LEVEL}},
Journal = {{SOCIAL BEHAVIOR AND PERSONALITY}},
Year = {{2018}},
Volume = {{46}},
Number = {{11}},
Pages = {{1869-1880}},
Month = {{NOV}},
Abstract = {{The need for cognition (NC) refers to the tendency for people to vary in
   the extent to which they engage in, and enjoy, cognitively effortful
   activities. However, few studies on NC have been conducted to
   investigate cognitive processes by using eye-tracking technology. Thus,
   we measured differences in eye movement between individuals high versus
   low in NC. We presented 43 undergraduates with persuasive messages on
   postponed retirement. Meanwhile, their eye movements were recorded using
   eye-tracking technology. Additionally, participants completed measures
   of attitude and recall of arguments. Our findings showed that
   participants high in NC recalled more arguments but did not form more
   favorable attitudes than did those low in NC. Furthermore, compared to
   those low in NC, those with high NC recorded longer eye fixation
   duration, more fixations, slower reading speed, and shorter saccade
   (movement) lengths. Finally, there were no differences between the two
   groups concerning the distribution (short, medium, and long fixations)
   and the proportion of regressions. Eye-tracking technology contributes
   to further understanding of characteristics of individuals high versus
   low in NC during reading.}},
Publisher = {{SOC PERSONALITY RES INC}},
Address = {{P O BOX 1539, PALMERSTON NORTH 5330, NEW ZEALAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Miao, DM (Reprint Author), Fourth Mil Med Univ, Dept Psychol, 169 Changle West Rd, Xian 710032, Shaanxi, Peoples R China.
   Wu, Di; Gao, Yuntao; Miao, Danmin, Fourth Mil Med Univ, Dept Psychol, 169 Changle West Rd, Xian 710032, Shaanxi, Peoples R China.}},
DOI = {{10.2224/sbp.7316}},
ISSN = {{0301-2212}},
EISSN = {{1179-6391}},
Keywords = {{persuasion; need for cognition; eye-tracking technology; cognitive
   processes}},
Keywords-Plus = {{MODERATING ROLE; MOVEMENTS}},
Research-Areas = {{Psychology}},
Web-of-Science-Categories  = {{Psychology, Social}},
Author-Email = {{dmmiao@fmmu.edu.cn}},
Number-of-Cited-References = {{30}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{2}},
Usage-Count-Since-2013 = {{12}},
Journal-ISO = {{Soc. Behav. Pers.}},
Doc-Delivery-Number = {{HA1OE}},
Unique-ID = {{ISI:000449983900009}},
DA = {{2019-10-28}},
}

@article{ ISI:000447615200001,
Author = {Lee, Tak Hyung and Kim, Minah and Kwak, Yoo Bin and Hwang, Wu Jeong and
   Kim, Taekwan and Choi, Jung-Seok and Kwon, Jun Soo},
Title = {{Altered Eye-Movement Patterns During Text Reading in
   Obsessive-Compulsive Disorder and Internet Gaming Disorder}},
Journal = {{FRONTIERS IN BEHAVIORAL NEUROSCIENCE}},
Year = {{2018}},
Volume = {{12}},
Month = {{OCT 18}},
Abstract = {{Obsessive-compulsive disorder (OCD) and internet gaming disorder (IGD),
   which are similar in that both involve repetitive behaviors and related
   with cognitive dysfunctions, frequently begin in early adolescence,
   which is a critical period for learning. Although the deterioration in
   cognitive functioning caused by these conditions may have adverse
   effects on information processing, such as text reading, there has been
   no comprehensive research on the objective indicators of altered reading
   patterns in these patients. Therefore, we evaluated eye-movement
   patterns during text reading in patients with OCD or IGD. In total, 20
   patients with OCD, 28 patients with IGD and 24 healthy controls (HCs)
   participated in the reading task using an eye tracker. We compared the
   fixation durations (FDs), saccade amplitudes and eye-movement
   regressions of the three groups during reading. We explored
   relationships between the parameters reflecting altered reading patterns
   and those reflecting the severity of clinical symptoms. The average FDs
   and forward saccade amplitudes did not differ significantly among the
   groups. There were more eye-movement regressions in patients with OCD
   than in patients with IGD and HCs. No correlation was found between
   altered eye-movement patterns during reading and the severity of
   clinical symptoms in any of the patient groups. The significantly
   increased number of regressions (NRs) in the OCD group during reading
   may reflect these patients' difficulties with inferential information
   processing, whereas the reading pattern in the IGD group is relatively
   intact. These findings suggest that patients with OCD and patients with
   IGD have different eye-movement patterns during reading reflecting
   distinct cognitive impairments in the two patient groups.}},
Publisher = {{FRONTIERS MEDIA SA}},
Address = {{AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Choi, JS (Reprint Author), Seoul Natl Univ, Dept Brain \& Cognit Sci, Coll Med, Seoul, South Korea.
   Choi, JS (Reprint Author), SMG SNU Boramae Med Ctr, Dept Psychiat, Seoul, South Korea.
   Lee, Tak Hyung; Kwak, Yoo Bin; Hwang, Wu Jeong; Kim, Taekwan; Kwon, Jun Soo, Seoul Natl Univ, Dept Brain \& Cognit Sci, Coll Nat Sci, Seoul, South Korea.
   Kim, Minah; Choi, Jung-Seok; Kwon, Jun Soo, Seoul Natl Univ, Dept Brain \& Cognit Sci, Coll Med, Seoul, South Korea.
   Choi, Jung-Seok, SMG SNU Boramae Med Ctr, Dept Psychiat, Seoul, South Korea.
   Kwon, Jun Soo, SNU MRC, Inst Human Behav Med, Seoul, South Korea.}},
DOI = {{10.3389/fnbeh.2018.00248}},
Article-Number = {{248}},
ISSN = {{1662-5153}},
Keywords = {{information processing; eye-movement; reading; obsessive-compulsive
   disorder; internet gaming disorder}},
Keywords-Plus = {{ADDICTIVE DISORDERS; ATTENTIONAL BIAS; GAME GENRE; IMPULSIVITY; ANXIETY;
   MODEL; NEUROPSYCHOLOGY; COMPREHENSION; ADOLESCENTS; INHIBITION}},
Research-Areas = {{Behavioral Sciences; Neurosciences \& Neurology}},
Web-of-Science-Categories  = {{Behavioral Sciences; Neurosciences}},
Author-Email = {{choijs73@gmail.com}},
ORCID-Numbers = {{kim, minah/0000-0001-8668-0817
   Hwang, Wu Jeong/0000-0002-2546-3103}},
Funding-Acknowledgement = {{National Research Foundation of KoreaNational Research Foundation of
   Korea {[}2014M3C7A1062894]}},
Funding-Text = {{This work was supported by a grant from the National Research Foundation
   of Korea (Grant No. 2014M3C7A1062894).}},
Number-of-Cited-References = {{57}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{2}},
Usage-Count-Since-2013 = {{5}},
Journal-ISO = {{Front. Behav. Neurosci.}},
Doc-Delivery-Number = {{GX3ID}},
Unique-ID = {{ISI:000447615200001}},
OA = {{DOAJ Gold, Green Published}},
DA = {{2019-10-28}},
}

@article{ ISI:000456452800008,
Author = {Zhan, Likan},
Title = {{Using Eye Movements Recorded in the Visual World Paradigm to Explore the
   Online Processing of Spoken Language}},
Journal = {{JOVE-JOURNAL OF VISUALIZED EXPERIMENTS}},
Year = {{2018}},
Number = {{140}},
Month = {{OCT}},
Abstract = {{In a typical eye tracking study using the visual world paradigm,
   participants' eye movements to objects or pictures in the visual
   workspace are recorded via an eye tracker as the participant produces or
   comprehends a spoken language describing the concurrent visual world.
   This paradigm has high versatility, as it can be used in a wide range of
   populations, including those who cannot read and/or who cannot overtly
   give their behavioral responses, such as preliterate children, elderly
   adults, and patients. More importantly, the paradigm is extremely
   sensitive to fine grained manipulations of the speech signal, and it can
   be used to study the online processing of most topics in language
   comprehension at multiple levels, such as the fine grained acoustic
   phonetic features, the properties of words, and the linguistic
   structures. The protocol described in this article illustrates how a
   typical visual world eye tracking study is conducted, with an example
   showing how the online processing of some semantically complex
   statements can be explored with the visual world paradigm.}},
Publisher = {{JOURNAL OF VISUALIZED EXPERIMENTS}},
Address = {{1 ALEWIFE CENTER, STE 200, CAMBRIDGE, MA 02140 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Zhan, LK (Reprint Author), Beijing Language \& Culture Univ, Sch Commun Sci, Inst Speech Pathol \& Brain Sci, Beijing, Peoples R China.
   Zhan, Likan, Beijing Language \& Culture Univ, Sch Commun Sci, Inst Speech Pathol \& Brain Sci, Beijing, Peoples R China.}},
DOI = {{10.3791/58086}},
Article-Number = {{e58086}},
ISSN = {{1940-087X}},
Keywords = {{Behavior; Issue 140; Eye tracking technique; visual world paradigm;
   spoken language; online processing; complex statement; generalized
   linear mixed model; binomial distribution; familywise error; Bonferroni
   adjustment}},
Keywords-Plus = {{SCALAR IMPLICATURES; SPEECH-PERCEPTION; RECOGNITION; INFORMATION;
   EYETRACKING; TRACKING; MODELS}},
Research-Areas = {{Science \& Technology - Other Topics}},
Web-of-Science-Categories  = {{Multidisciplinary Sciences}},
Author-Email = {{zhanlikan@hotmail.com}},
ResearcherID-Numbers = {{Zhan, Likan/B-5684-2019}},
ORCID-Numbers = {{Zhan, Likan/0000-0002-9275-3557}},
Funding-Acknowledgement = {{Science Foundation of Beijing Language and Cultural University under the
   Fundamental Research Funds for the Central Universities {[}15YJ050003]}},
Funding-Text = {{This research was supported by Science Foundation of Beijing Language
   and Cultural University under the Fundamental Research Funds for the
   Central Universities (Approval number 15YJ050003).}},
Number-of-Cited-References = {{51}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{2}},
Usage-Count-Since-2013 = {{4}},
Journal-ISO = {{J. Vis. Exp.}},
Doc-Delivery-Number = {{HI4WM}},
Unique-ID = {{ISI:000456452800008}},
DA = {{2019-10-28}},
}

@article{ ISI:000448093700024,
Author = {Meerhoff, L. A. and Bruneau, J. and Vu, A. and Olivier, A. -H and
   Pettre, J.},
Title = {{Guided by gaze: Prioritization strategy when navigating through a
   virtual crowd can be assessed through gaze activity}},
Journal = {{ACTA PSYCHOLOGICA}},
Year = {{2018}},
Volume = {{190}},
Pages = {{248-257}},
Month = {{OCT}},
Abstract = {{Modelling crowd behavior is essential for the management of mass events
   and pedestrian traffic. Current microscopic approaches consider the
   individual's behavior to predict the effect of individual actions in
   local interactions on the collective scale of the crowd motion. Recent
   developments in the use of virtual reality as an experimental tool have
   offered an opportunity to extend the understanding of these interactions
   in controlled and repeatable settings. Nevertheless, based on kinematics
   alone, it remains difficult to tease out how these interactions unfold.
   Therefore, we tested the hypothesis that gaze activity provides
   additional information about pedestrian interactions. Using an eye
   tracker, we recorded the participant's gaze behavior whilst navigating
   through a virtual crowd. Results revealed that gaze was consistently
   attracted to virtual walkers with the smallest values of distance at
   closest approach (DCA) and time to closest approach (TtCA), indicating a
   higher risk of collision. Moreover, virtual walkers gazed upon before an
   avoidance maneuver was initiated had a high risk of collision and were
   typically avoided in the subsequent avoidance maneuver. We argue that
   humans navigate through crowds by selecting only few interactions and
   that gaze reveals how a walker prioritizes these interactions. Moreover,
   we pose that combining kinematic and gaze data provides new
   opportunities for studying how interactions are selected by pedestrians
   walking through crowded dynamic environments.}},
Publisher = {{ELSEVIER}},
Address = {{RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Meerhoff, LA (Reprint Author), Univ Rennes, CNRS, INRIA, IRISA,UMR 6074, F-35000 Rennes, France.
   Meerhoff, LA (Reprint Author), Univ Rennes, INRIA, M2S, EA 7470, F-35000 Rennes, France.
   Meerhoff, L. A.; Bruneau, J.; Vu, A.; Olivier, A. -H; Pettre, J., Univ Rennes, CNRS, INRIA, IRISA,UMR 6074, F-35000 Rennes, France.
   Meerhoff, L. A.; Olivier, A. -H, Univ Rennes, INRIA, M2S, EA 7470, F-35000 Rennes, France.}},
DOI = {{10.1016/j.actpsy.2018.07.009}},
ISSN = {{0001-6918}},
EISSN = {{1873-6297}},
Keywords = {{Crowd navigation; Pareto optimality; Pedestrian; Virtual reality;
   Locomotion; Interaction neighborhood}},
Keywords-Plus = {{COLLISION-AVOIDANCE; BEHAVIOR}},
Research-Areas = {{Psychology}},
Web-of-Science-Categories  = {{Psychology, Experimental}},
Author-Email = {{rensmeerhoff@gmail.com}},
ResearcherID-Numbers = {{Meerhoff, Laurentius/P-1187-2018}},
ORCID-Numbers = {{Meerhoff, Laurentius/0000-0003-4386-0919}},
Funding-Acknowledgement = {{University of Rennes 2 (Scholarship), France; French National Research
   Agency, project Percolation, FranceFrench National Research Agency (ANR)
   {[}ANR-13-JS02-0008]; Brittany region, project SAD 2015-INTERACT, France
   {[}9297]}},
Funding-Text = {{This research has received funding from the University of Rennes 2
   (Scholarship), France; the French National Research Agency, project
   Percolation (ANR-13-JS02-0008), France; and Brittany region, project SAD
   2015-INTERACT (9297), France.}},
Number-of-Cited-References = {{29}},
Times-Cited = {{2}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{4}},
Journal-ISO = {{Acta Psychol.}},
Doc-Delivery-Number = {{GX9BO}},
Unique-ID = {{ISI:000448093700024}},
OA = {{Green Published}},
DA = {{2019-10-28}},
}

@article{ ISI:000447614900004,
Author = {Zivcevska, Marija and Blakeman, Alan and Lei, Shaobo and Goltz, Herbert
   C. and Wong, Agnes M. F.},
Title = {{Binocular Summation in Postillumination Pupil Response Driven by
   Melanopsin-Containing Retinal Ganglion Cells}},
Journal = {{INVESTIGATIVE OPHTHALMOLOGY \& VISUAL SCIENCE}},
Year = {{2018}},
Volume = {{59}},
Number = {{12}},
Pages = {{4968-4977}},
Month = {{OCT}},
Abstract = {{PURPOSE. To investigate how melanopsin-mediated intrinsically
   photosensitive retinal ganglion cell (ipRGC) signals are integrated
   binocularly using chromatic pupillometry. We hypothesized that if the
   melanopsin system is summative, there will be a greater postillumination
   pupillary response (PIPR) under binocular conditions after viewing
   bright blue light.
   METHODS. Pupillary responses in 10 visually normal participants were
   recorded with an eye tracker following full-field stimulation of red
   (long wavelength) and blue (short wavelength) light of equal intensity
   (dim: 0.1 cd {[}candela]/m(2), bright: 60 cd/m(2)) and duration (400
   ms). Individual monocular (left eye) pupil responses were measured
   first, followed by binocular responses. Each participant repeated the
   same protocol on 3 separate days, at similar times of day. PIPR was
   recorded for bright red and blue conditions only, whereas maximum
   pupillary constriction (MPC) was measured under both bright and dim
   conditions during red and blue light stimulation.
   RESULTS. Bright blue light stimulation induced greater PIPR under
   binocular than monocular viewing conditions (F(1,9) = 79.52, P < 0.001).
   Bright red light stimulation induced minimal PIPR and showed no
   significant difference between viewing conditions post Bonferroni
   correction (F(1,9) = 5.49, P = 0.04). MPC was greater during binocular
   than monocular viewing conditions for all light stimuli, but was
   greatest following blue compared to red light stimulation.
   CONCLUSIONS. A larger PIPR was induced using a binocular than a
   monocular full-field stimulus of equal intensity and duration,
   demonstrating that melanopsin-mediated ipRGC signals are summated
   binocularly. This study expands our current understanding of the
   melanopsin system and may be used as an additional marker to stratify
   diseases according to their etiologies.}},
Publisher = {{ASSOC RESEARCH VISION OPHTHALMOLOGY INC}},
Address = {{12300 TWINBROOK PARKWAY, ROCKVILLE, MD 20852-1606 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Wong, AMF (Reprint Author), Hosp Sick Children, Dept Ophthalmol \& Vis Sci, 555 Univ Ave, Toronto, ON M5G 1X8, Canada.
   Zivcevska, Marija, Univ Toronto, Inst Med Sci, Toronto, ON, Canada.
   Zivcevska, Marija; Goltz, Herbert C.; Wong, Agnes M. F., Hosp Sick Children, Program Neurosci \& Mental Hlth, Toronto, ON, Canada.
   Blakeman, Alan; Lei, Shaobo; Goltz, Herbert C.; Wong, Agnes M. F., Univ Toronto, Dept Ophthalmol \& Vis Sci, Toronto, ON, Canada.
   Goltz, Herbert C.; Wong, Agnes M. F., Toronto Western Hosp, Krembil Res Inst, Toronto, ON, Canada.
   Wong, Agnes M. F., Hosp Sick Children, Dept Ophthalmol \& Vis Sci, 555 Univ Ave, Toronto, ON M5G 1X8, Canada.}},
DOI = {{10.1167/iovs.18-24639}},
ISSN = {{0146-0404}},
EISSN = {{1552-5783}},
Keywords = {{melanopsin; binocular; monocular; PIPR; summation}},
Keywords-Plus = {{FIELD CHROMATIC PUPILLOMETRY; INNATE MICROSTRABISMIC CATS; PRETECTAL
   OLIVARY NUCLEUS; LIGHT REFLEX; MACAQUE MONKEY; CONE; ROD; LUMINANCE;
   PROJECTIONS; IRRADIANCE}},
Research-Areas = {{Ophthalmology}},
Web-of-Science-Categories  = {{Ophthalmology}},
Author-Email = {{agnes.wong@sickkids.ca}},
Funding-Acknowledgement = {{Canada Foundation for InnovationCanada Foundation for Innovation; John
   and Melinda Thompson Endowment Fund for Vision Neuroscience; Department
   of Ophthalmology and Vision Sciences at The Hospital for Sick Children}},
Funding-Text = {{Supported by the Canada Foundation for Innovation, John and Melinda
   Thompson Endowment Fund for Vision Neuroscience, and the Department of
   Ophthalmology and Vision Sciences at The Hospital for Sick Children.}},
Number-of-Cited-References = {{67}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{3}},
Journal-ISO = {{Invest. Ophthalmol. Vis. Sci.}},
Doc-Delivery-Number = {{GX3IB}},
Unique-ID = {{ISI:000447614900004}},
DA = {{2019-10-28}},
}

@article{ ISI:000445987100005,
Author = {Pakkert, Martijn and Rosemann, Alexander L. P. and van Duijnhoven,
   Juliette and Donners, Maurice A. H.},
Title = {{Glare quantification for indoor volleyball}},
Journal = {{BUILDING AND ENVIRONMENT}},
Year = {{2018}},
Volume = {{143}},
Pages = {{48-58}},
Month = {{OCT 1}},
Abstract = {{Sports facilities all over the world apply LED lighting. The combination
   of high luminance and small luminous surfaces causes a high probability
   of glare and LED lighting contains these specifications. There are
   specific situations for which validated glare models exist, such as
   offices or outdoor soccer fields, although indoor sports facilities are
   not one of them. Additionally, we do not know the degree to which
   lighting may impact athletes' performance. Contradictory research exists
   on whether glare decreases task performance, and whether any decrease is
   due to discomfort glare or disability glare. In the current research,
   objective performance measurements were conducted on a volleyball court
   with both amateur and professional athletes from the Dutch national
   indoor volleyball competition-the Eredivisie. An eye tracker was used to
   see if gaze data contributed to a better understanding of performance or
   the subjective experience of glare. The results show that athletes'
   performance was not decreased due to glare, although the subjective
   experiences, measured by discomfort and non-acceptance, increased
   significantly. The current unified glare rating (UGR) glare model has a
   strong correlation with the discomfort findings, although the
   combination of source luminance and background luminance predicts
   discomfort and non-acceptance even better. This paper demonstrates that
   existing glare models perform well for indoor sports environments.}},
Publisher = {{PERGAMON-ELSEVIER SCIENCE LTD}},
Address = {{THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Pakkert, M (Reprint Author), Signify, Boschdijk 525, NL-5621 JG Eindhoven, Netherlands.
   Rosemann, Alexander L. P.; van Duijnhoven, Juliette, Eindhoven Univ Technol, Eindhoven, Netherlands.
   Pakkert, Martijn; Donners, Maurice A. H., Signify, Boschdijk 525, NL-5621 JG Eindhoven, Netherlands.}},
DOI = {{10.1016/j.buildenv.2018.06.053}},
ISSN = {{0360-1323}},
EISSN = {{1873-684X}},
Keywords = {{Sports lighting; Glare; Eye tracking; Discomfort; Disability}},
Keywords-Plus = {{DISCOMFORT GLARE; EYE-MOVEMENT; PERFORMANCE; BASKETBALL; INFORMATION;
   EXPOSURE}},
Research-Areas = {{Construction \& Building Technology; Engineering}},
Web-of-Science-Categories  = {{Construction \& Building Technology; Engineering, Environmental;
   Engineering, Civil}},
Author-Email = {{martijn.pakkert.m@signify.com}},
Funding-Acknowledgement = {{Signify}},
Funding-Text = {{The participants received an incentive for participating, made possible
   by funds provided by Signify. I wish to thank the volleyball club Taurus
   and the Student Sports Centre Eindhoven for their cooperation and for
   providing us the opportunity to conduct experiments in their sports
   halls.}},
Number-of-Cited-References = {{54}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{2}},
Usage-Count-Since-2013 = {{7}},
Journal-ISO = {{Build. Environ.}},
Doc-Delivery-Number = {{GV3IH}},
Unique-ID = {{ISI:000445987100005}},
DA = {{2019-10-28}},
}

@article{ ISI:000444435300002,
Author = {de Smet, Milou J. R. and Leijten, Marielle and Van Waes, Luuk},
Title = {{Exploring the Process of Reading During Writing Using Eye Tracking and
   Keystroke Logging}},
Journal = {{WRITTEN COMMUNICATION}},
Year = {{2018}},
Volume = {{35}},
Number = {{4}},
Pages = {{411-447}},
Month = {{OCT}},
Abstract = {{This study aims to explore the process of reading during writing. More
   specifically, it investigates whether a combination of keystroke logging
   data and eye tracking data yields a better understanding of cognitive
   processes underlying fluent and nonfluent text production. First, a
   technical procedure describes how writing process data from the
   keystroke logging program Inputlog are merged with reading process data
   from the Tobii TX300 eye tracker. Next, a theoretical schema on reading
   during writing is presented, which served as a basis for the observation
   context we created for our experiment. This schema was tested by
   observing 24 university students in professional communication (skilled
   writers) who typed short sentences that were manipulated to elicit
   fluent or nonfluent writing. The experimental sentences were organized
   into four different conditions, aiming at (a) fluent writing, (b)
   reflection about correct spelling of homophone verbs, (c) local
   revision, and (d) global revision. Results showed that it is possible to
   manipulate degrees of nonfluent writing in terms of time on task and
   percentage of nonfluent key transitions. However, reading behavior was
   affected only for the conditions that explicitly required revision. This
   suggests that nonfluent writing does not always affect the reading
   behavior, supporting the parallel and cascading processing hypothesis.}},
Publisher = {{SAGE PUBLICATIONS INC}},
Address = {{2455 TELLER RD, THOUSAND OAKS, CA 91320 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Van Waes, L (Reprint Author), Univ Antwerp, Fac Business \& Econ, Dept Management, Prinsstr 13,C-457, B-2000 Antwerp, Belgium.
   de Smet, Milou J. R.; Leijten, Marielle; Van Waes, Luuk, Univ Antwerp, Fac Business \& Econ, Dept Management, Prinsstr 13,C-457, B-2000 Antwerp, Belgium.}},
DOI = {{10.1177/0741088318788070}},
ISSN = {{0741-0883}},
EISSN = {{1552-8472}},
Keywords = {{writing process; reading process; eye tracking; keystroke logging;
   fluent writing; nonfluent writing; individual pause threshold}},
Keywords-Plus = {{MULTILEVEL ANALYSIS; COGNITIVE-PROCESSES; ERROR-CORRECTION;
   WORD-FREQUENCY; TEXT QUALITY; COORDINATION; EYETRACKING; MOVEMENTS;
   PAUSES; PERFORMANCE}},
Research-Areas = {{Communication}},
Web-of-Science-Categories  = {{Communication}},
Author-Email = {{luuk.vanwaes@uantwerpen.be}},
ResearcherID-Numbers = {{Van Waes, Luuk/A-9373-2012}},
Number-of-Cited-References = {{76}},
Times-Cited = {{2}},
Usage-Count-Last-180-days = {{4}},
Usage-Count-Since-2013 = {{11}},
Journal-ISO = {{Writ. Commun.}},
Doc-Delivery-Number = {{GT3WR}},
Unique-ID = {{ISI:000444435300002}},
DA = {{2019-10-28}},
}

@article{ ISI:000439751700001,
Author = {Takacs, Zsofia K. and Bus, Adriana G.},
Title = {{How pictures in picture storybooks support young children's story
   comprehension: An eye-tracking experiment}},
Journal = {{JOURNAL OF EXPERIMENTAL CHILD PSYCHOLOGY}},
Year = {{2018}},
Volume = {{174}},
Pages = {{1-12}},
Month = {{OCT}},
Abstract = {{In a within-participant design, 41 children (mean age = 64 months, range
   = 50-81) listened to brief stories in four conditions. Written text was
   present on the screen in all conditions (similar to the typical
   storybook experience) but combined with other sources of information:
   (a) only oral narration, (b) oral narration and a picture that was
   congruent with the narration, (c) oral narration and an incongruent
   picture, and (d) only a picture but no oral narration. Children's eye
   movements while looking at the screen were recorded with an eye-tracker.
   An important finding was that a congruent picture contributed
   substantially to children's story retellings, more so than a picture
   that was incongruent with the narration. The eye-tracking data showed
   that children explored pictures in a way that they could maximally
   integrate the narration and the picture. Consequences for interactive
   reading and picture storybook format are discussed. (C) 2018 Elsevier
   Inc. All rights reserved.}},
Publisher = {{ELSEVIER SCIENCE INC}},
Address = {{360 PARK AVE SOUTH, NEW YORK, NY 10010-1710 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Takacs, ZK (Reprint Author), Etitvos Lorand Univ, Inst Educ, Egyet Ter 1-3, H-1053 Budapest, Hungary.
   Takacs, Zsofia K.; Bus, Adriana G., Etitvos Lorand Univ, Inst Educ, Egyet Ter 1-3, H-1053 Budapest, Hungary.
   Bus, Adriana G., Vrije Univ Amsterdam, Dept Language Literature \& Commun, NL-1081 HV Amsterdam, Netherlands.}},
DOI = {{10.1016/j.jecp.2018.04.013}},
ISSN = {{0022-0965}},
EISSN = {{1096-0457}},
Keywords = {{Picture storybooks; Dual coding; Eye-tracking; Kindergarten; Multimedia
   learning; Experiment}},
Keywords-Plus = {{READ}},
Research-Areas = {{Psychology}},
Web-of-Science-Categories  = {{Psychology, Developmental; Psychology, Experimental}},
Author-Email = {{takacs.zsofia@ppk.elte.hu}},
Funding-Acknowledgement = {{Netherlands Organisation for Scientific Research (NWO)Netherlands
   Organization for Scientific Research (NWO) {[}411-07-216]; Hungarian
   National Research, Development and Innovation Office {[}PD121297];
   UNKP-17-4 New National Excellence Program of the Ministry of Human
   Capacities {[}UNKP-17-4-I-ELTE-125]}},
Funding-Text = {{This work was supported by the Netherlands Organisation for Scientific
   Research (NWO; Grant 411-07-216), the Hungarian National Research,
   Development and Innovation Office (Grant PD121297), and the UNKP-17-4
   New National Excellence Program of the Ministry of Human Capacities
   (Grant UNKP-17-4-I-ELTE-125).}},
Number-of-Cited-References = {{21}},
Times-Cited = {{2}},
Usage-Count-Last-180-days = {{4}},
Usage-Count-Since-2013 = {{45}},
Journal-ISO = {{J. Exp. Child Psychol.}},
Doc-Delivery-Number = {{GO1WT}},
Unique-ID = {{ISI:000439751700001}},
DA = {{2019-10-28}},
}

@article{ ISI:000441687500006,
Author = {Jeelani, Idris and Han, Kevin and Albert, Alex},
Title = {{Automating and scaling personalized safety training using eye-tracking
   data}},
Journal = {{AUTOMATION IN CONSTRUCTION}},
Year = {{2018}},
Volume = {{93}},
Pages = {{63-77}},
Month = {{SEP}},
Abstract = {{Research has shown that a large proportion of hazards remain
   unrecognized, which expose construction workers to unanticipated safety
   risks. Recent studies have also found that a strong correlation exists
   between viewing patterns of workers, captured using eye-tracking
   devices, and their hazard recognition performance. Therefore, it is
   important to analyze the viewing patterns of workers to gain a better
   understanding of their hazard recognition performance. From the training
   standpoint, scan paths and attention maps, generated using eye tracking
   technology, can be used effectively to provide personalized and focused
   feedback to workers. Such feedback is used to communicate the search
   process deficiency to workers in order to trigger self-reflection and
   subsequently improve their hazard recognition performance. This paper
   proposes a computer vision-based method that tracks workers on a
   construction site and automatically locates their fixation points,
   collected using a wearable eye-tracker, on a 3D point cloud. This data
   is then used to analyze their viewing behavior and compute their
   attention distribution. The presented case studies validate the proposed
   method.}},
Publisher = {{ELSEVIER SCIENCE BV}},
Address = {{PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Jeelani, I (Reprint Author), North Carolina State Univ, Dept Civil Construct \& Environm Engn, 2501 Stinson Dr, Raleigh, NC 27695 USA.
   Jeelani, Idris; Han, Kevin; Albert, Alex, North Carolina State Univ, Dept Civil Construct \& Environm Engn, 2501 Stinson Dr, Raleigh, NC 27695 USA.}},
DOI = {{10.1016/j.autcon.2018.05.006}},
ISSN = {{0926-5805}},
EISSN = {{1872-7891}},
Keywords = {{Hazard recognition; Construction safety; 3D-reconstruction;
   Eye-tracking; Computer vision; Personalized training}},
Keywords-Plus = {{HAZARD IDENTIFICATION; COMPUTER VISION; POINT CLOUDS; CONSTRUCTION;
   INTEGRATION; PROGRESS; WORKERS; SEARCH; MODEL}},
Research-Areas = {{Construction \& Building Technology; Engineering}},
Web-of-Science-Categories  = {{Construction \& Building Technology; Engineering, Civil}},
Author-Email = {{idrisj@ncsu.edu}},
ORCID-Numbers = {{Han, Kevin/0000-0002-2995-8381}},
Number-of-Cited-References = {{79}},
Times-Cited = {{9}},
Usage-Count-Last-180-days = {{11}},
Usage-Count-Since-2013 = {{26}},
Journal-ISO = {{Autom. Constr.}},
Doc-Delivery-Number = {{GQ5AH}},
Unique-ID = {{ISI:000441687500006}},
DA = {{2019-10-28}},
}

@article{ ISI:000433245400006,
Author = {Sun, Liang and Shao, Hua and Li, Shuyang and Huang, Xiaoxun and Yang,
   Wenyan},
Title = {{Integrated Application of Eye Movement Analysis and Beauty Estimation in
   the Visual Landscape Quality Estimation of Urban Waterfront Park}},
Journal = {{INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE}},
Year = {{2018}},
Volume = {{32}},
Number = {{9}},
Month = {{SEP}},
Abstract = {{Beauty estimation is a common method for landscape quality estimation,
   although it has some limitations. With eye tracker, the visual behaviors
   of the subjects during the estimation can be recorded. Through the
   analyses of heat maps, path maps and eye movement data, the
   psychological changes of the subjects and the underlying law of beauty
   aesthetic can be understood, which will provide supplementation to
   beauty estimation. This paper studied the beauty estimation of urban
   waterfront parks and proofed that the landscape quality estimation
   method focussing on beauty estimation and assisted by eye movement
   tracking is feasible. It can improve the objectiveness and accuracy of
   landscape quality estimation to some extent and provide a comprehensive
   understanding of the effects and combination law of landscape
   characteristic elements.}},
Publisher = {{WORLD SCIENTIFIC PUBL CO PTE LTD}},
Address = {{5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Sun, L (Reprint Author), China Univ Min \& Technol, Architecture, Xuzhou, Jiangsu, Peoples R China.
   Sun, Liang; Shao, Hua; Li, Shuyang; Huang, Xiaoxun; Yang, Wenyan, China Univ Min \& Technol, Architecture, Xuzhou, Jiangsu, Peoples R China.
   Shao, Hua, Yancheng Inst Technol, Landscape Planning \& Design, Yancheng, Peoples R China.}},
DOI = {{10.1142/S0218001418560104}},
Article-Number = {{1856010}},
ISSN = {{0218-0014}},
EISSN = {{1793-6381}},
Keywords = {{Eye movement analysis; beauty estimation; urban waterfront park}},
Keywords-Plus = {{PERCEPTION; TRACKING}},
Research-Areas = {{Computer Science}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence}},
Author-Email = {{sunliang@cumt.edu.cn}},
ORCID-Numbers = {{Sun, Liang/0000-0002-0758-8447}},
Funding-Acknowledgement = {{National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China {[}41171441]; Ministry of Housing and Urban Science
   and Technology Project funded of China {[}2015-K1-031]; Building Energy
   Conservation and Construction Technology Collaborative Innovation Center
   for Open Fund of Jiangsu Province {[}SJXTY1511]; China University of
   Mining and Research Funding for basic scientific research projects
   {[}2011QNB14]}},
Funding-Text = {{The authors acknowledge the National Natural Science Foundation of China
   (Grant No. 41171441), the Ministry of Housing and Urban Science and
   Technology Project funded of China (Grant No. 2015-K1-031), the Building
   Energy Conservation and Construction Technology Collaborative Innovation
   Center for Open Fund of Jiangsu Province (Grant No. SJXTY1511) and the
   China University of Mining and Research Funding for basic scientific
   research projects (Grant No. 2011QNB14).}},
Number-of-Cited-References = {{42}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{9}},
Usage-Count-Since-2013 = {{36}},
Journal-ISO = {{Int. J. Pattern Recognit. Artif. Intell.}},
Doc-Delivery-Number = {{GH2QB}},
Unique-ID = {{ISI:000433245400006}},
DA = {{2019-10-28}},
}

@article{ ISI:000444759200013,
Author = {Harrar, Vanessa and Le Trung, William and Malienko, Anton and Khan,
   Aarlenne Zein},
Title = {{A nonvisual eye tracker calibration method for video-based tracking}},
Journal = {{JOURNAL OF VISION}},
Year = {{2018}},
Volume = {{18}},
Number = {{9}},
Month = {{SEP}},
Abstract = {{Video-based eye trackers have enabled major advancements in our
   understanding of eye movements through their ease of use and their
   non-invasiveness. One necessity to obtain accurate eye recordings using
   video-based trackers is calibration. The aim of the current study was to
   determine the feasibility and reliability of alternative calibration
   methods for scenarios in which the standard visual-calibration is not
   possible. Fourteen participants were tested using the EyeLink 1000 Plus
   video-based eye tracker, and each completed the following 5-point
   calibration methods: 1) standard visual-target calibration, 2) described
   calibration where participants were provided with verbal instructions
   about where to direct their eyes (without vision of the screen), 3)
   proprioceptive calibration where participants were asked to look at
   their hidden finger, 4) replacement calibration, where the visual
   calibration was performed by 3 different people; the calibrators were
   temporary substitutes for the participants. Following calibration,
   participants performed a simple visually-guided saccade task to 16
   randomly presented targets on a grid. We found that precision errors
   were comparable across the alternative calibration methods. In terms of
   accuracy, compared to the standard calibration, nonvisual calibration
   methods (described and proprioception) led to significantly larger
   errors, whilst the replacement calibration method had much smaller
   errors. In conditions where calibration is not possible, for example
   when testing blind or visually impaired people who are unable to foveate
   the calibration targets, we suggest that using a single stand-in to
   perform the calibration is a simple and easy alternative calibration
   method, which should only cause a minimal decrease in accuracy.}},
Publisher = {{ASSOC RESEARCH VISION OPHTHALMOLOGY INC}},
Address = {{12300 TWINBROOK PARKWAY, ROCKVILLE, MD 20852-1606 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Khan, AZ (Reprint Author), Univ Montreal, Sch Optometry, Vis Attent \& Act Lab VISATTAC, Montreal, PQ, Canada.
   Harrar, Vanessa; Le Trung, William; Malienko, Anton; Khan, Aarlenne Zein, Univ Montreal, Sch Optometry, Vis Attent \& Act Lab VISATTAC, Montreal, PQ, Canada.}},
DOI = {{10.1167/18.9.13}},
Article-Number = {{13}},
ISSN = {{1534-7362}},
Keywords = {{touch; auditory; multisensory; amodal; crossmodal}},
Keywords-Plus = {{GAZE TRACKING; MOVEMENTS; SACCADES}},
Research-Areas = {{Ophthalmology}},
Web-of-Science-Categories  = {{Ophthalmology}},
Author-Email = {{aarlenne.khan@umontreal.ca}},
Funding-Acknowledgement = {{National Sciences and Engineering Research Council of CanadaNatural
   Sciences and Engineering Research Council of Canada; Canada Research
   Chairs ProgramCanada Research Chairs}},
Funding-Text = {{AZK was funded by National Sciences and Engineering Research Council of
   Canada and the Canada Research Chairs Program.}},
Number-of-Cited-References = {{39}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{7}},
Journal-ISO = {{J. Vision}},
Doc-Delivery-Number = {{GT8BU}},
Unique-ID = {{ISI:000444759200013}},
OA = {{DOAJ Gold}},
DA = {{2019-10-28}},
}

@article{ ISI:000442603600005,
Author = {Pierdicca, Roberto and Paolanti, Marina and Naspetti, Simona and
   Mandolesi, Serena and Zanoli, Raffaele and Frontoni, Emanuele},
Title = {{User-Centered Predictive Model for Improving Cultural Heritage Augmented
   Reality Applications: An HMM-Based Approach for Eye-Tracking Data}},
Journal = {{JOURNAL OF IMAGING}},
Year = {{2018}},
Volume = {{4}},
Number = {{8}},
Month = {{AUG}},
Abstract = {{Today, museum visits are perceived as an opportunity for individuals to
   explore and make up their own minds. The increasing technical
   capabilities of Augmented Reality (AR) technology have raised audience
   expectations, advancing the use of mobile AR in cultural heritage (CH)
   settings. Hence, there is the need to define a criteria, based on users'
   preference, able to drive developers and insiders toward a more
   conscious development of AR-based applications. Starting from previous
   research (performed to define a protocol for understanding the visual
   behaviour of subjects looking at paintings), this paper introduces a
   truly predictive model of the museum visitor's visual behaviour,
   measured by an eye tracker. A Hidden Markov Model (HMM) approach is
   presented, able to predict users' attention in front of a painting.
   Furthermore, this research compares users' behaviour between adults and
   children, expanding the results to different kind of users, thus
   providing a reliable approach to eye trajectories. Tests have been
   conducted defining areas of interest (AOI) and observing the most
   visited ones, attempting the prediction of subsequent transitions
   between AOIs. The results demonstrate the effectiveness and suitability
   of our approach, with performance evaluation values that exceed 90\%.}},
Publisher = {{MDPI}},
Address = {{ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Paolanti, M (Reprint Author), Univ Politecn Marche, Dipartimento Ingn Informaz, I-60131 Ancona, Italy.
   Pierdicca, Roberto, Univ Politecn Marche, Dipartimento Ingn Civile Edile \& Architettura, I-60131 Ancona, Italy.
   Paolanti, Marina; Frontoni, Emanuele, Univ Politecn Marche, Dipartimento Ingn Informaz, I-60131 Ancona, Italy.
   Naspetti, Simona; Mandolesi, Serena, Univ Politecn Marche, Dept Mat Environm Sci \& Urban Planning, I-60131 Ancona, Italy.
   Zanoli, Raffaele, Univ Politecn Marche, Dept Agr Food \& Environm Sci, I-60131 Ancona, Italy.}},
DOI = {{10.3390/jimaging4080101}},
Article-Number = {{101}},
ISSN = {{2313-433X}},
Keywords = {{hidden markov models; eye-tracking; augmented reality applications;
   cultural heritage}},
Keywords-Plus = {{HIDDEN MARKOV-MODELS; RECOGNITION; MOVEMENT; MUSEUM; YARBUS; SYSTEM}},
Research-Areas = {{Imaging Science \& Photographic Technology}},
Web-of-Science-Categories  = {{Imaging Science \& Photographic Technology}},
Author-Email = {{r.pierdicca@staff.univpm.it
   m.paolanti@staff.univpm.it
   naspetti@agrecon.univpm.it
   mandolesi@agrecon.univpm.it
   zanoli@agrecon.univpm.it
   e.frontoni@staff.it}},
ResearcherID-Numbers = {{Mandolesi, Serena/H-8759-2017
   Pierdicca, Roberto/B-3193-2016
   Zanoli, Raffaele/I-4476-2012
   Naspetti, Simona/C-1143-2013
   }},
ORCID-Numbers = {{Mandolesi, Serena/0000-0001-5565-6902
   Pierdicca, Roberto/0000-0002-9160-834X
   Zanoli, Raffaele/0000-0002-7108-397X
   Naspetti, Simona/0000-0003-0778-2386
   Paolanti, Marina/0000-0002-5523-7174}},
Number-of-Cited-References = {{41}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{3}},
Journal-ISO = {{J. Imaging}},
Doc-Delivery-Number = {{GR4RH}},
Unique-ID = {{ISI:000442603600005}},
OA = {{DOAJ Gold}},
DA = {{2019-10-28}},
}

@article{ ISI:000438350500011,
Author = {Kaefer, Tanya},
Title = {{The role of topic-related background knowledge in visual attention to
   illustration and children's word learning during shared book reading}},
Journal = {{JOURNAL OF RESEARCH IN READING}},
Year = {{2018}},
Volume = {{41}},
Number = {{3}},
Pages = {{582-596}},
Month = {{AUG}},
Abstract = {{The purpose of this study was to examine how background knowledge of a
   topic may influence children's attention to different elements of
   storybook illustrations and how that influences word learning. Forty-one
   kindergarten students were administered a test about a familiar topic
   (i.e., birds). Participants were then read either a fictional story
   about a familiar topic (birds) or a fictional story about a novel topic
   (wugs) on an eye-tracker monitor. Results suggest that, for children who
   heard the familiar story, those who knew more about the category were
   faster to orient to the illustration of the novel word than children
   with lower background knowledge. Accordingly, children who were faster
   to orient to the illustration were more likely to learn the word. These
   results may suggest that one mechanism by which background knowledge
   improves implicit learning in shared-book reading contexts is by guiding
   attention to the named elements of the illustrations.}},
Publisher = {{WILEY}},
Address = {{111 RIVER ST, HOBOKEN 07030-5774, NJ USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Kaefer, T (Reprint Author), Lakehead Univ, 955 Oliver Rd, Thunder Bay, ON P7B 5E1, Canada.
   Kaefer, Tanya, Lakehead Univ, Fac Educ, 955 Oliver Rd, Thunder Bay, ON P7B 5E1, Canada.}},
DOI = {{10.1111/1467-9817.12127}},
ISSN = {{0141-0423}},
EISSN = {{1467-9817}},
Keywords-Plus = {{EYE-MOVEMENT; VOCABULARY; COMPREHENSION; EXPLANATIONS; CONSTRUCTION;
   ACQUISITION; STORYBOOKS; OTHERS}},
Research-Areas = {{Education \& Educational Research; Psychology}},
Web-of-Science-Categories  = {{Education \& Educational Research; Psychology, Educational}},
Author-Email = {{tkaefer@lakeheadu.ca}},
ORCID-Numbers = {{Kaefer, Tanya/0000-0002-2562-0965}},
Funding-Acknowledgement = {{Lakehead University {[}1464135]}},
Funding-Text = {{Portions of this work were presented at the semiannual meeting of the
   Society for Research on Child Development, March 2015, Philadelphia, PA.
   We gratefully acknowledge funding from the Lakehead University Research
   Development Fund (grant 1464135). Many thanks to the children who
   participated in the research.}},
Number-of-Cited-References = {{34}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{13}},
Journal-ISO = {{J. Res. Read.}},
Doc-Delivery-Number = {{GM7GM}},
Unique-ID = {{ISI:000438350500011}},
DA = {{2019-10-28}},
}

@article{ ISI:000442151500017,
Author = {Lewis, Dawna E. and Smith, Nicholas A. and Spalding, Jody L. and
   Valente, Daniel L.},
Title = {{Looking Behavior and Audiovisual Speech Understanding in Children With
   Normal Hearing and Children With Mild Bilateral or Unilateral Hearing
   Loss}},
Journal = {{EAR AND HEARING}},
Year = {{2018}},
Volume = {{39}},
Number = {{4}},
Pages = {{783-794}},
Month = {{JUL-AUG}},
Abstract = {{Objectives: Visual information from talkers facilitates speech
   intelligibility for listeners when audibility is challenged by
   environmental noise and hearing loss. Less is known about how listeners
   actively process and attend to visual information from different talkers
   in complex multi-talker environments. This study tracked looking
   behavior in children with normal hearing (NH), mild bilateral hearing
   loss (MBHL), and unilateral hearing loss (UHL) in a complex multi-talker
   environment to examine the extent to which children look at talkers and
   whether looking patterns relate to performance on a speech-understanding
   task. It was hypothesized that performance would decrease as perceptual
   complexity increased and that children with hearing loss would perform
   more poorly than their peers with NH. Children with MBHL or UHL were
   expected to demonstrate greater attention to individual talkers during
   multi-talker exchanges, indicating that they were more likely to attempt
   to use visual information from talkers to assist in speech understanding
   in adverse acoustics. It also was of interest to examine whether MBHL,
   versus UHL, would differentially affect performance and looking
   behavior.
   Design: Eighteen children with NH, eight children with MBHL, and 10
   children with UHL participated (8-12 years). They followed audiovisual
   instructions for placing objects on a mat under three conditions: a
   single talker providing instructions via a video monitor, four possible
   talkers alternately providing instructions on separate monitors in front
   of the listener, and the same four talkers providing both target and
   nontarget information. Multi-talker background noise was presented at a
   5 dB signal-to-noise ratio during testing. An eye tracker monitored
   looking behavior while children performed the experimental task.
   Results: Behavioral task performance was higher for children with NH
   than for either group of children with hearing loss. There were no
   differences in performance between children with UHL and children with
   MBHL. Eye-tracker analysis revealed that children with NH looked more at
   the screens overall than did children with MBHL or UHL, though
   individual differences were greater in the groups with hearing loss.
   Listeners in all groups spent a small proportion of time looking at
   relevant screens as talkers spoke. Although looking was distributed
   across all screens, there was a bias toward the right side of the
   display. There was no relationship between overall looking behavior and
   performance on the task.
   Conclusions: The present study examined the processing of audiovisual
   speech in the context of a naturalistic task. Results demonstrated that
   children distributed their looking to a variety of sources during the
   task, but that children with NH were more likely to look at screens than
   were those with MBHL/UHL. However, all groups looked at the relevant
   talkers as they were speaking only a small proportion of the time.
   Despite variability in looking behavior, listeners were able to follow
   the audiovisual instructions and children with NH demonstrated better
   performance than children with MBHL/UHL. These results suggest that
   performance on some challenging multi-talker audiovisual tasks is not
   dependent on visual fixation to relevant talkers for children with NH or
   with MBHL/UHL.}},
Publisher = {{LIPPINCOTT WILLIAMS \& WILKINS}},
Address = {{TWO COMMERCE SQ, 2001 MARKET ST, PHILADELPHIA, PA 19103 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Lewis, DE (Reprint Author), Boys Town Natl Res Hosp, 555 N 30th St, Omaha, NE 68131 USA.
   Lewis, Dawna E.; Smith, Nicholas A.; Spalding, Jody L.; Valente, Daniel L., Boys Town Natl Res Hosp, Ctr Hearing Res, Omaha, NE 68131 USA.
   Valente, Daniel L., SAS Inst Inc, JMP Div, Cary, NC USA.}},
DOI = {{10.1097/AUD.0000000000000534}},
ISSN = {{0196-0202}},
EISSN = {{1538-4667}},
Keywords-Plus = {{VISUALLY-GUIDED ATTENTION; AUDITORY SPEECH; GAZE BEHAVIOR; SOUND
   LOCALIZATION; IMPAIRED CHILDREN; 2-TALKER MASKER; SCHOOL-CHILDREN;
   DEGRADED SPEECH; VISIBLE SPEECH; SHAPED NOISE}},
Research-Areas = {{Audiology \& Speech-Language Pathology; Otorhinolaryngology}},
Web-of-Science-Categories  = {{Audiology \& Speech-Language Pathology; Otorhinolaryngology}},
Author-Email = {{dawna.lewis@boystown.org}},
Funding-Acknowledgement = {{NIHUnited States Department of Health \& Human ServicesNational
   Institutes of Health (NIH) - USA {[}R03 DC009675, R03 DC009884, T32
   DC000013, P20 GM109023, P30 DC004662]}},
Funding-Text = {{This work was supported by NIH grants R03 DC009675, R03 DC009884, T32
   DC000013, P20 GM109023, and P30 DC004662.}},
Number-of-Cited-References = {{90}},
Times-Cited = {{1}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{13}},
Journal-ISO = {{Ear Hear.}},
Doc-Delivery-Number = {{GQ9WZ}},
Unique-ID = {{ISI:000442151500017}},
OA = {{Green Accepted}},
DA = {{2019-10-28}},
}

@article{ ISI:000437222800003,
Author = {Lavoie, Ewen B. and Valevicius, Aida M. and Boser, Quinn A. and Kovic,
   Ognjen and Vette, Albert H. and Pilarski, Patrick M. and Hebert,
   Jacqueline S. and Chapman, Craig S.},
Title = {{Using synchronized eye and motion tracking to determine high-precision
   eye-movement patterns during object-interaction tasks}},
Journal = {{JOURNAL OF VISION}},
Year = {{2018}},
Volume = {{18}},
Number = {{6}},
Month = {{JUN}},
Abstract = {{This study explores the role that vision plays in sequential object
   interactions. We used a head-mounted eye tracker and upper-limb motion
   capture to quantify visual behavior while participants performed two
   standardized functional tasks. By simultaneously recording eye and
   motion tracking, we precisely segmented participants' visual data using
   the movement data, yielding a consistent and highly functionally
   resolved data set of real-world object-interaction tasks. Our results
   show that participants spend nearly the full duration of a trial
   fixating on objects relevant to the task, little time fixating on their
   own hand when reaching toward an object, and slightly more time-
   although still very little-fixating on the object in their hand when
   transporting it. A consistent spatial and temporal pattern of fixations
   was found across participants. In brief, participants fixate an object
   to be picked up at least half a second before their hand arrives at the
   object and stay fixated on the object until they begin to transport it,
   at which point they shift their fixation directly to the drop-off
   location of the object, where they stay fixated until the object is
   successfully released. This pattern provides additional evidence of a
   common system for the integration of vision and object interaction in
   humans, and is consistent with theoretical frameworks hypothesizing the
   distribution of attention to future action targets as part of eye and
   hand-movement preparation. Our results thus aid the understanding of
   visual attention allocation during planning of object interactions both
   inside and outside the field of view.}},
Publisher = {{ASSOC RESEARCH VISION OPHTHALMOLOGY INC}},
Address = {{12300 TWINBROOK PARKWAY, ROCKVILLE, MD 20852-1606 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Chapman, CS (Reprint Author), Univ Alberta, Fac Kinesiol Sport \& Recreat, Edmonton, AB, Canada.
   Chapman, CS (Reprint Author), Univ Alberta, Neurosci \& Mental Hlth Inst, Edmonton, AB, Canada.
   Lavoie, Ewen B.; Chapman, Craig S., Univ Alberta, Fac Kinesiol Sport \& Recreat, Edmonton, AB, Canada.
   Valevicius, Aida M.; Boser, Quinn A.; Kovic, Ognjen; Pilarski, Patrick M., Univ Alberta, Dept Biomed Engn, Edmonton, AB, Canada.
   Kovic, Ognjen, Univ Alberta, Dept Med, Div Phys Med \& Rehabil, Edmonton, AB, Canada.
   Kovic, Ognjen, Univ Alberta, Dept Mech Engn, Edmonton, AB, Canada.
   Hebert, Jacqueline S., Glenrose Rehabil Hosp, Alberta Hlth Serv, Edmonton, AB, Canada.
   Vette, Albert H.; Hebert, Jacqueline S.; Chapman, Craig S., Univ Alberta, Neurosci \& Mental Hlth Inst, Edmonton, AB, Canada.
   Vette, Albert H.; Pilarski, Patrick M., Univ Alberta, Dept Med, Div Phys Med \& Rehabil, Edmonton, AB, Canada.}},
DOI = {{10.1167/18.6.18}},
Article-Number = {{18}},
ISSN = {{1534-7362}},
Keywords = {{visuomotor control; eye-hand coordination; visual attention; object
   interaction; eye- and motion-tracking synchronization; sequential object
   movement}},
Keywords-Plus = {{VISUAL INFORMATION; HAND COORDINATION; NATURAL BEHAVIOR; VISION;
   DEPLOYMENT; SEQUENCES; ATTENTION; CORTEX; MEMORY; GAZE}},
Research-Areas = {{Ophthalmology}},
Web-of-Science-Categories  = {{Ophthalmology}},
Author-Email = {{c.s.chapman@ualberta.ca}},
ResearcherID-Numbers = {{Hebert, Jacqueline/E-1881-2016}},
ORCID-Numbers = {{Hebert, Jacqueline/0000-0003-0788-0568}},
Funding-Acknowledgement = {{Defense Advanced Research Projects Agency (DARPA) BTO through the DARPA
   Contracts Management Office {[}N66001-15-C-4015]}},
Funding-Text = {{This work was sponsored by the Defense Advanced Research Projects Agency
   (DARPA) BTO under the auspices of Dr. Doug Weber and Dr. Al Emondi
   through the DARPA Contracts Management Office Grant/Contract No.
   N66001-15-C-4015.}},
Number-of-Cited-References = {{42}},
Times-Cited = {{3}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{3}},
Journal-ISO = {{J. Vision}},
Doc-Delivery-Number = {{GL5QG}},
Unique-ID = {{ISI:000437222800003}},
OA = {{DOAJ Gold}},
DA = {{2019-10-28}},
}

@article{ ISI:000435407200001,
Author = {Eckstein, Grant and Casper, Rachel and Chan, Jacob and Blackwell, Logan},
Title = {{Assessment of L2 student writing: Does teacher disciplinary background
   matter?}},
Journal = {{JOURNAL OF WRITING RESEARCH}},
Year = {{2018}},
Volume = {{10}},
Number = {{1}},
Pages = {{1-23}},
Month = {{JUN}},
Abstract = {{This preliminary study examines the rating behavior of five composition
   and five ESL writing teachers while evaluating a text from a
   university-level non-native (L2) English speaking student. Using an eye
   tracker, we measured raters' dwell times and reading behaviors across
   four areas of interest rhetoric, organization, vocabulary, and grammar.
   Results indicate that raters with differing disciplinary backgrounds
   read the text differently. L2 writing teachers tended to spend more time
   on and re-read the rhetorical, lexical, and grammatical features of the
   text while skipping over more of the grammar errors, while composition
   teachers read the text more deliberately. The findings suggest L2
   writing teachers were more prone to skim and scan for information on
   which to base a grade while composition teachers delayed rating
   decisions until after reviewing the entire text, which is corroborated
   in previous research. These findings can expand our understanding of how
   disciplinary background can influence rating processes, which can inform
   rater training procedures, especially in disciplinary writing contexts
   where L2 writing is judged by individuals with and without expertise in
   composition or second language writing. Moreover, it demonstrates the
   utility of eye-tracking methods to examine the cognitive processes
   associated with reading and scoring student writing.}},
Publisher = {{UNIV ANTWERP}},
Address = {{CAMPUS GROENENBORGER, 171 GROENENBORGERLAAN, ANTWERP, 2020, BELGIUM}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Eckstein, G (Reprint Author), Brigham Young Univ, 4071 JFSB, Provo, UT 84602 USA.
   Eckstein, Grant; Casper, Rachel; Chan, Jacob; Blackwell, Logan, Brigham Young Univ, 4071 JFSB, Provo, UT 84602 USA.}},
DOI = {{10.17239/jowr-2018.10.01.01}},
ISSN = {{2030-1006}},
EISSN = {{2294-3307}},
Keywords = {{Eye-tracking; Composition; L2; Writing and Assessment; Cognitive process}},
Keywords-Plus = {{EYE; TRACKING; MODEL; L1}},
Research-Areas = {{Education \& Educational Research}},
Web-of-Science-Categories  = {{Education \& Educational Research}},
Author-Email = {{grant\_eckstein@byu.edu}},
Number-of-Cited-References = {{41}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{8}},
Journal-ISO = {{J. Writ. Res.}},
Doc-Delivery-Number = {{GJ5FQ}},
Unique-ID = {{ISI:000435407200001}},
OA = {{DOAJ Gold}},
DA = {{2019-10-28}},
}

@article{ ISI:000453449600005,
Author = {Barcelos, Janinne and Gomes, Suely and Oliveira, Frederico},
Title = {{Eyetracking analysis of the use of photography in scientific
   dissemination}},
Journal = {{EM QUESTAO}},
Year = {{2018}},
Volume = {{24}},
Number = {{2}},
Pages = {{83-108}},
Month = {{MAY-AUG}},
Abstract = {{This paper investigates photography as an enabling tool in the process
   of understanding scientific knowledge by the general public. Our
   hypothesis is that scientific texts, associated with photography, have
   major possibility to spark the interest of general public and facilitate
   the understanding of the information. It is based on Flusser's theory
   that images such as photography provide less abstract thoughts than
   texts; in Samain's papers that classify photography as a valuable way to
   communicate; and other scholars theories such as Correia, Gruszynski and
   Castedo, Machado and Mora - who believe that speech and writing no
   longer need to be the only forms of scientific communication. The
   research is characterized as exploratory, with experimental - laboratory
   design and it adopts qualitative and quantitative approaches. The
   results show different patterns of reading, degree of attention and
   understanding of the scientific text, when subjects are presented to
   different text structures. The results show different patterns of
   reading, as much as different levels of attention and understanding,
   when the subjects were presented to diverse text structures (text with
   photos, text by itself and photos by themselves). The text with photos
   was better understood and considered easy to read, followed by the text
   by itself. The least understood material was the one that presented only
   photos. Participants who read texts considered that photos helped
   reading and apprehending its content. The group that read only photos
   missed the text. It is considered that the research hypothesis was
   confirmed.}},
Publisher = {{UNIV FEDERAL RIO GRANDE SUL, FAC BIBLIOTECONOMIA \& COMUNICACAO}},
Address = {{RUA RAMIRO BARCELOS, 2705, SALA 519, PORTO ALEGRE, RS 90040-060, BRAZIL}},
Type = {{Article}},
Language = {{Portuguese}},
Affiliation = {{Barcelos, J (Reprint Author), Univ Brasilia, Brasilia, DF, Brazil.
   Barcelos, Janinne, Univ Brasilia, Brasilia, DF, Brazil.
   Gomes, Suely; Oliveira, Frederico, Univ Fed Goias, Goiania, Go, Brazil.}},
DOI = {{10.19132/1808-5245242.83-108}},
ISSN = {{1807-8893}},
EISSN = {{1808-5245}},
Keywords = {{Popularization of science; Eye tracker; Photography}},
Research-Areas = {{Information Science \& Library Science}},
Web-of-Science-Categories  = {{Information Science \& Library Science}},
Author-Email = {{janbarcelos@hotmail.com
   suelyhenriquegomes@gmail.com
   freddroliveira@gmail.com}},
ORCID-Numbers = {{Oliveira, Frederico/0000-0001-5653-5715}},
Number-of-Cited-References = {{25}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Journal-ISO = {{Em. Questao}},
Doc-Delivery-Number = {{HE5SZ}},
Unique-ID = {{ISI:000453449600005}},
OA = {{DOAJ Gold}},
DA = {{2019-10-28}},
}

@article{ ISI:000425558800001,
Author = {Huang, Yi-Ting},
Title = {{The female gaze: Content composition and slot position in personalized
   banner ads, and how they influence visual attention in online shoppers}},
Journal = {{COMPUTERS IN HUMAN BEHAVIOR}},
Year = {{2018}},
Volume = {{82}},
Pages = {{1-15}},
Month = {{MAY}},
Abstract = {{With the overload of information on the internet today, consumer
   attention is becoming increasingly precious. Online retailers often use
   banner advertising for products that consumers have previously viewed so
   as to achieve retargeting. This study examined the attention that female
   consumers paid to product-based personalized banner ads for online
   apparel retailers, and how this attention was influenced by visual
   content composition and slot position. Using a screen eye tracker, we
   analyzed the interaction effects of ad slot position (content relevance,
   top-center or right sidebar slot on portal site) and the content
   composition (image-text ratio, discount information, use of models) of
   personalized banner ads on relevant eye movement data. This data allowed
   for increased understanding of the combinations of ad content/layout
   composition and slot position that gain the most attention. The results
   revealed that placing apparel banner ads next to articles associated
   with apparel and fashion increases the overall amount of attention that
   consumers give the ad. Furthermore, ads in the top-center slot on portal
   sites receive greater total contact time and number of fixations than
   those in the right sidebar. We further found that adding discount
   information and using models to display the garment in banner ads can
   effectively increase the total contact time and number of fixations. (C)
   2017 Elsevier Ltd. All rights reserved.}},
Publisher = {{PERGAMON-ELSEVIER SCIENCE LTD}},
Address = {{THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Huang, YT (Reprint Author), Chung Yuan Christian Univ, Dept Commercial Design, 200 Chung Pei Rd, Taoyuan 32023, Taiwan.
   Huang, Yi-Ting, Chung Yuan Christian Univ, Dept Commercial Design, 200 Chung Pei Rd, Taoyuan 32023, Taiwan.}},
DOI = {{10.1016/j.chb.2017.12.038}},
ISSN = {{0747-5632}},
EISSN = {{1873-7692}},
Keywords = {{Personalized banners; Ad slot position; Ad content; Visual attention;
   Eye tracking; Online apparel retailer}},
Keywords-Plus = {{EYE-TRACKING; PRODUCT PRESENTATION; GENDER-DIFFERENCES; MENTAL
   SIMULATION; DESIGN; ANIMATION; RESPONSES; SEARCH; BRAND; INFORMATION}},
Research-Areas = {{Psychology}},
Web-of-Science-Categories  = {{Psychology, Multidisciplinary; Psychology, Experimental}},
ResearcherID-Numbers = {{Huang, Yi-Ting/P-1675-2018}},
ORCID-Numbers = {{Huang, Yi-Ting/0000-0003-2517-3066}},
Funding-Acknowledgement = {{Ministry of Science and Technology in TaiwanMinistry of Science and
   Technology, Taiwan {[}MOST 106-2410-H-033-034]}},
Funding-Text = {{This research project was supported by the Ministry of Science and
   Technology in Taiwan; Grant No. MOST 106-2410-H-033-034. The author
   thanks Ting-Wei Tung for help in scoring the eye-tracking data.}},
Number-of-Cited-References = {{117}},
Times-Cited = {{4}},
Usage-Count-Last-180-days = {{4}},
Usage-Count-Since-2013 = {{82}},
Journal-ISO = {{Comput. Hum. Behav.}},
Doc-Delivery-Number = {{FW8DT}},
Unique-ID = {{ISI:000425558800001}},
DA = {{2019-10-28}},
}

@article{ ISI:000434678100005,
Author = {Obaidellah, Unaizah and Al Haek, Mohammed and Cheng, Peter C. -H.},
Title = {{A Survey on the Usage of Eye-Tracking in Computer Programming}},
Journal = {{ACM COMPUTING SURVEYS}},
Year = {{2018}},
Volume = {{51}},
Number = {{1}},
Month = {{APR}},
Abstract = {{Traditional quantitative research methods of data collection in
   programming, such as questionnaires and interviews, are the most common
   approaches for researchers in this field. However, in recent years,
   eye-tracking has been on the rise as a new method of collecting evidence
   of visual attention and the cognitive process of programmers.
   Eye-tracking has been used by researchers in the field of programming to
   analyze and understand a variety of tasks such as comprehension and
   debugging. In this article, we will focus on reporting how experiments
   that used eye-trackers in programming research are conducted, and the
   information that can be collected from these experiments. In this
   mapping study, we identify and report on 63 studies, published between
   1990 and June 2017, collected and gathered via manual search on digital
   libraries and databases related to computer science and computer
   engineering. Among the five main areas of research interest are program
   comprehension and debugging, which received an increased interest in
   recent years, non-code comprehension, collaborative programming, and
   requirements traceability research, which had the fewest number of
   publications due to possible limitations of the eye-tracking technology
   in this type of experiments. We find that most of the participants in
   these studies were students and faculty members from institutions of
   higher learning, and while they performed programming tasks on a range
   of programming languages and programming representations, we find Java
   language and Unified Modeling Language (UML) representation to be the
   most used materials. We also report on a range of eye-trackers and
   attention tracking tools that have been utilized, and find Tobii
   eye-trackers to be the most used devices by researchers.}},
Publisher = {{ASSOC COMPUTING MACHINERY}},
Address = {{2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Obaidellah, U (Reprint Author), Univ Malaya, Fac Comp Sci \& Informat Technol, Dept Artificial Intelligence, Kuala Lumpur 50603, Malaysia.
   Obaidellah, Unaizah; Al Haek, Mohammed, Univ Malaya, Fac Comp Sci \& Informat Technol, Dept Artificial Intelligence, Kuala Lumpur 50603, Malaysia.
   Cheng, Peter C. -H., Univ Sussex, Sch Engn \& Informat, Dept Informat, Brighton BN1 9QH, E Sussex, England.}},
DOI = {{10.1145/3145904}},
Article-Number = {{5}},
ISSN = {{0360-0300}},
EISSN = {{1557-7341}},
Keywords = {{Eye tracker; programming; empirical studies; HCI; debugging;
   comprehension; participants; test materials; eye-tracking metrics}},
Keywords-Plus = {{MOVEMENTS}},
Research-Areas = {{Computer Science}},
Web-of-Science-Categories  = {{Computer Science, Theory \& Methods}},
Author-Email = {{unaizah@um.edu.my
   alhaekmohammed@gmail.com
   p.c.h.cheng@sussex.ac.uk}},
ResearcherID-Numbers = {{Haek, Mohammed Al/K-3729-2019
   Obaidellah, Unaizah/M-2068-2016}},
ORCID-Numbers = {{Haek, Mohammed Al/0000-0002-0420-3572
   Obaidellah, Unaizah/0000-0003-4822-2174}},
Funding-Acknowledgement = {{University of MalayaUniversiti Malaya; Ministry of Higher Education,
   MalaysiaMinistry of Education, Malaysia {[}FP062-2014A, RP030C-14AET]}},
Funding-Text = {{This work is supported by the University of Malaya and Ministry of
   Higher Education, Malaysia, under grants FP062-2014A and RP030C-14AET.}},
Number-of-Cited-References = {{83}},
Times-Cited = {{4}},
Usage-Count-Last-180-days = {{5}},
Usage-Count-Since-2013 = {{18}},
Journal-ISO = {{ACM Comput. Surv.}},
Doc-Delivery-Number = {{GI7JF}},
Unique-ID = {{ISI:000434678100005}},
OA = {{Green Accepted}},
DA = {{2019-10-28}},
}

@article{ ISI:000428878600002,
Author = {Stuart, Samuel and Lord, Sue and Galna, Brook and Rochester, Lynn},
Title = {{Saccade frequency response to visual cues during gait in Parkinson's
   disease: the selective role of attention}},
Journal = {{EUROPEAN JOURNAL OF NEUROSCIENCE}},
Year = {{2018}},
Volume = {{47}},
Number = {{7}},
Pages = {{769-778}},
Month = {{APR}},
Abstract = {{Gait impairment is a core feature of Parkinson's disease (PD) with
   implications for falls risk. Visual cues improve gait in PD, but the
   underlying mechanisms are unclear. Evidence suggests that attention and
   vision play an important role; however, the relative contribution from
   each is unclear. Measurement of visual exploration (specifically saccade
   frequency) during gait allows for real-time measurement of attention and
   vision. Understanding how visual cues influence visual exploration may
   allow inferences of the underlying mechanisms to response which could
   help to develop effective therapeutics. This study aimed to examine
   saccade frequency during gait in response to a visual cue in PD and
   older adults and investigate the roles of attention and vision in visual
   cue response in PD. A mobile eye-tracker measured saccade frequency
   during gait in 55 people with PD and 32 age-matched controls.
   Participants walked in a straight line with and without a visual cue
   (50cm transverse lines) presented under single task and dual-task
   (concurrent digit span recall). Saccade frequency was reduced when
   walking in PD compared to controls; however, visual cues ameliorated
   saccadic deficit. Visual cues significantly increased saccade frequency
   in both PD and controls under both single task and dual-task. Attention
   rather than visual function was central to saccade frequency and gait
   response to visual cues in PD. In conclusion, this study highlights the
   impact of visual cues on visual exploration when walking and the
   important role of attention in PD. Understanding these complex features
   will help inform intervention development.}},
Publisher = {{WILEY}},
Address = {{111 RIVER ST, HOBOKEN 07030-5774, NJ USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Rochester, L (Reprint Author), Newcastle Univ, Inst Neurosci, Inst Ageing, Clin Ageing Res Unit, Newcastle Upon Tyne NE4 5PL, Tyne \& Wear, England.
   Stuart, Samuel; Lord, Sue; Galna, Brook; Rochester, Lynn, Newcastle Univ, Inst Neurosci, Inst Ageing, Clin Ageing Res Unit, Newcastle Upon Tyne NE4 5PL, Tyne \& Wear, England.
   Stuart, Samuel, Oregon Hlth \& Sci Univ, Dept Neurol, Portland, OR 97201 USA.
   Lord, Sue, Auckland Univ Technol, Sch Clin Sci, Auckland, New Zealand.
   Galna, Brook, Newcastle Univ, Sch Biomed Sci, Newcastle Upon Tyne, Tyne \& Wear, England.
   Rochester, Lynn, Newcastle Upon Tyne Hosp NHS Fdn Trust, Newcastle Upon Tyne, Tyne \& Wear, England.}},
DOI = {{10.1111/ejn.13864}},
ISSN = {{0953-816X}},
EISSN = {{1460-9568}},
Keywords = {{attention; gait; Parkinson's disease; saccades; vision; visual cues}},
Keywords-Plus = {{STRIDE LENGTH REGULATION; PHYSICAL-THERAPY; EYE-MOVEMENTS; AUDITORY
   CUES; OLDER-ADULTS; DUAL-TASK; PEOPLE; DEMENTIA; LEVODOPA; STRATEGY}},
Research-Areas = {{Neurosciences \& Neurology}},
Web-of-Science-Categories  = {{Neurosciences}},
Author-Email = {{lynn.rochester@newcastle.ac.uk}},
ResearcherID-Numbers = {{Stuart, Samuel/I-6449-2019
   }},
ORCID-Numbers = {{Stuart, Samuel/0000-0001-6846-9372
   Rochester, Lynn/0000-0001-5774-9272}},
Funding-Acknowledgement = {{National Institute for Health Research (NIHR)National Institute for
   Health Research (NIHR); Newcastle Biomedical Research Unit (BRU); Centre
   (BRC) based at Newcastle upon Tyne Hospitals NHS Foundation Trust;
   Newcastle University; NIHR Newcastle CRF Infrastructure funding}},
Funding-Text = {{This research is supported by the National Institute for Health Research
   (NIHR), Newcastle Biomedical Research Unit (BRU) and Centre (BRC) based
   at Newcastle upon Tyne Hospitals NHS Foundation Trust and Newcastle
   University. The research was also supported by NIHR Newcastle CRF
   Infrastructure funding. The views expressed are those of the authors and
   not necessarily those of the NHS, the NIHR or the Department of Health.
   The authors would like to acknowledge Adhan Hickey (Research Technician,
   Newcastle University), Henry King (Master's student) and Dr Alan Godfrey
   (Research Associate, Newcastle University) for their assistance with
   data collection and processing, and Dr Shirley Coleman (Senior
   Statistician, Newcastle University) for her advice regarding data
   analysis. Finally, we thank all of the participants involved in the
   study for their time and input.}},
Number-of-Cited-References = {{62}},
Times-Cited = {{2}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{9}},
Journal-ISO = {{Eur. J. Neurosci.}},
Doc-Delivery-Number = {{GB2JP}},
Unique-ID = {{ISI:000428878600002}},
DA = {{2019-10-28}},
}

@article{ ISI:000457272700088,
Author = {Lee, Seolhwa and Hooshyar, Danial and Ji, Hyesung and Nam, Kichun and
   Lim, Heuiseok},
Title = {{Mining biometric data to predict programmer expertise and task
   difficulty}},
Journal = {{CLUSTER COMPUTING-THE JOURNAL OF NETWORKS SOFTWARE TOOLS AND
   APPLICATIONS}},
Year = {{2018}},
Volume = {{21}},
Number = {{1, SI}},
Pages = {{1097-1107}},
Month = {{MAR}},
Abstract = {{Programming mistakes frequently waste software developers' time and may
   lead to the introduction of bugs into their software, causing serious
   risks for their customers. Using the correlation between various
   software process metrics and defects, earlier work has traditionally
   attempted to spot such bug risks. However, this study departs from
   previous works in examining a more direct method of using
   psycho-physiological sensors data to detect the difficulty of program
   comprehension tasks and programmer level of expertise. By conducting a
   study with 38 expert and novice programmers, we investigated how well an
   electroencephalography and an eye-tracker can be utilized in predicting
   programmer expertise (novice/expert) and task difficulty
   (easy/difficult). Using data from both sensors, we could predict task
   difficulty and programmer level of expertise with 64.9 and 97.7\%
   precision and 68.6 and 96.4\% recall, respectively. The result shows it
   is possible to predict the perceived difficulty of a task and expertise
   level for developers using psycho-physiological sensors data. In
   addition, we found that while using single biometric sensor shows good
   results, the composition of both sensors lead to the best overall
   performance.}},
Publisher = {{SPRINGER}},
Address = {{233 SPRING ST, NEW YORK, NY 10013 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Lim, H (Reprint Author), Korea Univ, Dept Comp Sci \& Engn, Seoul, South Korea.
   Lee, Seolhwa; Hooshyar, Danial; Ji, Hyesung; Lim, Heuiseok, Korea Univ, Dept Comp Sci \& Engn, Seoul, South Korea.
   Nam, Kichun, Korea Univ, Dept Psychol, Seoul, South Korea.}},
DOI = {{10.1007/s10586-017-0746-2}},
ISSN = {{1386-7857}},
EISSN = {{1573-7543}},
Keywords = {{Code comprehension; Programming expertise; Task difficulty; Biometric
   data; Machine learning}},
Keywords-Plus = {{MENTAL WORKLOAD; EYE-MOVEMENTS; EEG}},
Research-Areas = {{Computer Science}},
Web-of-Science-Categories  = {{Computer Science, Information Systems; Computer Science, Theory \&
   Methods}},
Author-Email = {{whiteldark@korea.ac.kr
   danial\_hooshyar@korea.ac.kr
   hyesung84@korea.ac.kr
   kichun@korea.ac.kr
   limhseok@korea.ac.kr}},
ResearcherID-Numbers = {{Hooshyar, Danial/X-5126-2018}},
ORCID-Numbers = {{Hooshyar, Danial/0000-0002-9143-6648}},
Funding-Acknowledgement = {{ICT R\&D Program of MSIP/IITP {[}2016(B0101-16-0340)]; National Research
   Foundation of Korea(NRF) - Korea Government(MSIP) {[}R1610941]}},
Funding-Text = {{This work was supported by the ICT R\&D Program of MSIP/IITP {[}Grant
   Number 2016(B0101-16-0340)]. Development of distribution and diffusion
   service technology through individual and collective intelligence to
   digital contents. ``This work was supported by the National Research
   Foundation of Korea(NRF) Grant funded by the Korea Government(MSIP) (No.
   R1610941).{''}}},
Number-of-Cited-References = {{31}},
Times-Cited = {{2}},
Usage-Count-Last-180-days = {{2}},
Usage-Count-Since-2013 = {{3}},
Journal-ISO = {{Cluster Comput.}},
Doc-Delivery-Number = {{HJ6DA}},
Unique-ID = {{ISI:000457272700088}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000471035200038,
Author = {Wong, S. F. and Cong, Y. F.},
Editor = {{Sormaz, D and Suer, G and Chen, FF}},
Title = {{Comparison of Visual Performance with Operational Fatigue Level based on
   Eye Tracking Model}},
Booktitle = {{28TH INTERNATIONAL CONFERENCE ON FLEXIBLE AUTOMATION AND INTELLIGENT
   MANUFACTURING (FAIM2018): GLOBAL INTEGRATION OF INTELLIGENT
   MANUFACTURING AND SMART INDUSTRY FOR GOOD OF HUMANITY}},
Series = {{Procedia Manufacturing}},
Year = {{2018}},
Volume = {{17}},
Pages = {{302-308}},
Note = {{28th International Conference on Flexible Automation and Intelligent
   Manufacturing (FAIM) - Global Integration of Intelligent Manufacturing
   and Smart Industry for Good of Humanity, Columbus, OH, JUN 11-14, 2018}},
Organization = {{Ohio Univ, Russ Coll Engn \& Technol; Stanley Elect; Amer Makes; Simio}},
Abstract = {{Fatigue is factor which can cause health problems and decrease work
   efficiency. State of the art fatigue studies are mainly committed to
   investigating impacts of the Visual Display Terminal (VDT) screen
   luminance, resolution, and the distance between the VDT and operators.
   Fewer studies commit to understanding visual performance experienced by
   the operator when facing different conditions. This paper aims to find
   out whether the degree of fatigue and color combination of the interface
   that can influence visual performance when interacting with a text-based
   and eye tracking under exercise and no-exercise consideration. The
   viewing performance of 30 subjects was estimated by comparing eye
   tracker measurements before and after moderate exercise.
   Visual reaction time is significantly affected by color backgrounds
   Color background transfer results in a long reaction time, while the
   white background transfer leads to a short time reaction time. The
   testers who exercise before the experiment have more positive
   performance than those who do not in an attention test. For the subjects
   who exercised, the longer they were able to rest, the better their
   performance. Therefore, engaging in appropriate exercise may have a good
   effect on the performance of operators. Additionally, the change in
   interface motivates eye movements and improves eye performance. (C) 2018
   The Authors. Published by Elsevier B.V.}},
Publisher = {{ELSEVIER SCIENCE BV}},
Address = {{SARA BURGERHARTSTRAAT 25, PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Wong, SF (Reprint Author), Univ Macau, Dept Electromech Engn, Zhuhai, Peoples R China.
   Wong, S. F.; Cong, Y. F., Univ Macau, Dept Electromech Engn, Zhuhai, Peoples R China.}},
DOI = {{10.1016/j.promfg.2018.10.050}},
ISSN = {{2351-9789}},
Keywords = {{Visual Performance; Fatigue Level; Eye Tracking; Exercise Consideration}},
Keywords-Plus = {{COLOR COMBINATIONS; ACUTE EXERCISE; POLARITY; WORK; TERM}},
Research-Areas = {{Computer Science; Engineering}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Engineering, Manufacturing}},
Author-Email = {{fstsfw@umac.mo}},
Funding-Acknowledgement = {{University of Macau; Macau Science and Technology Development Fund
   {[}166/2017/A]}},
Funding-Text = {{The authors acknowledge the funding support by the University of Macau
   under conference grant and the Macau Science and Technology Development
   Fund under research grant number 166/2017/A.}},
Number-of-Cited-References = {{19}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Doc-Delivery-Number = {{BM9JO}},
Unique-ID = {{ISI:000471035200038}},
OA = {{Bronze}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000470913000003,
Author = {Obaidellah, Unaizah and Al Haek, Mohammed},
Editor = {{Spencer, SN}},
Title = {{Evaluating gender difference on algorithmic problems using eye-tracker}},
Booktitle = {{2018 ACM SYMPOSIUM ON EYE TRACKING RESEARCH \& APPLICATIONS (ETRA 2018)}},
Year = {{2018}},
Note = {{ACM Symposium on Eye Tracking Research and Applications (ETRA), SWPS
   Univ Social Sci \& Humanities, Warsaw, OMAN, JUN 14-17, 2018}},
Organization = {{ACM SIGGRAPH; ACM SIGCHI}},
Abstract = {{Gender differences in programming comprehension has been a topic of
   discussion in recent years. We conducted an eye -tracking study on 51(21
   female, 30 male) computer science undergraduate university students to
   examine their cognitive processes in pseudocode comprehension. We aim to
   identify their reading strategies and eye gaze behavior on the
   comprehension of pseudocodes in terms of performance and visual effort
   when solving algorithmic problems of varying difficulty levels. Each
   student completed a series of tasks requiring them to rearrange
   randomized pseudocode statements in a correct order for the problem
   presented. Our results indicated that the speed of analyzing the
   problems were faster among male students, although female students
   fixated longer in understanding the problem requirements. In addition,
   female students more commonly fixated on indicative verbs (i.e., prompt,
   print), while male students fixated more on operational statements
   (i.e., loops, variables calculations, file handling).}},
Publisher = {{ASSOC COMPUTING MACHINERY}},
Address = {{1515 BROADWAY, NEW YORK, NY 10036-9998 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Obaidellah, U (Reprint Author), Univ Malaya, Dept Artificial Intelligence, Fac Comp Sci \& Informat Technol, Kuala Lumpur, Malaysia.
   Obaidellah, Unaizah; Al Haek, Mohammed, Univ Malaya, Dept Artificial Intelligence, Fac Comp Sci \& Informat Technol, Kuala Lumpur, Malaysia.}},
DOI = {{10.1145/3204493.3204537}},
ISBN = {{978-1-4503-5706-7}},
Keywords = {{eye-tracking; gender differences; novice programmers; pseudocode;
   algorithms problems}},
Research-Areas = {{Computer Science}},
Web-of-Science-Categories  = {{Computer Science, Cybernetics; Computer Science, Interdisciplinary
   Applications}},
Author-Email = {{unaizah@um.edu.my
   alhaekmohammed@gmail.com}},
Funding-Acknowledgement = {{ {[}RP030C-14AET]}},
Funding-Text = {{The authors would like to thank the reviewers and associate editor for
   their comments and helpful suggestions. This work is supported by grant
   No.: RP030C-14AET.}},
Number-of-Cited-References = {{11}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Doc-Delivery-Number = {{BM9GS}},
Unique-ID = {{ISI:000470913000003}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000468880500028,
Author = {Baskoro, Kezia and Widyanti, Ari},
Editor = {{Suhardi and Langi, AZR and Arman, AA}},
Book-Group-Author = {{IEEE}},
Title = {{Usability Evaluation on an Indonesian Mobile Application for Small
   Business Lending}},
Booktitle = {{2018 INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY SYSTEMS AND
   INNOVATION (ICITSI)}},
Series = {{International Conference on Information Technology Systems and
   Innovation (ICITSI)}},
Year = {{2018}},
Pages = {{148-153}},
Note = {{5th International Conference on Information Technology Systems and
   Innovation (ICITSI), Inst Teknologi Bandung, Sch Elect Engn \& Informat,
   INDONESIA, OCT 22-26, 2018}},
Organization = {{IEEE Indonesia Sect; Inst Teknologi Bandung, Informat Technol Res Grp;
   Inst Teknologi Bandung, Serv Comp Res Grp; Univ Andalas, Fac Informat
   Technol}},
Abstract = {{This study aims to evaluate the usability of Indonesian mobile
   application for small business lending regarding its effectiveness,
   efficiency, and satisfaction, as well as through the understanding of
   users' comments gathered through retrospective think aloud. The study
   was conducted in two different experiments, one using smartphones and
   the other using an eye tracker device, to operate the application. Data
   was obtained from a total of 60 participants who were asked to do a
   series of tasks on the application and executed retrospective think
   aloud. Participants for the smartphone experiment also filled out a
   questionnaire on user interface satisfaction (QUIS).
   Based on the acquired findings, the application was found to have good
   effectiveness and satisfaction, but still has problems regarding
   efficiency and users' grievances found through retrospective think
   aloud.}},
Publisher = {{IEEE}},
Address = {{345 E 47TH ST, NEW YORK, NY 10017 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Baskoro, K (Reprint Author), Bandung Inst Technol, Dept Ind Technol, Bandung, Indonesia.
   Baskoro, Kezia; Widyanti, Ari, Bandung Inst Technol, Dept Ind Technol, Bandung, Indonesia.}},
ISSN = {{2443-1451}},
ISBN = {{978-1-5386-5693-8}},
Keywords = {{usability; usability evaluation; eye tracker; retrospective think aloud;
   mobile application; small business lending}},
Research-Areas = {{Computer Science}},
Web-of-Science-Categories  = {{Computer Science, Information Systems}},
Author-Email = {{kezsekarsari@gmail.com
   widyanti@mail.ti.itb.ac.id}},
Funding-Acknowledgement = {{Ergonomics, Work Engineering and Work Safety Laboratory ITB}},
Funding-Text = {{This work was supported by Ergonomics, Work Engineering and Work Safety
   Laboratory ITB.}},
Number-of-Cited-References = {{27}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Doc-Delivery-Number = {{BM8DG}},
Unique-ID = {{ISI:000468880500028}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000467070300019,
Author = {Augereau, Olivier and Jacquet, Clement and Kise, Koichi and Journet,
   Nicholas},
Book-Group-Author = {{IEEE}},
Title = {{Vocabulometer: a Web Platform for Document and Reader Mutual Analysis}},
Booktitle = {{2018 13TH IAPR INTERNATIONAL WORKSHOP ON DOCUMENT ANALYSIS SYSTEMS (DAS)}},
Year = {{2018}},
Pages = {{109-114}},
Note = {{13th IAPR International Workshop on Document Analysis Systems (DAS), TU
   Wien, Vienna, AUSTRIA, APR 24-27, 2018}},
Organization = {{IAPR; APA IT; Naver Labs Europe; Comp Vis Lab}},
Abstract = {{We present the Vocabulometer, a reading assistant system designed to
   record the reading activity of a user with an eye tracker and to extract
   mutual information about the users and the read documents. The
   Vocabulometer stands as a web platform and can be used for analyzing the
   comprehension of the user, the comprehensibility of the document,
   predicting the difficult words, recommending document according to the
   reader's in order to increase his skills, etc. Since the last years,
   with the development of lowcost eye trackers, the technology is now
   accessible for many people, which will allow using data mining and
   machine learning algorithms for the mutual analysis of documents and
   readers.}},
Publisher = {{IEEE}},
Address = {{345 E 47TH ST, NEW YORK, NY 10017 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Augereau, O (Reprint Author), Osaka Prefecture Univ, IDAKS, Sakai, Osaka, Japan.
   Augereau, Olivier; Jacquet, Clement; Kise, Koichi, Osaka Prefecture Univ, IDAKS, Sakai, Osaka, Japan.
   Journet, Nicholas, Bordeaux Univ, LaBRI, Talence, France.}},
DOI = {{10.1109/DAS.2018.59}},
ISBN = {{978-1-5386-3346-5}},
Keywords = {{eye tracking; document analysis; reader analysis; mutual analysis; web
   platform}},
Research-Areas = {{Computer Science}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering}},
Author-Email = {{augereau.o@gmail.com
   journet@labri.fr}},
Funding-Acknowledgement = {{JST CRESTJapan Science \& Technology Agency (JST)Core Research for
   Evolutional Science and Technology (CREST) {[}JP-MJCR16E1]; JSPSMinistry
   of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan
   Society for the Promotion of Science {[}15K12172]; JSPS KAKENHIMinistry
   of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan
   Society for the Promotion of ScienceGrants-in-Aid for Scientific
   Research (KAKENHI) {[}16K16089]; Key Project Grant Program of Osaka
   Prefecture University}},
Funding-Text = {{This research was in part supported by JST CREST (JP-MJCR16E1), JSPS
   Grant-in-Aid for Scientific Research (15K12172), JSPS KAKENHI 16K16089,
   and the Key Project Grant Program of Osaka Prefecture University. We
   would like to thank Antoine Pirrone, Pierre Celor, Souha Guissouma,
   Bissen Hamdi and Boris Mansencal for the help provided during the global
   architecture construction.}},
Number-of-Cited-References = {{24}},
Times-Cited = {{1}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Doc-Delivery-Number = {{BM6MT}},
Unique-ID = {{ISI:000467070300019}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000462163500140,
Author = {Wang, Xuan and Meng, Fang and Huang, Xiyu},
Editor = {{Li, W and Li, Q and Wang, L}},
Title = {{Visual Behaviors Analysis Based on Eye Tracker in Subjective Image
   Quality Assessment}},
Booktitle = {{2018 11TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING,
   BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI 2018)}},
Year = {{2018}},
Note = {{11th International Congress on Image and Signal Processing, BioMedical
   Engineering and Informatics (CISP-BMEI), Beijing, PEOPLES R CHINA, OCT
   13-15, 2018}},
Organization = {{IEEE; IEEE Engn Med \& Biol Soc; Beijing Univ Chem Technol; Beijing Inst
   Technol; E China Normal Univ}},
Abstract = {{The study of the visual behaviors of observers in subjective image
   quality assessment is helpful in understanding of human visual system.
   It can also be used to improve the reliability of assessment results. In
   this paper, we propose a subjective image quality assessment system
   based on eye tracker. The system can record the viewpoints of observers'
   eye movements in the process of experiment. The system also can analyze
   the visual behaviors of observers. In the experiment, we first verify
   the accuracy of the recorded viewpoints. After, we choose the observers
   with different professional backgrounds. From the analysis of the
   assessment results and the corresponding viewpoints, we can see that
   non-experts are more easily to be attracted by the image contents, and
   the viewpoints of experts are more overall.}},
Publisher = {{IEEE}},
Address = {{345 E 47TH ST, NEW YORK, NY 10017 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Wang, X (Reprint Author), Commun Univ China, Coll Informat Engn, Beijing 100024, Peoples R China.
   Wang, Xuan; Meng, Fang; Huang, Xiyu, Commun Univ China, Coll Informat Engn, Beijing 100024, Peoples R China.}},
ISBN = {{978-1-5386-7604-2}},
Keywords = {{Eye tracker; visual behaviors; subjective image quality assessment}},
Research-Areas = {{Engineering; Imaging Science \& Photographic Technology}},
Web-of-Science-Categories  = {{Engineering, Biomedical; Engineering, Electrical \& Electronic; Imaging
   Science \& Photographic Technology}},
Number-of-Cited-References = {{8}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Doc-Delivery-Number = {{BM3IZ}},
Unique-ID = {{ISI:000462163500140}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000460476300004,
Author = {Aronson, Reuben M. and Santini, Thiago and Kuebler, Thomas C. and
   Kasneci, Enkelejda and Srinivasa, Siddhartha and Admoni, Henny},
Book-Group-Author = {{Assoc Comp Machinery}},
Title = {{Eye-Hand Behavior in Human-Robot Shared Manipulation}},
Booktitle = {{HRI `18: PROCEEDINGS OF THE 2018 ACM/IEEE INTERNATIONAL CONFERENCE ON
   HUMAN-ROBOT INTERACTION}},
Series = {{ACM IEEE International Conference on Human-Robot Interaction}},
Year = {{2018}},
Pages = {{4-13}},
Note = {{13th Annual ACM/IEEE International Conference on Human-Robot Interaction
   (HRI), Chicago, IL, MAR 05-08, 2018}},
Organization = {{Assoc Comp Machinery; IEEE; ACM SIGCHI; ACM SIGAI; IEEE Robot \& Automat
   Soc; Kinova; Disney Res; LuxAI; Toyota Res Inst; Furhat Robot; Honda Res
   Inst; Google; Beam; Robotis; Savioke; Yujin Robot; Misty Robot; Hebi
   Robot; Haption; Otto Motors; AAAI}},
Abstract = {{Shared autonomy systems enhance people's abilities to perform activities
   of daily living using robotic manipulators. Recent systems succeed by
   first identifying their operators' intentions, typically by analyzing
   the user's joystick input. To enhance this recognition, it is useful to
   characterize people's behavior while performing such a task.
   Furthermore, eye gaze is a rich source of information for understanding
   operator intention. The goal of this paper is to provide novel insights
   into the dynamics of control behavior and eye gaze in human-robot shared
   manipulation tasks. To achieve this goal, we conduct a data collection
   study that uses an eye tracker to record eye gaze during a human-robot
   shared manipulation activity, both with and without shared autonomy
   assistance. We process the gaze signals from the study to extract gaze
   features like saccades, fixations, smooth pursuits, and scan paths. We
   analyze those features to identify novel patterns of gaze behaviors and
   highlight where these patterns are similar to and different from
   previous findings about eye gaze in human-only manipulation tasks. The
   work described in this paper lays a foundation for a model of natural
   human eye gaze in human-robot shared manipulation.}},
Publisher = {{ASSOC COMPUTING MACHINERY}},
Address = {{1515 BROADWAY, NEW YORK, NY 10036-9998 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Aronson, RM (Reprint Author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   Aronson, Reuben M.; Admoni, Henny, Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   Santini, Thiago; Kuebler, Thomas C.; Kasneci, Enkelejda, Univ Tubingen, Tubingen, Germany.
   Srinivasa, Siddhartha, Univ Washington, Seattle, WA 98195 USA.}},
DOI = {{10.1145/3171221.3171287}},
ISSN = {{2167-2121}},
EISSN = {{2167-2148}},
ISBN = {{978-1-4503-4953-6}},
Keywords = {{human-robot interaction; eye gaze; eye tracking; shared autonomy;
   nonverbal communication}},
Keywords-Plus = {{GAZE; MOVEMENTS; TIME; COORDINATION; ATTENTION; LOAD}},
Research-Areas = {{Computer Science; Engineering; Robotics}},
Web-of-Science-Categories  = {{Computer Science, Cybernetics; Engineering, Electrical \& Electronic;
   Robotics}},
Author-Email = {{rmaronson@cmu.edu
   thiago.santini@uni-tuebingen.de
   thomas.kuebler@uni-tuebingen.de
   enkelejda.kasneci@uni-tuebingen.de
   siddh@cs.uw.edu
   henny@cmu.edu}},
Number-of-Cited-References = {{68}},
Times-Cited = {{5}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{2}},
Doc-Delivery-Number = {{BM1SD}},
Unique-ID = {{ISI:000460476300004}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000458669700075,
Author = {Su, Qi and Chen, Fei and Li, Hanfei and Yan, Nan and Wang, Lan},
Book-Group-Author = {{IEEE}},
Title = {{Multimodal Emotion Perception in Children with Autism Spectrum Disorder
   by Eye Tracking Study}},
Booktitle = {{2018 IEEE-EMBS CONFERENCE ON BIOMEDICAL ENGINEERING AND SCIENCES
   (IECBES)}},
Series = {{IEEE EMBS Conference on Biomedical Engineering and Sciences}},
Year = {{2018}},
Pages = {{382-387}},
Note = {{IEEE-EMBS Conference on Biomedical Engineering and Sciences (IECBES),
   Kuching, MALAYSIA, DEC 03-06, 2018}},
Organization = {{SARAWAK Convent Bur; Physiolog Measurement}},
Abstract = {{Children with autism spectrum disorder (ASD) have difficulties in
   emotional processing. Children with ASD seem to have different cognitive
   patterns during multimodal emotion recognition compared with typically
   developing (TD) children. In order to explore the differences, in the
   current study, perceptual judgment data and eye gaze data while watching
   the ecological valid facial stimuli were analyzed. The perceptual
   judgment could intuitively reflect the perception deficits in ASD, and
   the eye tracker could accurately track their fixation and scan paths
   during emotion perception. Perceptual results indicated that children
   with ASD had more difficulties in emotion recognition in comparison with
   TD children, especially for negative emotions. Eye gaze analysis also
   showed that children with ASD had decreased fixation duration on the
   areas of interests (AOIs) compared to TD children, yet they seemed to
   pay more attention to negative facial expressions than to positive ones.
   Furthermore, the trajectory of eye gaze movement showed difference
   patterns between children with ASD and TD children. The TD children have
   dynamically changed their gaze directions from one region to the other,
   while most of the children with ASD only tended to focus on one
   particular region. These results may deepen our understanding of the
   multimodal emotion recognition pattern in the ASD cohort with a more
   natural communication environment.}},
Publisher = {{IEEE}},
Address = {{345 E 47TH ST, NEW YORK, NY 10017 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Su, Q (Reprint Author), Chinese Acad Sci Dept, Shenzhen Inst Adv Technol, CAS Key Lab Human Machine Intelligence Synergy Sy, Shenzhen, Peoples R China.
   Su, Qi; Li, Hanfei; Yan, Nan; Wang, Lan, Chinese Acad Sci Dept, Shenzhen Inst Adv Technol, CAS Key Lab Human Machine Intelligence Synergy Sy, Shenzhen, Peoples R China.
   Chen, Fei, Hong Kong Polytech Univ, Dept Chinese \& Bilingual Studies, Hong Kong, Peoples R China.}},
ISSN = {{2374-3220}},
ISBN = {{978-1-5386-2471-5}},
Keywords = {{multimodal emotion perception; Autism Spectrum Disorder; Eye-tracking}},
Keywords-Plus = {{HIGH-FUNCTIONING AUTISM; FACIAL EXPRESSION RECOGNITION; COMPLEX EMOTION;
   INDIVIDUALS; COMPETENCE; ATTENTION; MIND}},
Research-Areas = {{Engineering}},
Web-of-Science-Categories  = {{Engineering, Biomedical}},
Author-Email = {{qi.su@siat.ac.cn
   chenfeianthony@gmail.com
   645024789@qq.com
   nan.yan@siat.ac.cn
   lan.wang@siat.ac.cn}},
Funding-Acknowledgement = {{National Natural Science Foundation of China (NSFC)National Natural
   Science Foundation of China {[}U1736203, U1736202]; ShenZhen Fundamental
   Research Program {[}JCYJ20160429184226930, KQJSCX20170731163308665];
   Shenzhen Speech Rehabilitation Technology Laboratory; Health and Health
   Services Research Fund (HHSRF), Shenzhen Fundamental Research Program
   {[}JCYJ20170413161611534]}},
Funding-Text = {{This work is supported by National Natural Science Foundation of China
   (NSFC U1736203), ShenZhen Fundamental Research Program
   JCYJ20160429184226930, KQJSCX20170731163308665, National Natural Science
   Foundation of China (NSFC, U1736202), Shenzhen Speech Rehabilitation
   Technology Laboratory and Health and Health Services Research Fund
   (HHSRF), Shenzhen Fundamental Research Program JCYJ20170413161611534.}},
Number-of-Cited-References = {{39}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{4}},
Usage-Count-Since-2013 = {{4}},
Doc-Delivery-Number = {{BM0DK}},
Unique-ID = {{ISI:000458669700075}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000458680100045,
Author = {Brishtel, Iuliia and Ishimaru, Shoya and Augereau, Olivier and Kise,
   Koichi and Dengel, Andreas},
Book-Group-Author = {{ACM}},
Title = {{Assessing Cognitive Workload on Printed and Electronic Media using
   Eye-Tracker and EDA Wristband}},
Booktitle = {{COMPANION OF THE 23RD INTERNATIONAL CONFERENCE ON INTELLIGENT USER
   INTERFACES (IUI'18)}},
Year = {{2018}},
Note = {{23rd International Conference on Intelligent User Interfaces (IUI),
   Tokyo, JAPAN, MAR 07-11, 2018}},
Organization = {{Assoc Comp Machinery; ACM SIGAI; ACM SIGCHI}},
Abstract = {{With the expansion of e-learning platforms, we receive a great
   opportunity to learn and study just using an electronic device. In this
   paper, we measured the differences in information processing on screen
   and paper with 18 participants using an eye-tracker and an EDA
   wristband. Our findings show that the media type has a significant
   influence on cognitive workload and understandability of the content.
   The results of this work are of vital importance for the design of new
   intelligent user interfaces and reveal the necessity to take mental
   processes of users more into account.}},
Publisher = {{ASSOC COMPUTING MACHINERY}},
Address = {{1515 BROADWAY, NEW YORK, NY 10036-9998 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Brishtel, I (Reprint Author), German Res Ctr Artificial Intelligence DFKI, Kaiserslautern, Germany.
   Brishtel, I (Reprint Author), TU Kaiserslautern, Kaiserslautern, Germany.
   Brishtel, Iuliia; Ishimaru, Shoya; Dengel, Andreas, German Res Ctr Artificial Intelligence DFKI, Kaiserslautern, Germany.
   Brishtel, Iuliia; Ishimaru, Shoya; Dengel, Andreas, TU Kaiserslautern, Kaiserslautern, Germany.
   Ishimaru, Shoya; Augereau, Olivier; Kise, Koichi; Dengel, Andreas, Osaka Prefecture Univ, Sakai, Osaka, Japan.}},
DOI = {{10.1145/3180308.3180354}},
ISBN = {{978-1-4503-5571-1}},
Keywords = {{E-learning; Eye-Tracking; Reading; Information Processing; Electrodermal
   Activity; Cognitive Workload; User-Interface}},
Research-Areas = {{Computer Science}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering}},
Author-Email = {{Iuliia.Brishtel@dfki.de
   Shoya.Ishimaru@dfki.de
   augereau@m.cs.osakafu-u.ac.jp
   kise@cs.osakafu-u.ac.jp
   Andreas.Dengel@dfki.de}},
Number-of-Cited-References = {{5}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Doc-Delivery-Number = {{BM0DZ}},
Unique-ID = {{ISI:000458680100045}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000453563500007,
Author = {Wan, Qianwen and Rajeev, Srijith and Kaszowska, Aleksandra and Panetta,
   Karen and Taylor, Holly A. and Agaian, Sos},
Editor = {{Agaian, SS and Jassim, SA and DelMarco, SP and Asari, VK}},
Title = {{Fixation oriented object segmentation using mobile eye tracker}},
Booktitle = {{MOBILE MULTIMEDIA/IMAGE PROCESSING, SECURITY, AND APPLICATIONS 2018}},
Series = {{Proceedings of SPIE}},
Year = {{2018}},
Volume = {{10668}},
Note = {{Conference on Mobile Multimedia/Image Processing, Security, and
   Applications, Orlando, FL, APR 16-17, 2018}},
Organization = {{SPIE}},
Abstract = {{Eye tracking technology allows researchers to monitor position of the
   eye and infer one's gaze direction, which is used to understand the
   nature of human attention within psychology, cognitive science,
   marketing and artificial intelligence. Commercially available
   head-mounted eye trackers allow researchers to track pupil movements
   (saccades and fixations) using infrared camera and capture the field of
   vision by a front-facing scene camera. The wearable eye tracker opened a
   new way to research in unconstrained environment settings; however, the
   recorded scene video typically has non-uniform illumination, low quality
   image frames, and moving scene objects. One of the most important tasks
   for analyzing the recorded scene video data is finding the boundary
   between different objects in a single frame.
   This paper presents a multi-level fixation-oriented object segmentation
   method (MFoOS) to solve the above challenges in segmenting the scene
   objects in video data collected by the eye tracker in order to support
   cognition research. MFoOS shows its advancement in position-invariance,
   illumination, noise tolerance and is task-driven. The proposed method is
   tested using real-world case studies designed by our team of
   psychologists focused on understanding visual attention in human problem
   solving. The extensive computer simulation demonstrates the method's
   accuracy and robustness for fixation-oriented object segmentation.
   Moreover, a deep-learning image semantic segmentation combining MFoOS
   results as label data was explored to demonstrate the possibility of
   on-line deployment of eye tracker fixation-oriented object segmentation.}},
Publisher = {{SPIE-INT SOC OPTICAL ENGINEERING}},
Address = {{1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Wan, QW (Reprint Author), Tufts Univ, Dept Elect \& Comp Engn, Medford, MA 02155 USA.
   Wan, Qianwen; Rajeev, Srijith; Panetta, Karen, Tufts Univ, Dept Elect \& Comp Engn, Medford, MA 02155 USA.
   Kaszowska, Aleksandra; Taylor, Holly A., Tufts Univ, Dept Psychol, Medford, MA 02155 USA.
   Agaian, Sos, CUNY, Comp Sci, New York, NY 10017 USA.}},
DOI = {{10.1117/12.2304868}},
ISSN = {{0277-786X}},
EISSN = {{1996-756X}},
ISBN = {{978-1-5106-1848-0}},
Keywords = {{Eye tracking technology; multi-level fixation-oriented object
   segmentation; image semantic segmentation; online deployment; cognitive
   science}},
Keywords-Plus = {{VISUAL ANALYTICS; IMAGE}},
Research-Areas = {{Optics; Imaging Science \& Photographic Technology}},
Web-of-Science-Categories  = {{Optics; Imaging Science \& Photographic Technology}},
Funding-Acknowledgement = {{Tufts University School of Engineering, Center for Applied Brain \&
   Cognitive Sciences (CABCS) {[}A452001 ARM981]}},
Funding-Text = {{This work is supported by the Tufts University School of Engineering,
   Center for Applied Brain \& Cognitive Sciences (CABCS) under award
   numbers A452001 ARM981. Any opinions, findings, and conclusions or
   recommendations expressed in this publication are those of the author(s)
   and do not necessarily reflect the views of the CABCS.}},
Number-of-Cited-References = {{33}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Doc-Delivery-Number = {{BL6DA}},
Unique-ID = {{ISI:000453563500007}},
DA = {{2019-10-28}},
}

@article{ ISI:000417657300002,
Author = {Tang, Chaochen and Yang, Xiaolin and Chen, Xu and Ameen, Asif and Xie,
   Guanghui},
Title = {{Sorghum biomass and quality and soil nitrogen balance response to
   nitrogen rate on semiarid marginal land}},
Journal = {{FIELD CROPS RESEARCH}},
Year = {{2018}},
Volume = {{215}},
Pages = {{12-22}},
Month = {{JAN}},
Abstract = {{An understanding of biomass yield and quality response to N supply is
   essential to breeding and cultivation of sorghum (Sorghum bicolor (L.)
   Moench) for the production of bioethanol or forage. A two - year field
   experiment was conducted in 2013 and 2014 on semiarid marginal land in
   Inner Mongolia (39 degrees 10'N, 109 degrees 53'E) to determine the
   effects of N fertilizer rate (0, 60, 120, and 240 kg ha(-1)) on biomass
   yield and quality of sweet sorghum (GT-8) and biomass sorghum (GN-11),
   and on soil conditions. Results indicated that (1) biomass yield,
   leaf/stem ratio, and crude protein (CP), protein yield, theoretical
   ethanol yield (TEY), and soil net NO3--N accumulation of both sorghum
   varieties in 2013 and 2014 significantly increased with the increasing N
   fertilizer rate from 0 to 240 kg ha(-1), whereas dry matter ratio and
   relative feed value (RFV) varied inversely with the N application rates.
   (2) Compared to biomass sorghum (GN-11), sweet sorghum (GT-8)
   demonstrated significant enhancement of forage quality components
   including CP, dry matter digestibility (DMD), dry matter intake (DMI),
   total digestible nutrients (TDN), net energy for lactation (NEL), and
   RFV, with stems exhibiting higher forage quality than leaves. Biomass
   sorghum (GN-11) exhibited better performance for bioenergy production,
   with an average biomass yield of 7.9 t ha(-1) and TEY of 3046.7 L ha(-1)
   averaged across all N rates. (3) Annual average amount of net NO3--N
   accumulation in 0-90 cm layers of two years significantly increased when
   fertilizer N exceeded 120 kg N ha(-1). In short, N fertilizer of 60 kg
   ha(-1) is recommended to sweet sorghum for the forage production and 120
   kg N ha(-1) to biomass sorghum for bioenergy feedstock sustainability,
   respectively, meanwhile to prevent the soil degradation on semiarid
   marginal lands.}},
Publisher = {{ELSEVIER SCIENCE BV}},
Address = {{PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Yang, XL (Reprint Author), China Agr Univ, Coll Agron \& Biotechnol, 2 Yuanmingyuan West Rd, Beijing 100193, Peoples R China.
   Tang, Chaochen; Yang, Xiaolin; Chen, Xu; Ameen, Asif; Xie, Guanghui, China Agr Univ, Coll Agron \& Biotechnol, 2 Yuanmingyuan West Rd, Beijing 100193, Peoples R China.
   Tang, Chaochen; Yang, Xiaolin; Chen, Xu; Ameen, Asif; Xie, Guanghui, China Agr Univ, Natl Energy R\&D Ctr Nonfood Biomass, Beijing 100193, Peoples R China.}},
DOI = {{10.1016/j.fcr.2017.09.031}},
ISSN = {{0378-4290}},
EISSN = {{1872-6852}},
Keywords = {{Sweet sorghum; Biomass sorghum; Quality components; Theoretical ethanol
   yield; Net soil nitrate accumulation}},
Keywords-Plus = {{WATER-USE-EFFICIENCY; IN-VITRO DIGESTIBILITY; BICOLOR L. MOENCH; SWEET
   SORGHUM; FORAGE SORGHUM; NUTRITIVE-VALUE; BIOENERGY FEEDSTOCK;
   ETHANOL-PRODUCTION; FIBER SORGHUM; WINTER-WHEAT}},
Research-Areas = {{Agriculture}},
Web-of-Science-Categories  = {{Agronomy}},
Author-Email = {{yangxiaolin429@163.com}},
ORCID-Numbers = {{Ameen, Asif/0000-0002-3982-7000}},
Funding-Acknowledgement = {{National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China {[}31470555]}},
Funding-Text = {{This work was supported by the National Natural Science Foundation of
   China (31470555).}},
Number-of-Cited-References = {{70}},
Times-Cited = {{5}},
Usage-Count-Last-180-days = {{3}},
Usage-Count-Since-2013 = {{26}},
Journal-ISO = {{Field Crop. Res.}},
Doc-Delivery-Number = {{FP5KB}},
Unique-ID = {{ISI:000417657300002}},
DA = {{2019-10-28}},
}

@article{ ISI:000417657300013,
Author = {Ordonez, Raziel A. and Castellano, Michael J. and Hatfield, Jerry L. and
   Helmers, Matthew J. and Licht, Mark A. and Liebman, Matt and Dietzel,
   Ranae and Martinez-Feria, Rafael and Iqbal, Javed and Puntel, Laila A.
   and Cordova, S. Carolina and Togliatti, Kaitlin and Wright, Emily E. and
   Archontoulis, Sotirios V.},
Title = {{Maize and soybean root front velocity and maximum depth in Iowa, USA}},
Journal = {{FIELD CROPS RESEARCH}},
Year = {{2018}},
Volume = {{215}},
Pages = {{122-131}},
Month = {{JAN}},
Abstract = {{Quantitative measurements of root traits can improve our understanding
   of how crops respond to soil and weather conditions, but such data are
   rare. Our objective was to quantify maximum root depth and root front
   velocity (RFV) for maize (Zea mays) and soybean (Glycine max) crops
   across a range of growing conditions in the Midwest USA. Two sets of
   root measurements were taken every 10-15 days: in the crop row (in-row)
   and between two crop rows (center-row) across six Iowa sites having
   different management practices such as planting dates and drainage
   systems, totaling 20 replicated experimental treatments. Temporal root
   data were best described by linear segmental functions. Maize RFV was
   0.62 +/- 0.2 cm d(-1) until the 5th leaf stage when it increased to 3.12
   +/- 0.03 an d(-1) until maximum depth occurred at the 18th leaf stage
   (860 degrees Cd after planting). Similar to maize, soybean RFV was 1.19
   +/- 0.4 cm d(-1) until the 3rd node when it increased to 3.31 +/- 0.5 cm
   d(-1) until maximum root depth occurred at the 13th node (813.6 degrees
   C d after planting). The maximum root depth was similar between crops (P
   > 0.05) and ranged from 120 to 157 cm across 18 experimental treatments,
   and 89-90 cm in two experimental treatments. Root depth did not exceed
   the average water table (two weeks prior to start grain filling) and
   there was a significant relationship between maximum root depth and
   water table depth (R-2 = 0.61; P = 0.001). Current models of root
   dynamics rely on temperature as the main control on root growth; our
   results provide strong support for this relationship (R-2 > 0.76; P <
   0.001), but suggest that water table depth should also be considered,
   particularly in conditions such as the Midwest USA where excess water
   routinely limits crop production. These results can assist crop model
   calibration and improvements as well as agronomic assessments and plant
   breeding efforts in this region.}},
Publisher = {{ELSEVIER SCIENCE BV}},
Address = {{PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Ordonez, RA; Archontoulis, SV (Reprint Author), Iowa State Univ, Dept Agron, Agron Hall, Ames, IA 50011 USA.
   Ordonez, Raziel A.; Castellano, Michael J.; Licht, Mark A.; Liebman, Matt; Dietzel, Ranae; Martinez-Feria, Rafael; Iqbal, Javed; Puntel, Laila A.; Cordova, S. Carolina; Togliatti, Kaitlin; Wright, Emily E.; Archontoulis, Sotirios V., Iowa State Univ, Dept Agron, Agron Hall, Ames, IA 50011 USA.
   Hatfield, Jerry L., USDA ARS, Natl Lab Agr \& Environm, Ames, IA 50011 USA.
   Helmers, Matthew J., Iowa State Univ, Dept Agr \& Biosyst Engn, Elings Hall, Ames, IA 50011 USA.}},
DOI = {{10.1016/j.fcr.2017.09.003}},
ISSN = {{0378-4290}},
EISSN = {{1872-6852}},
Keywords = {{Root depth; Root front velocity; Water table; Temperature; Modeling}},
Keywords-Plus = {{ZEA-MAYS L.; WATER EXTRACTION PATTERNS; TRITICUM-AESTIVUM L.;
   SOIL-WATER; GRAIN-YIELD; GREEN-REVOLUTION; CROPPING SYSTEMS; FIELD
   CONDITIONS; AVAILABLE WATER; NITROGEN LOSSES}},
Research-Areas = {{Agriculture}},
Web-of-Science-Categories  = {{Agronomy}},
Author-Email = {{ordonez@iastate.edu
   sarchont@iastate.edu}},
ResearcherID-Numbers = {{Castellano, Michael/A-9687-2008
   Licht, Mark/L-4030-2019
   }},
ORCID-Numbers = {{Castellano, Michael/0000-0003-1411-7931
   Licht, Mark/0000-0001-6640-7856
   Martinez-Feria, Rafael/0000-0002-4230-5684}},
Funding-Acknowledgement = {{Iowa Soybean Association, Department of Agronomy, Plant Science
   Institute of Iowa State University; USDA-NIFAUnited States Department of
   Agriculture (USDA) {[}10W03814]}},
Funding-Text = {{This study was funded by the Iowa Soybean Association, Department of
   Agronomy, Plant Science Institute of Iowa State University, and
   USDA-NIFA Hatch project 10W03814. We thank Emily Marrs, Cooper Smith and
   Oluwakorede Olugbenle for their help in data collection.}},
Number-of-Cited-References = {{88}},
Times-Cited = {{13}},
Usage-Count-Last-180-days = {{2}},
Usage-Count-Since-2013 = {{20}},
Journal-ISO = {{Field Crop. Res.}},
Doc-Delivery-Number = {{FP5KB}},
Unique-ID = {{ISI:000417657300013}},
OA = {{Green Published}},
DA = {{2019-10-28}},
}

@article{ ISI:000438481300005,
Author = {Salehuddin, Khazriyati},
Title = {{CAN THE EYE TRACKER REVEAL HOW THE QUR'AN CAN BE LEARNED BY HEART?}},
Journal = {{AL-SHAJARAH}},
Year = {{2018}},
Volume = {{23}},
Number = {{1}},
Pages = {{125-147}},
Abstract = {{Memorization is a mental process that enables one to remember verbatim.
   This skill is important to Muslims as Muslims are encouraged to memorize
   the Qur'an regardless of what their first language is. Various
   researches have been conducted and published in describing the best
   techniques to memorize the Qur'an. Huffaz (i.e., individuals who have
   memorized all the 6236 verses of the Qur'an) also often share their
   strategies on how the Qur'an, which is in Arabic, can be memorized.
   These published works, however, are based on off-line information (i.e.,
   information based on what the huffaz thought took place while they were
   learning the Qur'an by heart). On-line information, however, is equally
   important, particularly because it provides information even on the
   unconscious activities that the huffaz engage in when memorizing the
   Qur'an. One way of collecting on-line information from the huffaz and
   tahfiz students (i.e., students who are memorizing the Qur'an) is
   through the use of the eye tracker, a device that enables researchers to
   track the eye movements of those who read the Qur'an to memorize it.
   Hence, this manuscript illustrates how the eye tracker can be used to
   investigate the cognitive processes that tahfiz students go through when
   the act of memorizing the Qur'an is performed.}},
Publisher = {{INT ISLAMIC UNIV MALAYSIA}},
Address = {{NO 24 PERSIARAN DUTA, TAMAN DUTA, KUALA LUMPUR, 50480, MALAYSIA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Salehuddin, K (Reprint Author), Univ Kebangsaan Malaysia, Bangi, Malaysia.
   Salehuddin, K (Reprint Author), Univ Kebangsaan Malaysia, Language \& Cognit Cluster, Bangi, Malaysia.
   Salehuddin, Khazriyati, Univ Kebangsaan Malaysia, Bangi, Malaysia.
   Salehuddin, Khazriyati, Univ Kebangsaan Malaysia, Language \& Cognit Cluster, Bangi, Malaysia.}},
ISSN = {{1394-6870}},
Keywords = {{Cognitive Processes; Eye Movements; Memorization; Psycholinguistics;
   Qur'an; Reading}},
Keywords-Plus = {{COMPREHENSION; SKILLS}},
Research-Areas = {{Religion}},
Web-of-Science-Categories  = {{Religion}},
Funding-Acknowledgement = {{Ministry of Higher Education of MalaysiaMinistry of Education, Malaysia
   {[}FRGS/1/2015/SSI01/UKM/02/4]}},
Funding-Text = {{The author would like to thank the Ministry of Higher Education of
   Malaysia for funding the FRGS/1/2015/SSI01/UKM/02/4 research project
   entitled Qur'anic Memorization Techniques: A Psycholinguistic Module for
   Non-Arabic-Speaking Malay speakers.}},
Number-of-Cited-References = {{28}},
Times-Cited = {{1}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{1}},
Journal-ISO = {{Al-Shajarah}},
Doc-Delivery-Number = {{GM8LY}},
Unique-ID = {{ISI:000438481300005}},
DA = {{2019-10-28}},
}

@article{ ISI:000437119900007,
Author = {Fievez, Faustine Perrin and Lions, Cynthia and Bucci, Maria Pia},
Title = {{Preliminary Study: Impact of Strabismus and Surgery on Eye Movements
   When Children are Reading}},
Journal = {{STRABISMUS}},
Year = {{2018}},
Volume = {{26}},
Number = {{2}},
Pages = {{96-104}},
Abstract = {{Aim: To evaluate differences in eye movements during reading in
   strabismic children and in non-strabismic age-matched children, and to
   evaluate the potential effect of strabismus surgery on eye movement
   performance.
   Methods: The eye movements of nine strabismic children from 11 to 15
   years old were recorded with an eye tracker as they were reading a text
   under three visual conditions before and six months after eye surgery.
   The results were compared with those obtained from control groups of
   non-strabismic age-matched children. Eye movements were recorded during
   reading a text with both eyes open and under monocular vision with the
   dominant and non-dominant eye alternately covered.
   Results: The duration of fixation was longer in strabismic children than
   in age-matched non-strabismic children. Children read faster under
   dominant eye open condition than under both eyes open condition. Surgery
   allowed an increase of reading speed and decrease of fixation duration.
   The number of backward saccades significantly decreased after surgery.
   Conclusion: In strabismic children, eye movements during reading are
   impaired. The reduction of the squint allowed a better word
   comprehension.}},
Publisher = {{TAYLOR \& FRANCIS INC}},
Address = {{530 WALNUT STREET, STE 850, PHILADELPHIA, PA 19106 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Bucci, MP (Reprint Author), Robert Debre Univ Hosp, INSERM, UMR 1141, 48 Blvd Serurier, F-75019 Paris, France.
   Fievez, Faustine Perrin; Lions, Cynthia; Bucci, Maria Pia, Univ Paris 07, Robert Debre Univ Hosp, INSERM, UMR 1141, Paris, France.
   Fievez, Faustine Perrin; Lions, Cynthia; Bucci, Maria Pia, Robert Debre Univ Hosp, Vestibular \& Oculomotor Evaluat Unit, ENT Dept, Paris, France.
   Lions, Cynthia, Claude Bernard Lyon 1 Univ, Lyon Neurosci Res Ctr, INSERM, CNRS,UMR 5292,U1028, Villeurbanne, France.}},
DOI = {{10.1080/09273972.2018.1445761}},
ISSN = {{0927-3972}},
EISSN = {{1744-5132}},
Keywords = {{Fixation; reading; saccades; strabismus; surgery}},
Keywords-Plus = {{BINOCULAR COORDINATION; AMBLYOPIA; SACCADES; PREVALENCE}},
Research-Areas = {{Ophthalmology}},
Web-of-Science-Categories  = {{Ophthalmology}},
Author-Email = {{maria-pia.bucci@inserm.fr}},
Funding-Acknowledgement = {{Fondation pour la Recherche Medicale `Pathophysiology of the Visual
   System 2013' {[}DVS20131228511]}},
Funding-Text = {{The study was supported by Fondation pour la Recherche Medicale
   `Pathophysiology of the Visual System 2013' (DVS20131228511).}},
Number-of-Cited-References = {{25}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{4}},
Journal-ISO = {{Strabismus}},
Doc-Delivery-Number = {{GL4KM}},
Unique-ID = {{ISI:000437119900007}},
DA = {{2019-10-28}},
}

@article{ ISI:000434632700011,
Author = {Rocha Porto, Maria Augusta and Ko Freitag, Raquel Meister and Tejada,
   Julian},
Title = {{Silent reading time and reading aloud with young, older adults and
   elderly people}},
Journal = {{LETRAS DE HOJE-ESTUDOS E DEBATES EM LINGUISTICA LITERATURA E LINGUA
   PORTUGUESA}},
Year = {{2018}},
Volume = {{53}},
Number = {{1}},
Pages = {{100-108}},
Month = {{JAN-MAR}},
Abstract = {{This study aims to measure the Portuguese and English reading time (in
   silent and reading aloud texts), as well as the eyetracking control (the
   amount and the number of fixations and regressions (go-past reading),
   with three different experimental groups (10 for each one of the
   groups): young, old, and aging-elderly people, using eye-tracker
   EyeTribe. The results showed that the young participant group differed
   from the other two groups, with the measure length time ranging from 200
   to 250 ms. The young participant group obtained a longer time in reading
   English than Portuguese. The elderly group showed a higher number than
   the established standard one of 500 ms, averaging between 600-650 ms. In
   English texts, the reading time was longer in the three categories. The
   participants read English texts faster when reading silently than when
   reading aloud.}},
Publisher = {{EDITORA UNIV PONTIFICIA UNIV CATOLICA RIO GRANDE SUL-EDIPUCRS}},
Address = {{AVE IPIRANGA 6681, PREDIO 33, PORTO ALEGRE, RS 90619-900, BRAZIL}},
Type = {{Article}},
Language = {{Portuguese}},
Affiliation = {{Porto, MAR (Reprint Author), Univ Fed Sergipe, Sao Cristovao, SE, Brazil.
   Rocha Porto, Maria Augusta; Ko Freitag, Raquel Meister; Tejada, Julian, Univ Fed Sergipe, Sao Cristovao, SE, Brazil.}},
DOI = {{10.15448/1984-7726.2018.1.28670}},
ISSN = {{0101-3335}},
EISSN = {{1984-7726}},
Keywords = {{Reading time; Aging; Eye tracking}},
Keywords-Plus = {{EYE-MOVEMENTS; COMPREHENSION}},
Research-Areas = {{Literature}},
Web-of-Science-Categories  = {{Literature}},
Author-Email = {{mariaaugusta.porto@gmail.com}},
ResearcherID-Numbers = {{Freitag, Raquel/E-4156-2017
   }},
ORCID-Numbers = {{Freitag, Raquel/0000-0002-4972-4320
   Tejada, Julian/0000-0003-0275-3578}},
Number-of-Cited-References = {{23}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{3}},
Journal-ISO = {{Let. Hoje}},
Doc-Delivery-Number = {{GI6SO}},
Unique-ID = {{ISI:000434632700011}},
OA = {{DOAJ Gold}},
DA = {{2019-10-28}},
}

@article{ ISI:000428122800015,
Author = {Wan, Qian and Song, Sha Sha and Li, Xiao He and Zhang, Qi and Yang, Xiao
   and Zhang, Ya Chi and Fei, Ben Hua and Yao, Li Hong},
Title = {{THE VISUAL PERCEPTION OF THE CARDBOARD PORDUCT USING EYE-TRACKING
   TECHNOLOGY}},
Journal = {{WOOD RESEARCH}},
Year = {{2018}},
Volume = {{63}},
Number = {{1}},
Pages = {{165-178}},
Abstract = {{Consumers' visual perception towards the product's appearance can
   largely affect their preference and purchase intention of the product.
   Recently, the cardboard product, as a kind of environmentally friendly
   product, is becoming more and more popular in the market. Therefore,
   understanding the perception of consumers' visual evaluation toward
   different cardboard products is crucial for cardboard product design.
   This study used eye-tracking technology and subjective evaluation
   together to investigate people's visual perception evoked by different
   cardboard products. Nine different cardboard products' pictures chosen
   from the internet were divided into three different types and three
   different assembly structures. Participants were then asked to observe
   those pictures when their eye movement behaviors were recorded by an eye
   tracker. Additionally, a questionnaire about the participant's fondness
   and purchase of those cardboard products were filled out after the
   eye-tracking test. Consumers spent less average fixation duration on the
   cardboard product with more usability and more familiar form to evaluate
   their appearances. And stronger fondness and purchase intention of those
   kinds of cardboard products were showed in consumers' visual perception.
   The exploring of the eye movement measurement on the visual perception
   can provide an accurate method for designers to better understanding the
   consumer's fondness and purchase intention of the cardboard product.
   Taking consumers' eye tracking metrics into account may help the product
   design meet their real needs in the cardboard product market.}},
Publisher = {{SLOVAK FOREST PRODUCTS RESEARCH INST}},
Address = {{LAMACSKA CESTA 3, BRATISLAVA, SK-841 04, SLOVAKIA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Song, SS (Reprint Author), Beijing Forestory Univ, Beijing Key Lab Wood Sci \& Engn, Beijing, Peoples R China.
   Yao, LH (Reprint Author), Inner Mongolia Agr Univ, Coll Mat Sci \& Art Design, Hohhot, Peoples R China.
   Wan, Qian; Song, Sha Sha; Zhang, Qi; Yang, Xiao; Zhang, Ya Chi, Beijing Forestory Univ, Beijing Key Lab Wood Sci \& Engn, Beijing, Peoples R China.
   Li, Xiao He, Univ Pittsburgh, Dept Educ, Pittsburgh, PA USA.
   Fei, Ben Hua, Int Ctr Bamboo \& Rattan, 8 Futong Dongdajie, Beijing 100102, Peoples R China.
   Yao, Li Hong, Inner Mongolia Agr Univ, Coll Mat Sci \& Art Design, Hohhot, Peoples R China.}},
ISSN = {{1336-4561}},
Keywords = {{Visual perception; eye tracking; cardboard product; product design}},
Keywords-Plus = {{EVENT-RELATED POTENTIALS; PRODUCT FORM DESIGN; MOVEMENTS; ATTENTION;
   SATISFACTION; INFORMATION; BEHAVIOR; ONLINE}},
Research-Areas = {{Materials Science}},
Web-of-Science-Categories  = {{Materials Science, Paper \& Wood}},
Author-Email = {{songrui\_1688@126.com
   yaolihong82@163.com}},
Funding-Acknowledgement = {{``Nation Key R\&D Program{''} support projects ``Demonstration of
   continuous complete manufacturing technology integration for the
   materials of bamboo-based furniture{''} {[}2016YFD0600905]; Co-built
   Project; Beijing Municipal Education Commission ``R\&D on Key Technology
   in Scientific Utilization of Non-wood Plant Raw Material{''}}},
Funding-Text = {{The study was supported by ``Nation Key R\&D Program{''} support
   projects ``Demonstration of continuous complete manufacturing technology
   integration for the materials of bamboo-based furniture{''}
   (2016YFD0600905). The support of the Co-built Project with Beijing
   Municipal Education Commission ``R\&D on Key Technology in Scientific
   Utilization of Non-wood Plant Raw Material{''}. Thanks to King-Far
   International Inc for providing us the research equipment and technology
   support.}},
Number-of-Cited-References = {{49}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{7}},
Journal-ISO = {{Wood Res.}},
Doc-Delivery-Number = {{GA2BX}},
Unique-ID = {{ISI:000428122800015}},
DA = {{2019-10-28}},
}

@article{ ISI:000426283600001,
Author = {Memar, Amirhossein H. and Esfahani, Ehsan T.},
Title = {{Physiological Measures for Human Performance Analysis in Human-Robot
   Teamwork: Case of Tele-Exploration}},
Journal = {{IEEE ACCESS}},
Year = {{2018}},
Volume = {{6}},
Pages = {{3694-3705}},
Abstract = {{Continuous monitoring of mental workload and situation awareness in
   operational environments is useful for understanding and prediction of
   human performance. Such information can be used to develop real-time
   adaptive systems to enhance human performance. In this paper, we
   investigate the use of workload- and attention-related physiological
   measures to predict operator performance and situation awareness in the
   context of tele-exploration with a small team of robots. A user study is
   conducted based on a simulated scenario involving visual scanning and
   manual control tasks with varying levels of task-load. Brain activity
   and eye movements of the participants are monitored across the
   experimental tasks using electroencephalogram and eye tracker sensors.
   The performances of the subjects are evaluated in terms of target
   detection and situation awareness (primary metrics) as well as reaction
   time and false detection (secondary metrics). Moreover, individual
   differences in two specific visual skills, visual search (VS) and
   multi-object tracking (MOT) are considered as between-subject factors in
   the experimental design. The main effects of task type and individual
   differences reveal that VS and MOT skill have significant effects on
   target detection and situation awareness, respectively. The correlations
   of physiological measures with the task performance and situation
   awareness are analyzed. The results suggest that brain-based features
   (mental workload and distraction) which represent the covert aspect of
   attention are better suited to predict the secondary performance
   metrics. On the other hand, glance-based features which represent the
   overt aspect of attention are shown to be the best predictors of the
   primary performance metrics.}},
Publisher = {{IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC}},
Address = {{445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Esfahani, ET (Reprint Author), Univ Buffalo, Dept Mech \& Aerosp Engn, Buffalo, NY 14260 USA.
   Memar, Amirhossein H.; Esfahani, Ehsan T., Univ Buffalo, Dept Mech \& Aerosp Engn, Buffalo, NY 14260 USA.}},
DOI = {{10.1109/ACCESS.2018.2790838}},
ISSN = {{2169-3536}},
Keywords = {{Human-robot interaction; human performance; individual differences;
   mental workload; physiological measures; situation awareness}},
Keywords-Plus = {{VIDEO-GAME EXPERIENCE; SITUATION AWARENESS; SUPERVISORY CONTROL; MENTAL
   WORKLOAD; MULTIPLE ROBOTS; AUTOMATION; SYSTEM; MEMORY; EEG; SKILLS}},
Research-Areas = {{Computer Science; Engineering; Telecommunications}},
Web-of-Science-Categories  = {{Computer Science, Information Systems; Engineering, Electrical \&
   Electronic; Telecommunications}},
Author-Email = {{ehsanesf@buffalo.edu}},
ResearcherID-Numbers = {{Esfahani, Ehsan T./B-6095-2008}},
ORCID-Numbers = {{Esfahani, Ehsan T./0000-0001-5893-4664}},
Number-of-Cited-References = {{58}},
Times-Cited = {{1}},
Usage-Count-Last-180-days = {{2}},
Usage-Count-Since-2013 = {{8}},
Journal-ISO = {{IEEE Access}},
Doc-Delivery-Number = {{FX7PU}},
Unique-ID = {{ISI:000426283600001}},
OA = {{DOAJ Gold}},
DA = {{2019-10-28}},
}

@article{ ISI:000414663900010,
Author = {Watalingam, Renuka Devi and Richetelli, Nicole and Pelz, Jeff B. and
   Speir, Jacqueline A.},
Title = {{Eye tracking to evaluate evidence recognition in crime scene
   investigations}},
Journal = {{FORENSIC SCIENCE INTERNATIONAL}},
Year = {{2017}},
Volume = {{280}},
Pages = {{64-80}},
Month = {{NOV}},
Abstract = {{Crime scene analysts are the core of criminal investigations; decisions
   made at the scene greatly affect the speed of analysis and the quality
   of conclusions, thereby directly impacting the successful resolution of
   a case. If an examiner fails to recognize the pertinence of an item on
   scene, the analyst's theory regarding the crime will be limited.
   Conversely, unselective evidence collection will most likely include
   irrelevant material, thus increasing a forensic laboratory's backlog and
   potentially sending the investigation into an unproductive and costly
   direction. Therefore, it is critical that analysts recognize and
   properly evaluate forensic evidence that can assess the relative support
   of differing hypotheses related to event reconstruction. With this in
   mind, the aim of this study was to determine if quantitative eye
   tracking data and qualitative reconstruction accuracy could be used to
   distinguish investigator expertise. In order to assess this, 32
   participants were successfully recruited and categorized as experts or
   trained novices based on their practical experiences and educational
   backgrounds. Each volunteer then processed a mock crime scene while
   wearing a mobile eye tracker, wherein visual fixations, durations,
   search patterns, and reconstruction accuracy were evaluated. The eye
   tracking data (dwell time and task percentage on areas of interest or
   AOIs) were compared using Earth Mover's Distance (EMD) and the
   Needleman-Wunsch (N-W) algorithm, revealing significant group
   differences for both search duration (EMD), as well as search sequence
   (N-W). More specifically, experts exhibited greater dissimilarity in
   search duration, but greater similarity in search sequences than their
   novice counterparts. In addition to the quantitative visual assessment
   of examiner variability, each participant's reconstruction skill was
   assessed using a 22-point binary scoring system, in which significant
   group differences were detected as a function of total reconstruction
   accuracy. This result, coupled with the fact that the study failed to
   detect a significant difference between the groups when evaluating the
   total time needed to complete the investigation, indicates that experts
   are more efficient and effective. Finally, the results presented here
   provide a basis for continued research in the use of eye trackers to
   assess expertise in complex and distributed environments, including
   suggestions for future work, and cautions regarding the degree to which
   visual attention can infer cognitive understanding. (C) 2017 Elsevier
   B.V. All rights reserved.}},
Publisher = {{ELSEVIER IRELAND LTD}},
Address = {{ELSEVIER HOUSE, BROOKVALE PLAZA, EAST PARK SHANNON, CO, CLARE, 00000,
   IRELAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Speir, JA (Reprint Author), West Virginia Univ, 208 Oglebay Hall,POB 6121, Morgantown, WV 26506 USA.
   Watalingam, Renuka Devi; Richetelli, Nicole; Speir, Jacqueline A., West Virginia Univ, 208 Oglebay Hall,POB 6121, Morgantown, WV 26506 USA.
   Pelz, Jeff B., Rochester Inst Technol, Chester F Carlson Ctr Imaging Sci, 54 Lomb Mem Dr, Rochester, NY 14623 USA.
   Watalingam, Renuka Devi, Ocala Police Dept, Evidence Sect, 402 S Pine Ave, Ocala, FL 34471 USA.}},
DOI = {{10.1016/j.forsciint.2017.08.012}},
ISSN = {{0379-0738}},
EISSN = {{1872-6283}},
Keywords = {{Crime scene investigation; Expertise; Eye tracking; Significance of
   evidence; Interpretation of evidence}},
Keywords-Plus = {{PERFORMANCE; STRATEGIES; EXPERIENCE; NOVICES; SKILLS}},
Research-Areas = {{Legal Medicine}},
Web-of-Science-Categories  = {{Medicine, Legal}},
Author-Email = {{Jacqueline.Speir@mail.wvu.edu}},
ORCID-Numbers = {{Richetelli, Nicole/0000-0001-6126-2504
   Speir, Jacqueline/0000-0002-8833-4800
   Pelz, Jeff/0000-0002-0649-3876}},
Number-of-Cited-References = {{30}},
Times-Cited = {{2}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{18}},
Journal-ISO = {{Forensic Sci.Int.}},
Doc-Delivery-Number = {{FM0NO}},
Unique-ID = {{ISI:000414663900010}},
DA = {{2019-10-28}},
}

@article{ ISI:000412506900011,
Author = {Keller, Carmen and Junghans, Alex},
Title = {{Does Guiding Toward Task-Relevant Information Help Improve Graph
   Processing and Graph Comprehension of Individuals with Low or High
   Numeracy? An Eye-Tracker Experiment}},
Journal = {{MEDICAL DECISION MAKING}},
Year = {{2017}},
Volume = {{37}},
Number = {{8}},
Pages = {{942-954}},
Month = {{NOV}},
Abstract = {{Background. Individuals with low numeracy have difficulties with
   understanding complex graphs. Combining the information-processing
   approach to numeracy with graph comprehension and information-reduction
   theories, we examined whether high numerates' better comprehension might
   be explained by their closer attention to task-relevant graphical
   elements, from which they would expect numerical information to
   understand the graph. Furthermore, we investigated whether participants
   could be trained in improving their attention to task-relevant
   information and graph comprehension. Design. In an eye-tracker
   experiment (N = 110) involving a sample from the general population, we
   presented participants with 2 hypothetical scenarios (stomach cancer,
   leukemia) showing survival curves for 2 treatments. In the training
   condition, participants received written instructions on how to read the
   graph. In the control condition, participants received another text. We
   tracked participants' eye movements while they answered 9 knowledge
   questions. The sum constituted graph comprehension. We analyzed visual
   attention to task-relevant graphical elements by using relative fixation
   durations and relative fixation counts. Results. The mediation analysis
   revealed a significant (P < 0.05) indirect effect of numeracy on graph
   comprehension through visual attention to task-relevant information,
   which did not differ between the 2 conditions. Training had a
   significant main effect on visual attention (P < 0.05) but not on graph
   comprehension (P < 0.07). Conclusions. Individuals with high numeracy
   have better graph comprehension due to their greater attention to
   task-relevant graphical elements than individuals with low numeracy.
   With appropriate instructions, both groups can be trained to improve
   their graph-processing efficiency. Future research should examine (e.g.,
   motivational) mediators between visual attention and graph comprehension
   to develop appropriate instructions that also result in higher graph
   comprehension.}},
Publisher = {{SAGE PUBLICATIONS INC}},
Address = {{2455 TELLER RD, THOUSAND OAKS, CA 91320 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Keller, C (Reprint Author), ZHAW Sch Appl Psychol, Pfingstweidstr 96, CH-8005 Zurich, Switzerland.
   Keller, Carmen, Swiss Fed Inst Technol, Dept Hlth Sci \& Technol, Consumer Behav, Zurich, Switzerland.
   Junghans, Alex, Dept Hlth Sci \& Technol, Consumer Behav, Zurich, Switzerland.}},
DOI = {{10.1177/0272989X17713473}},
ISSN = {{0272-989X}},
EISSN = {{1552-681X}},
Keywords = {{numeracy; graph comprehension; visual attention; survival curve;
   training}},
Keywords-Plus = {{RISK PERCEPTION; TOP-DOWN; FORMAT; KNOWLEDGE; LITERACY; PEOPLE; BOTTOM;
   IMPACT}},
Research-Areas = {{Health Care Sciences \& Services; Medical Informatics}},
Web-of-Science-Categories  = {{Health Care Sciences \& Services; Medical Informatics}},
Author-Email = {{carmen.keller@zhaw.ch}},
Funding-Acknowledgement = {{Swiss National Science FoundationSwiss National Science Foundation
   (SNSF) {[}100014-143335/1]}},
Funding-Text = {{Financial support for this study was provided entirely by a grant from
   the Swiss National Science Foundation, under grant number
   100014-143335/1. The funding agreement ensured the authors' independence
   in designing the study, interpreting the data, writing, and publishing
   the report. Revision accepted for publication 25 April 2017.}},
Number-of-Cited-References = {{28}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{24}},
Journal-ISO = {{Med. Decis. Mak.}},
Doc-Delivery-Number = {{FJ1VA}},
Unique-ID = {{ISI:000412506900011}},
DA = {{2019-10-28}},
}

@article{ ISI:000419494000007,
Author = {Lim, Hyo Jung},
Title = {{Exploring Test Takers' Cognition in a High-stakes Reading Test: An
   Eye-tracking Study}},
Journal = {{JOURNAL OF ASIA TEFL}},
Year = {{2017}},
Volume = {{14}},
Number = {{3}},
Pages = {{482-500}},
Month = {{FAL}},
Abstract = {{Learning how test takers derive their answers, referred to as the
   test-taking process, is essential to test validation. However, research
   on test takers' mental process has been impeded by the excessive
   popularity of quantitative analysis and the limitations of research
   methods. By using eye-tracking technology, this study investigated
   Chinese ESL learners' reading processes in the iBT TOEFL reading section
   for the purpose of test validation. Ninety Chinese ESL students at a
   large Midwestern university were invited to an eye-tracking lab and
   asked to take a 20-minute reading test on a computer screen to which an
   eye tracker was attached. As in Bax and Weir (2012), this study analyzed
   only high scorers' eye movement data (e.g., gaze plots and fixation
   durations) based on the predetermined criteria. The reading time spent
   on the intended reading types was also calculated as evidence of
   validity. The results showed that participants predominantly exercised
   careful reading skills below the paragraph level, vocabulary questions
   seemingly failed to elicit inferencing ability, and expeditious reading
   rarely occurred. Learners did not spend more time reading a text
   associated with implicit questions compared to a text associated with
   explicit questions. The implication of the findings for test validity
   and test development are also discussed.}},
Publisher = {{ASIA TEFL}},
Address = {{ASIA TEFL, GWANGJU, 00000, SOUTH KOREA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Lim, HJ (Reprint Author), Kwangwoon Univ, Dept English Language \& Literature, Seoul, South Korea.
   Lim, Hyo Jung, Kwangwoon Univ, Dept English Language \& Literature, Seoul, South Korea.}},
DOI = {{10.18823/asiatefl.2017.14.3.7.482}},
ISSN = {{1738-3102}},
Keywords = {{cognitive validity; eye-tracking study; a reading test}},
Keywords-Plus = {{COMPREHENSION; MOVEMENTS}},
Research-Areas = {{Education \& Educational Research}},
Web-of-Science-Categories  = {{Education \& Educational Research}},
Author-Email = {{lim@kw.ac.kr}},
Number-of-Cited-References = {{45}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{3}},
Journal-ISO = {{J. Asia TEFL}},
Doc-Delivery-Number = {{FS0WI}},
Unique-ID = {{ISI:000419494000007}},
OA = {{Bronze}},
DA = {{2019-10-28}},
}

@article{ ISI:000406897400006,
Author = {Brazil, William and Caulfield, Brian},
Title = {{What makes an effective energy efficiency label? Assessing the
   performance of energy labels through eye-tracking experiments in Ireland}},
Journal = {{ENERGY RESEARCH \& SOCIAL SCIENCE}},
Year = {{2017}},
Volume = {{29}},
Pages = {{46-52}},
Month = {{JUL}},
Abstract = {{Energy labels are a means of providing consumers with a way of
   understanding the environmental and energy impacts associated with many
   products and services. These labels are designed to help consumers to
   compare options and to select more energy efficient products. Such
   labels often contain large amounts of information, displayed in a range
   of metrics, such as carbon dioxide emissions in grams, energy use in
   kilowatt hours, and noise emissions in decibels. This paper presents the
   results of an eye-tracking experiment undertaken to assess energy labels
   from both the whitegoods and motor sectors. Specific focus is put upon
   the elements that make up these labels, and how effective these elements
   are at transmitting the necessary information to the consumer. Based
   upon the results of this study it is possible to generate heat maps,
   scan paths, and statistical analysis formed from defined areas of
   interest within the energy labels. These results highlight the
   effectiveness of colour coding (traffic light style) and simple
   alphabetic grading, in terms of drawing the attention of consumers. This
   research also contained information recall tasks based upon the data
   provided to respondents, and an analysis of these results highlights the
   notable variation in recollection accuracy for the different types of
   information provided by the labels. Based upon these results, a number
   of recommendations are made concerning potential label design.}},
Publisher = {{ELSEVIER SCIENCE BV}},
Address = {{PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Caulfield, B (Reprint Author), Trinity Coll Dublin, Dublin, Ireland.
   Brazil, William; Caulfield, Brian, Trinity Coll Dublin, Dublin, Ireland.}},
DOI = {{10.1016/j.erss.2017.05.014}},
ISSN = {{2214-6296}},
EISSN = {{2214-6326}},
Keywords = {{Energy labels; Eye tracker}},
Keywords-Plus = {{INFORMATION}},
Research-Areas = {{Environmental Sciences \& Ecology}},
Web-of-Science-Categories  = {{Environmental Studies}},
Author-Email = {{brian.caulfield@tcd.ie}},
Funding-Acknowledgement = {{Environmental Protection Agency (Ireland)United States Environmental
   Protection Agency}},
Funding-Text = {{This research was funded by the Environmental Protection Agency
   (Ireland) under their sustainability research programme.}},
Number-of-Cited-References = {{18}},
Times-Cited = {{2}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{1}},
Journal-ISO = {{Energy Res. Soc. Sci.}},
Doc-Delivery-Number = {{FC5QS}},
Unique-ID = {{ISI:000406897400006}},
DA = {{2019-10-28}},
}

@article{ ISI:000399225200012,
Author = {Li, Lidong and Zhang, Qingnian},
Title = {{Research on Visual Cognition About Sharp Turn Sign Based on Driver's Eye
   Movement Characteristic}},
Journal = {{INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE}},
Year = {{2017}},
Volume = {{31}},
Number = {{7}},
Month = {{JUL}},
Abstract = {{As a traffic language, sharp turn sign is a kind of important road
   infrastructure that indicates a sharp turning will show up at the coming
   road, warning drivers to slow down to ensure the driving safety. In this
   paper, real vehicle test was carried out on mountain road with an eye
   tracker equipment. At differerent driving speeds, parameters of eye
   movement characteristics in the visual cognition process of sharp turn
   signs were collected, including distribution of gaze points, fixation
   and saccade. Simultaneously, driver's scan paths of recognizing sharp
   turn signs with diffrerent supporting forms were gathered. The results
   of the analysis of testing data showed that the dispersion of
   distribution of gaze points would increase with driving velocity.
   Saccade was the main method for driver to capture information of sharp
   turn signs. While driving speed was lower than 60 km/h, fixation was
   also one of the methods. For the visual cognition process of sharp turn
   sign with cantilever, compared to post, the searching scope was wider
   both in horizontal and vertical directions. This study is beneficial to
   evaluate the rationality of sharp turn signs, promoting the using
   efficiency of signs and improving the driving safety on mountain road.}},
Publisher = {{WORLD SCIENTIFIC PUBL CO PTE LTD}},
Address = {{5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Li, LD (Reprint Author), Wuhan Univ Technol, Sch Transportat, Wuhan 430063, Peoples R China.
   Li, Lidong; Zhang, Qingnian, Wuhan Univ Technol, Sch Transportat, Wuhan 430063, Peoples R China.}},
DOI = {{10.1142/S0218001417590121}},
Article-Number = {{1759012}},
ISSN = {{0218-0014}},
EISSN = {{1793-6381}},
Keywords = {{Sharp turn sign; visual cognition process; driver; eye movement
   characteristic}},
Keywords-Plus = {{TRAFFIC SIGNS; COMPREHENSIBILITY; BEHAVIOR; SAFETY}},
Research-Areas = {{Computer Science}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence}},
Author-Email = {{Lilidong@163.com
   zqnwhut@163.com}},
Number-of-Cited-References = {{22}},
Times-Cited = {{3}},
Usage-Count-Last-180-days = {{2}},
Usage-Count-Since-2013 = {{39}},
Journal-ISO = {{Int. J. Pattern Recognit. Artif. Intell.}},
Doc-Delivery-Number = {{ES0OF}},
Unique-ID = {{ISI:000399225200012}},
DA = {{2019-10-28}},
}

@article{ ISI:000404275500024,
Author = {Bekele, E. and Bian, D. and Peterman, J. and Park, S. and Sarkar, N.},
Title = {{Design of a Virtual Reality System for Affect Analysis in Facial
   Expressions (VR-SAAFE); Application to Schizophrenia}},
Journal = {{IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING}},
Year = {{2017}},
Volume = {{25}},
Number = {{6}},
Pages = {{739-749}},
Month = {{JUN}},
Abstract = {{Schizophrenia is a life-long, debilitating psychotic disorder with poor
   outcome that affects about 1\% of the population. Although
   pharmacotherapy can alleviate some of the acute psychotic symptoms,
   residual social impairments present a significant barrier that prevents
   successful rehabilitation. With limited resources and access to social
   skills training opportunities, innovative technology has emerged as a
   potentially powerful tool for intervention. In this paper, we present a
   novel virtual reality (VR)-based system for understanding facial emotion
   processing impairments that may lead to poor social outcome in
   schizophrenia. We henceforth call it a VR System for Affect Analysis in
   Facial Expressions (VR-SAAFE). This system integrates a VR-based task
   presentation platform that can minutely control facial expressions of an
   avatar with or without accompanying verbal interaction, with an
   eye-tracker to quantitatively measure a participants real-time gaze and
   a set of physiological sensors to infer his/her affective states to
   allow in-depth understanding of the emotion recognition mechanism of
   patients with schizophrenia based on quantitative metrics. A usability
   study with 12 patients with schizophrenia and 12 healthy controls was
   conducted to examine processing of the emotional faces. Preliminary
   results indicated that there were significant differences in the way
   patients with schizophrenia processed and responded towards the
   emotional faces presented in the VR environment compared with healthy
   control participants. The preliminary results underscore the utility of
   such a VR-based system that enables precise and quantitative assessment
   of social skill deficits in patients with schizophrenia.}},
Publisher = {{IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC}},
Address = {{445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Bekele, E (Reprint Author), Vanderbilt Univ, Dept Elect \& Comp Engn, Nashville, TN 37212 USA.
   Bekele, E.; Bian, D., Vanderbilt Univ, Dept Elect \& Comp Engn, Nashville, TN 37212 USA.
   Peterman, J.; Park, S., Vanderbilt Univ, Dept Psychol, Nashville, TN 37212 USA.
   Sarkar, N., Vanderbilt Univ, Dept Mech Engn \& Comp Engn, Nashville, TN 37212 USA.}},
DOI = {{10.1109/TNSRE.2016.2591556}},
ISSN = {{1534-4320}},
EISSN = {{1558-0210}},
Keywords = {{Affective computing; eye gaze; physiology; schizophrenia therapy;
   virtual reality (VR)}},
Keywords-Plus = {{EMOTIONAL RESPONSE; VISUAL SCANPATHS; PATIENTS SHOW; RECOGNITION;
   EXPERIENCE; PICTURES; CHILDREN; DEFICIT}},
Research-Areas = {{Engineering; Rehabilitation}},
Web-of-Science-Categories  = {{Engineering, Biomedical; Rehabilitation}},
Author-Email = {{esubalew.bekele@vanderbilt.edu}},
ORCID-Numbers = {{Bekele, Esube/0000-0002-3306-4583
   Park, Sohee/0000-0003-3797-4776}},
Funding-Acknowledgement = {{Brain and Behavior Research Foundation; National Science
   FoundationNational Science Foundation (NSF) {[}0967170]; National
   Institutes of HealthUnited States Department of Health \& Human
   ServicesNational Institutes of Health (NIH) - USA {[}R01-MH091102,
   T32-MH18921, 1R21MH106748-01A1]; National Institute of Child Health and
   Human DevelopmentUnited States Department of Health \& Human
   ServicesNational Institutes of Health (NIH) - USANIH Eunice Kennedy
   Shriver National Institute of Child Health \& Human Development (NICHD)
   {[}P30-HD15052]}},
Funding-Text = {{This work was supported in part by the Brain and Behavior Research
   Foundation (Formerly NARSAD), the National Science Foundation under
   Grant 0967170, the National Institutes of Health under Grant
   R01-MH091102 and Grant T32-MH18921, the National Institute of Child
   Health and Human Development under Grant P30-HD15052, and the National
   Institutes of Health under Grant 1R21MH106748-01A1.}},
Number-of-Cited-References = {{58}},
Times-Cited = {{3}},
Usage-Count-Last-180-days = {{4}},
Usage-Count-Since-2013 = {{29}},
Journal-ISO = {{IEEE Trans. Neural Syst. Rehabil. Eng.}},
Doc-Delivery-Number = {{EY8VE}},
Unique-ID = {{ISI:000404275500024}},
OA = {{Bronze, Green Accepted}},
DA = {{2019-10-28}},
}

@article{ ISI:000398427100013,
Author = {Chen, Xiu and Chen, Zhenzhong},
Title = {{Exploring visual attention using random walks based eye tracking
   protocols}},
Journal = {{JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION}},
Year = {{2017}},
Volume = {{45}},
Pages = {{147-155}},
Month = {{MAY}},
Abstract = {{Identifying visual attention plays an important role in understanding
   human behavior and optimizing relevant multimedia applications. In this
   paper, we propose a visual attention identification method based on
   random walks. In the proposed method, fixations recorded by the eye
   tracker are partitioned into clusters where each cluster presents a
   particular area of interest (AoI). In each cluster, we estimate the
   transition probabilities of the fixations based on their point-to-point
   adjacency in their spatial positions. We obtain the initial coefficients
   for the fixations according to their density. We utilizing random walks
   to iteratively update the coefficients until their convergency. Finally,
   the center of the AOI is calculated according to the convergent
   coefficients of the fixations. Experimental results demonstrate that our
   proposed method which combines the fixations' spatial and temporal
   relations, highlights the fixations of higher densities and eliminates
   the errors inside the cluster. It is more robust and accurate than
   traditional methods.(C) 2017 Published by Elsevier Inc.}},
Publisher = {{ACADEMIC PRESS INC ELSEVIER SCIENCE}},
Address = {{525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Chen, ZZ (Reprint Author), Wuhan Univ, Sch Remote Sensing \& Informat Engn, Luoyu Rd 129, Wuhan 430079, Peoples R China.
   Chen, Xiu; Chen, Zhenzhong, Wuhan Univ, Sch Remote Sensing \& Informat Engn, Luoyu Rd 129, Wuhan 430079, Peoples R China.}},
DOI = {{10.1016/j.jvcir.2017.02.005}},
ISSN = {{1047-3203}},
EISSN = {{1095-9076}},
Keywords = {{Eye tracking; Visual attention; Fixation; Area of interest; Random walks}},
Keywords-Plus = {{SALIENCY DETECTION MODEL; BIT ALLOCATION; VIDEO; SENSITIVITY; IMAGE;
   ALGORITHMS}},
Research-Areas = {{Computer Science}},
Web-of-Science-Categories  = {{Computer Science, Information Systems; Computer Science, Software
   Engineering}},
Author-Email = {{zzchen@whu.edu.cn}},
Funding-Acknowledgement = {{National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China {[}61471273, 91438203]; National Hightech R\&D
   Program of China (863 Program)National High Technology Research and
   Development Program of China {[}2015AA015903]; Natural Science
   Foundation of Hubei Province of ChinaNatural Science Foundation of Hubei
   Province {[}2015CFA053]}},
Funding-Text = {{This work was supported in part by National Natural Science Foundation
   of China (Nos. 61471273, 91438203), National Hightech R\&D Program of
   China (863 Program, 2015AA015903), and Natural Science Foundation of
   Hubei Province of China (No. 2015CFA053).}},
Number-of-Cited-References = {{37}},
Times-Cited = {{3}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{12}},
Journal-ISO = {{J. Vis. Commun. Image Represent.}},
Doc-Delivery-Number = {{EQ9TC}},
Unique-ID = {{ISI:000398427100013}},
DA = {{2019-10-28}},
}

@article{ ISI:000400743500007,
Author = {Choi, Sungmook},
Title = {{Processing and learning of enhanced English collocations: An eye
   movement study}},
Journal = {{LANGUAGE TEACHING RESEARCH}},
Year = {{2017}},
Volume = {{21}},
Number = {{3, SI}},
Pages = {{403-426}},
Month = {{MAY}},
Abstract = {{Research to date suggests that textual enhancement may positively affect
   the learning of multiword combinations known as collocations, but may
   impair recall of unenhanced text. However, the attentional mechanisms
   underlying such effects remain unclear. In this study, 38 undergraduate
   students were divided into two groups: one read a text containing
   typographically enhanced collocations (ET group) and the other read the
   same text with unenhanced collocations (the baseline text, or BT group).
   While reading, participants' eye movements were recorded with an
   eye-tracker. Results showed that the ET group spent significantly longer
   time processing target collocations, and performed better than the BT
   group in a post-reading collocation test. However, apart from the
   enhanced collocations, the ET group recalled significantly less
   unenhanced text than the BT group. Further investigation of eye fixation
   data showed that the ET group spent substantially longer time processing
   collocations which, according to a pretest, they were not familiar with
   than did the BT group, whereas the two groups did not differ
   significantly in their processing of familiar collocations.
   Collectively, the results suggest that the trade-off between collocation
   learning and recall of unenhanced text is due to additional cognitive
   resources being allocated to enhanced collocations that are new to the
   reader.}},
Publisher = {{SAGE PUBLICATIONS LTD}},
Address = {{1 OLIVERS YARD, 55 CITY ROAD, LONDON EC1Y 1SP, ENGLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Choi, S (Reprint Author), Kyungpook Natl Univ, Dept English Educ, 80 Daehakro, Daegu 702701, South Korea.
   Choi, Sungmook, Kyungpook Natl Univ, Dept English Educ, 80 Daehakro, Daegu 702701, South Korea.}},
DOI = {{10.1177/1362168816653271}},
ISSN = {{1362-1688}},
EISSN = {{1477-0954}},
Keywords = {{collocation learning; eye fixation; recall of textual information;
   textual enhancement; vocabulary acquisition}},
Keywords-Plus = {{FORMULAIC SEQUENCES; INPUT ENHANCEMENT; LEARNERS; COMPREHENSION;
   ATTENTION; GRAMMAR; INSTRUCTION; ACQUISITION; RETENTION; EXPLICIT}},
Research-Areas = {{Education \& Educational Research; Linguistics}},
Web-of-Science-Categories  = {{Education \& Educational Research; Linguistics}},
Author-Email = {{sungmookchoi@knu.ac.kr}},
Funding-Acknowledgement = {{Ministry of Education of the Republic of Korea; National Research
   Foundation of KoreaNational Research Foundation of Korea
   {[}NRF-2015S1A5A2A01010323]}},
Funding-Text = {{The author(s) disclosed receipt of the following financial support for
   the research, authorship, and/or publication of this article: This work
   was supported by the Ministry of Education of the Republic of Korea and
   the National Research Foundation of Korea (NRF-2015S1A5A2A01010323).}},
Number-of-Cited-References = {{79}},
Times-Cited = {{9}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{25}},
Journal-ISO = {{Lang. Teach Res.}},
Doc-Delivery-Number = {{EU1AD}},
Unique-ID = {{ISI:000400743500007}},
DA = {{2019-10-28}},
}

@article{ ISI:000397354200009,
Author = {Starke, Sandra D. and Baber, Chris and Cooke, Neil J. and Howes, Andrew},
Title = {{Workflows and individual differences during visually guided routine
   tasks in a road traffic management control room}},
Journal = {{APPLIED ERGONOMICS}},
Year = {{2017}},
Volume = {{61}},
Pages = {{79-89}},
Month = {{MAY}},
Abstract = {{Road traffic control rooms rely on human operators to monitor and
   interact with information presented on multiple displays. Past studies
   have found inconsistent use of available visual information sources in
   such settings across different domains. In this study, we aimed to
   broaden the understanding of observer behaviour in control rooms by
   analysing a case study in road traffic control. We conducted a field
   study in a live road traffic control room where five operators responded
   to incidents while wearing a mobile eye tracker. Using qualitative and
   quantitative approaches, we investigated the operators' workflow using
   ergonomics methods and quantified visual information sampling. We found
   that individuals showed differing preferences for viewing modalities and
   weighting of task components, with a strong coupling between eye and
   head movement. For the quantitative analysis of the eye tracking data,
   we propose a number of metrics which may prove useful to compare visual
   sampling behaviour across domains in future. (C) 2017 The Authors.
   Published by Elsevier Ltd.}},
Publisher = {{ELSEVIER SCI LTD}},
Address = {{THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Starke, SD (Reprint Author), Univ Birmingham, Sch Engn, Birmingham B15 2TT, W Midlands, England.
   Starke, Sandra D.; Baber, Chris; Cooke, Neil J., Univ Birmingham, Sch Engn, Birmingham B15 2TT, W Midlands, England.
   Howes, Andrew, Univ Birmingham, Sch Comp Sci, Birmingham B15 2TT, W Midlands, England.}},
DOI = {{10.1016/j.apergo.2017.01.006}},
ISSN = {{0003-6870}},
EISSN = {{1872-9126}},
Keywords = {{Control room operations; Eye tracking; Information search}},
Keywords-Plus = {{EYE-HEAD COORDINATION; GAZE; FAULT; STRATEGIES; MOVEMENTS; SEARCH}},
Research-Areas = {{Engineering; Psychology}},
Web-of-Science-Categories  = {{Engineering, Industrial; Ergonomics; Psychology, Applied}},
Author-Email = {{s.d.Starke@bham.ac.uk}},
ORCID-Numbers = {{Howes, Andrew/0000-0003-4251-1127
   Baber, Chris/0000-0002-1830-2272}},
Funding-Acknowledgement = {{European Union FP7 project SPEEDD {[}619435]}},
Funding-Text = {{The authors would like to thank Philippe Coutard and the staff of DIR
   Centre-Est for making this study possible and Natan Sorin Morar for
   proof-reading of, and comments on, an initial version of the paper. This
   work was part funded European Union FP7 project SPEEDD (619435).}},
Number-of-Cited-References = {{61}},
Times-Cited = {{3}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{17}},
Journal-ISO = {{Appl. Ergon.}},
Doc-Delivery-Number = {{EP4MI}},
Unique-ID = {{ISI:000397354200009}},
OA = {{Green Published, Other Gold}},
DA = {{2019-10-28}},
}

@article{ ISI:000395538200005,
Author = {Gomez, Steven R. and Jianu, Radu and Cabeen, Ryan and Guo, Hua and
   Laidlaw, David H.},
Title = {{Fauxvea: Crowdsourcing Gaze Location Estimates for Visualization
   Analysis Tasks}},
Journal = {{IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS}},
Year = {{2017}},
Volume = {{23}},
Number = {{2}},
Pages = {{1042-1055}},
Month = {{FEB}},
Abstract = {{We present the design and evaluation of a method for estimating gaze
   locations during the analysis of static visualizations using
   crowdsourcing. Understanding gaze patterns is helpful for evaluating
   visualizations and user behaviors, but traditional eye-tracking studies
   require specialized hardware and local users. To avoid these
   constraints, we developed a method called Fauxvea, which crowdsources
   visualization tasks on the Web and estimates gaze fixations through
   cursor interactions without eye-tracking hardware. We ran experiments to
   evaluate how gaze estimates from our method compare with eye-tracking
   data. First, we evaluated crowdsourced estimates for three common types
   of information visualizations and basic visualization tasks using Amazon
   Mechanical Turk (MTurk). In another, we reproduced findings from a
   previous eye-tracking study on tree layouts using our method on MTurk.
   Results from these experiments show that fixation estimates using
   Fauxvea are qualitatively and quantitatively similar to eye tracking on
   the same stimulus-task pairs. These findings suggest that crowdsourcing
   visual analysis tasks with static information visualizations could be a
   viable alternative to traditional eye-tracking studies for visualization
   research and design.}},
Publisher = {{IEEE COMPUTER SOC}},
Address = {{10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Gomez, SR (Reprint Author), Brown Univ, Dept Comp Sci, Providence, RI 02912 USA.
   Gomez, Steven R.; Cabeen, Ryan; Guo, Hua; Laidlaw, David H., Brown Univ, Dept Comp Sci, Providence, RI 02912 USA.
   Jianu, Radu, Florida Int Univ, Sch Comp \& Informat Sci, Miami, FL 33199 USA.}},
DOI = {{10.1109/TVCG.2016.2532331}},
ISSN = {{1077-2626}},
EISSN = {{1941-0506}},
Keywords = {{Eye tracking; crowdsourcing; focus window; information visualization;
   visual analysis; user studies}},
Keywords-Plus = {{RESTRICTED FOCUS VIEWER; TRACKING; ATTENTION}},
Research-Areas = {{Computer Science}},
Web-of-Science-Categories  = {{Computer Science, Software Engineering}},
Author-Email = {{steveg@cs.brown.edu
   rdjianu@cis.fiu.edu
   cabeen@cs.brown.edu
   huag@cs.brown.edu
   dhl@cs.brown.edu}},
ResearcherID-Numbers = {{Laidlaw, David/M-4686-2019}},
Funding-Acknowledgement = {{US National Science Foundation (NSF)National Science Foundation (NSF)
   {[}IIS-10-16623]}},
Funding-Text = {{This work was supported in part by US National Science Foundation (NSF)
   award IIS-10-16623. All opinions, findings, conclusions, or
   recommendations expressed in this document are those of the authors and
   do not necessarily reflect the views of the sponsoring agencies.}},
Number-of-Cited-References = {{31}},
Times-Cited = {{2}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{6}},
Journal-ISO = {{IEEE Trans. Vis. Comput. Graph.}},
Doc-Delivery-Number = {{EM8CG}},
Unique-ID = {{ISI:000395538200005}},
OA = {{Green Accepted, Bronze}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000452171300044,
Author = {Sandhu, Vedant and Wai, Aung Aung Phyo and Ho, Chih Ying},
Editor = {{Dong, M and Wang, L and Lu, Y and Li, H}},
Title = {{Evaluation of Learning Performance by Quantifying User's Engagement
   Investigation through Low-Cost Multi-Modal sensors}},
Booktitle = {{PROCEEDINGS OF THE 2017 INTERNATIONAL CONFERENCE ON ORANGE TECHNOLOGIES
   (ICOT)}},
Series = {{International Conference on Orange Technologies}},
Year = {{2017}},
Pages = {{180-183}},
Note = {{5th International Conference on Orange Technologies (ICOT), Natl Univ
   Singapore, Kent Ridge Campus, Singapore, SINGAPORE, DEC 08-10, 2017}},
Organization = {{COLiPS; IEEE; IEEE Singapore SMC Chapter; Singapore Tourism Board; Asia
   Pacific Signal \& Informat Proc Assoc}},
Abstract = {{Although new forms of learning methods emerge embracing digital
   technologies, there is still no solution to objectively assess student's
   engagement, something pertinent to learning performance. Besides the
   traditional class questionnaire and exam, measuring attention or
   engagement using sensors, in real-time, is quickly growing interest.
   This paper investigates how multimodal sensors attributes to quantify
   engagement levels through a set of learning experiments. We conducted
   experiments with 10 high school students who participated in different
   activities that lasted about one hour, comprising of a 2-phase
   experiment. Phase 1 involved collecting training data for the
   classifier. While phase 2 required participants to complete two reading
   comprehension tests with passages they liked and disliked, simulating an
   e-Learning experience. We use commercial low-cost sensors such as EEG
   headband, desktop eye tracker, PPG and GSR sensors to collect multimodal
   data. Different features from different sensors were extracted and
   labelled using our experiment design and tasks measuring reaction time.
   Accuracies upwards of 90\% were achieved while classifying the EEG data
   into 3-class engagement levels. We, thus, suggest leveraging multimodal
   sensors to quantify multi-dimensional indexes such as engagement,
   emotion etc., for real-time assessment of learning performance. We are
   hoping that our work paves ways for assessing learn performance in a
   multi-faceted criteria, encompassing different neural, physiological and
   psychological states}},
Publisher = {{IEEE}},
Address = {{345 E 47TH ST, NEW YORK, NY 10017 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Sandhu, V (Reprint Author), Meridian Jr Coll, Singapore, Singapore.
   Sandhu, Vedant, Meridian Jr Coll, Singapore, Singapore.
   Wai, Aung Aung Phyo, Nanyang Technol Univ, CIL Lab SCSE, Nanyang Ave, Singapore, Singapore.
   Ho, Chih Ying, Raffles Inst, Singapore, Singapore.}},
ISSN = {{2373-5376}},
ISBN = {{978-1-5386-3276-5}},
Keywords = {{Multimodal; Brain Computer Interface; Engagement Index; holistic}},
Research-Areas = {{Engineering}},
Web-of-Science-Categories  = {{Engineering, Electrical \& Electronic}},
Author-Email = {{sandhuvedant@gmail.com
   apwaung@ntu.edu.sg
   hochihying1999@gmail.com}},
Number-of-Cited-References = {{11}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Doc-Delivery-Number = {{BL5MN}},
Unique-ID = {{ISI:000452171300044}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000431848700057,
Author = {Okazaki, Yasuhisa and Yoshikawa, Atsushi},
Editor = {{Chen, W and Yang, JC and Ayub, AFM and Wong, SL and Mitrovic, A}},
Title = {{Cognitive Investigation of Dynamic Educational Presentation toward
   Better Utilization of Presentation Characteristics}},
Booktitle = {{25TH INTERNATIONAL CONFERENCE ON COMPUTERS IN EDUCATION (ICCE 2017):
   TECHNOLOGY AND INNOVATION: COMPUTER-BASED EDUCATIONAL SYSTEMS FOR THE
   21ST CENTURY}},
Year = {{2017}},
Pages = {{355-360}},
Note = {{25th International Conference on Computers in Education (ICCE) -
   Technology and Innovation - Computer-Based Educational Systems for the
   21st Century, Christchurch, NEW ZEALAND, DEC 04-08, 2017}},
Abstract = {{Through two cognitive experiments, we investigated fundamental
   characteristics of presentation methods for teaching materials. We
   conducted a subjective evaluation by questionnaire and an objective
   evaluation by comprehension test, as well as gaze analysis with an eye
   tracker to analyze differences between three cases: when learners were
   shown a handwriting process such as on a chalkboard, animated slides
   using presentation software, and only final written results. Results of
   the experiments confirmed the basic characteristics of each presentation
   as visual information and provided evidence showing advantages of
   showing the writing process under certain conditions. We also confirmed
   an induction effect by voice description as auditory information. These
   results contribute to the use of various presentation characteristics
   for maximizing the effect of teaching material presentation.}},
Publisher = {{ASIA PACIFIC SOC COMPUTERS IN EDUCATION}},
Address = {{NO 300, JUNGDA RD, JHONGLI DISTRICT, TAOYUAN CITY, 320, TAIWAN}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Okazaki, Y (Reprint Author), Saga Univ, Grad Sch Sci \& Engn, Saga, Japan.
   Okazaki, Yasuhisa, Saga Univ, Grad Sch Sci \& Engn, Saga, Japan.
   Yoshikawa, Atsushi, Tokyo Inst Technol, Interdisciplinary Grad Sch Sci \& Engn, Tokyo, Japan.}},
ISBN = {{978-986-94012-6-5}},
Keywords = {{gaze; eye tracker; writing process; chalkboard; slide}},
Research-Areas = {{Computer Science; Education \& Educational Research}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications; Computer Science, Theory \& Methods;
   Education \& Educational Research}},
Author-Email = {{okaz@cc.saga-u.ac.jp}},
Funding-Acknowledgement = {{JSPS KAKENHIMinistry of Education, Culture, Sports, Science and
   Technology, Japan (MEXT)Japan Society for the Promotion of
   ScienceGrants-in-Aid for Scientific Research (KAKENHI) {[}16K01117]}},
Funding-Text = {{This work was supported by JSPS KAKENHI (Grant-in-Aid for Scientific
   Research(C)) Grant Number 16K01117. We would like to thank Prof. Takaaki
   Sonoda and Ms. Noriko Nakashima at Integrated Center for Educational
   Research and Development of the Faculty of Education in Saga University
   for their kind support and cooperation in this research. We also thank
   to Mr. Kouhei Nishimura, Mr. Kenta Tashiro and all others who cooperated
   in our experiment.}},
Number-of-Cited-References = {{14}},
Times-Cited = {{1}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Doc-Delivery-Number = {{BK1KC}},
Unique-ID = {{ISI:000431848700057}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000427670800010,
Author = {Gunawan, Fergyanto E. and Wijaya, Oky and Soewito, Benfano and Candra,
   Sevenpri and Diana and Suharyanto, Cosmas E. and Sekishita, Nobumasa},
Editor = {{Riyadi, MA and Facta, M and Stiawan, D and Rahmawan, H}},
Title = {{An Analysis of Concentration Region on Powerpoint Slides using Eye
   Tracking}},
Booktitle = {{2017 4TH INTERNATIONAL CONFERENCE ON ELECTRICAL ENGINEERING, COMPUTER
   SCIENCE AND INFORMATICS (EECSI)}},
Year = {{2017}},
Pages = {{44-48}},
Note = {{4th International Conference on Electrical Engineering, Computer Science
   and Informatics (EECSI), Univ Ahmad Dahlan, Yogyakarta, INDONESIA, SEP
   19-21, 2017}},
Organization = {{IEEE Indonesia Sect; Inst Adv Engn \& Sci; Univ Diponegoro; Univ Gajah
   Mada; Univ Islam Sultan Agung; Univ Sriwijaya; Univ Teknologi Malaysia;
   Univ Muhammadiyah Malang}},
Abstract = {{Powerpoint slides have become one of the essential teaching tools in
   academic for both offline and online modes. It may play a useful role to
   facilitate discussion and information exchange. However, in our teaching
   experience, we find many students utilizing Powerpoint slides beyond
   their traditional functions. Many students fully rely on the slides as
   the main learning materials and, in some cases, substituting textbooks.
   This study intends to understand how students interact with the learning
   materials presented on Powerpoint slides. The interaction is measured
   using an eye tracker device called the Eye Tribe Tracker. Thirty
   sophomore and junior students are asked to participate. They are
   instructed to learn a topic in the subject of Introduction to Algorithm
   and Programming, a basic course in the computer science field. During
   the process, their fixation points are monitored and are related to the
   contents on the slides. The results are rather surprising. Many students
   read the slides in unexpected manners that may compromise their
   understanding and may lead to inaccurate interpretations.}},
Publisher = {{IEEE}},
Address = {{345 E 47TH ST, NEW YORK, NY 10017 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Gunawan, FE (Reprint Author), Bina Nusantara Univ, Ind Engn Dept, BINUS Grad Program, Jakarta 11530, Indonesia.
   Gunawan, Fergyanto E., Bina Nusantara Univ, Ind Engn Dept, BINUS Grad Program, Jakarta 11530, Indonesia.
   Wijaya, Oky; Soewito, Benfano; Diana, Bina Nusantara Univ, Comp Sci Dept, BINUS Grad Programs, Jakarta 11530, Indonesia.
   Candra, Sevenpri, Bina Nusantara Univ, BINUS Business Sch, Management Dept, Undergrad Program, Jakarta 11480, Indonesia.
   Suharyanto, Cosmas E., Putera Batam Univ, Comp Sci Dept, Riau Archipelago 29433, Indonesia.
   Sekishita, Nobumasa, Toyohashi Univ Technol, Dept Mech Engn, Toyohashi, Aichi 4418580, Japan.}},
ISBN = {{978-1-5386-0549-3}},
Keywords-Plus = {{VISUAL-ATTENTION; STUDENTS; EDUCATION}},
Research-Areas = {{Computer Science; Engineering}},
Web-of-Science-Categories  = {{Computer Science, Information Systems; Computer Science, Theory \&
   Methods; Engineering, Electrical \& Electronic}},
Author-Email = {{fgunawan@binus.edu}},
Number-of-Cited-References = {{15}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{4}},
Doc-Delivery-Number = {{BJ7TS}},
Unique-ID = {{ISI:000427670800010}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000428139100005,
Author = {Sanches, Charles Lima and Augereau, Olivier and Kise, Koichi},
Book-Group-Author = {{IEEE}},
Title = {{Using the eye gaze to predict document reading subjective understanding}},
Booktitle = {{2017 14TH IAPR INTERNATIONAL CONFERENCE ON DOCUMENT ANALYSIS AND
   RECOGNITION (ICDAR 2017), VOL 8}},
Series = {{Proceedings of the International Conference on Document Analysis and
   Recognition}},
Year = {{2017}},
Pages = {{28-31}},
Note = {{14th IAPR International Conference on Document Analysis and Recognition
   (ICDAR), Kyoto, JAPAN, NOV 09-15, 2017}},
Organization = {{IAPR}},
Abstract = {{The traditional way to analyze the content of a document is to perform
   document image analysis. However, analyzing how the user perceives a
   document is another way to get information about the documents but also
   about the users. By using sensors such as eye tracker, it is possible
   analyze the reader's skill or the document comprehensibility. In this
   paper, we focus on predicting the user's understanding as it can be used
   as a feedback either for the author (to improve his document) or the
   user (to review the parts he did not understand).
   The eye movements of the readers are recorded by an eye tracker while
   they read several documents, then several features are extracted and a
   support vector regression system is used to predict the readers'
   understanding. In our experiment, 17 subjects were asked to read 19
   documents for a total of 323 recordings. As a first result, we prove
   that the subjective understanding of the reader can be predicted more
   accurately by using the eye gaze than by asking a multiple choice
   question.}},
Publisher = {{IEEE}},
Address = {{345 E 47TH ST, NEW YORK, NY 10017 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Sanches, CL (Reprint Author), Osaka Prefecture Univ, Naka Ku, 1-1 Gakuen Cho, Osaka 5998531, Japan.
   Sanches, Charles Lima; Augereau, Olivier; Kise, Koichi, Osaka Prefecture Univ, Naka Ku, 1-1 Gakuen Cho, Osaka 5998531, Japan.}},
DOI = {{10.1109/ICDAR.2017.377}},
ISSN = {{1520-5363}},
ISBN = {{978-1-5386-3586-5}},
Keywords-Plus = {{MOVEMENTS}},
Research-Areas = {{Computer Science; Engineering}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Engineering, Electrical \&
   Electronic}},
Author-Email = {{charles-ls@hotmail.fr
   augereau.o@gmail.com
   kise@cs.osakafu-u.ac.jp}},
Funding-Acknowledgement = {{JST CRESTJapan Science \& Technology Agency (JST)Core Research for
   Evolutional Science and Technology (CREST) {[}JP-MJCR16E1]; JSPSMinistry
   of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan
   Society for the Promotion of Science {[}15K12172]; JSPS KAKENHIMinistry
   of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan
   Society for the Promotion of ScienceGrants-in-Aid for Scientific
   Research (KAKENHI) {[}16K16089]; Key Project Grant Program of Osaka
   Prefecture University}},
Funding-Text = {{This work is in part supported by JST CREST (JP-MJCR16E1), JSPS
   Grant-in-Aid for Scientific Research (15K12172), JSPS KAKENHI Grant
   Number (16K16089) and the Key Project Grant Program of Osaka Prefecture
   University.}},
Number-of-Cited-References = {{19}},
Times-Cited = {{1}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Doc-Delivery-Number = {{BJ8EL}},
Unique-ID = {{ISI:000428139100005}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000428139100006,
Author = {Ishimaru, Shoya and Jacob, Soumy and Roy, Apurba and Bukhari, Syed Saqib
   and Heisel, Carina and Grossmann, Nicolas and Thees, Michael and Kuhn,
   Jochen and Dengel, Andreas},
Book-Group-Author = {{IEEE}},
Title = {{Cognitive State Measurement on Learning Materials by Utilizing Eye
   Tracker and Thermal Camera}},
Booktitle = {{2017 14TH IAPR INTERNATIONAL CONFERENCE ON DOCUMENT ANALYSIS AND
   RECOGNITION (ICDAR 2017), VOL 8}},
Series = {{Proceedings of the International Conference on Document Analysis and
   Recognition}},
Year = {{2017}},
Pages = {{32-36}},
Note = {{14th IAPR International Conference on Document Analysis and Recognition
   (ICDAR), Kyoto, JAPAN, NOV 09-15, 2017}},
Organization = {{IAPR}},
Abstract = {{We demonstrate how information derived from pervasive sensors can
   quantify cognitive states of learners while they are reading a textbook.
   Eye tracking is one of the most effective approaches to measuring
   reading behavior. For example, high fixation duration represents a
   reader's attention on a document. However, it is still a challenging
   task to predict the reason for the attention (i.e., is it because of
   his/her interest or trouble of understanding?). In this paper, we
   utilize additional sensing modality to solve the problem. On the dataset
   of 12 high school students' reading behaviors, we have found that the
   changing of pupil diameter and nose temperature are highly correlated
   with their cognitive states including their interests and efforts for
   reading/solving tasks on learning materials in Physics.}},
Publisher = {{IEEE}},
Address = {{345 E 47TH ST, NEW YORK, NY 10017 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Ishimaru, S (Reprint Author), German Res Ctr Artificial Intelligence DFKI, Kaiserslautern, Germany.
   Ishimaru, S (Reprint Author), Univ Kaiserslautern, Kaiserslautern, Germany.
   Ishimaru, Shoya; Bukhari, Syed Saqib; Dengel, Andreas, German Res Ctr Artificial Intelligence DFKI, Kaiserslautern, Germany.
   Ishimaru, Shoya; Jacob, Soumy; Roy, Apurba; Bukhari, Syed Saqib; Heisel, Carina; Grossmann, Nicolas; Thees, Michael; Kuhn, Jochen; Dengel, Andreas, Univ Kaiserslautern, Kaiserslautern, Germany.}},
DOI = {{10.1109/ICDAR.2017.378}},
ISSN = {{1520-5363}},
ISBN = {{978-1-5386-3586-5}},
Keywords-Plus = {{MOVEMENTS; CURIOSITY; LOAD}},
Research-Areas = {{Computer Science; Engineering}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Engineering, Electrical \&
   Electronic}},
Author-Email = {{shoya.ishimaru@dfki.de
   syedsaqib.bukhari@dfki.de
   andreas.dengel@dfki.de}},
ResearcherID-Numbers = {{Kuhn, Jochen/K-4031-2014}},
ORCID-Numbers = {{Kuhn, Jochen/0000-0002-6985-3218}},
Funding-Acknowledgement = {{Federal Ministry of Education and Research (BMBF) of the joint
   initiative ``Qualitatsoffensive Lehrerbildung{''} of the Federal and the
   Federal States of Germany {[}01JA1616]}},
Funding-Text = {{This work is funded by the Federal Ministry of Education and Research
   (BMBF) in the framework of the joint initiative ``Qualitatsoffensive
   Lehrerbildung{''} of the Federal and the Federal States of Germany for
   the project ``U.EDU: Unified Education{''} (support code: 01JA1616). The
   authors are responsible for the content of this contribution.}},
Number-of-Cited-References = {{29}},
Times-Cited = {{2}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{2}},
Doc-Delivery-Number = {{BJ8EL}},
Unique-ID = {{ISI:000428139100006}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000426932500031,
Author = {Sanches, Charles Lima and Augereau, Olivier and Kise, Koichi},
Book-Group-Author = {{ACM}},
Title = {{Japanese Reading Objective Understanding Estimation by Eye Gaze Analysis}},
Booktitle = {{PROCEEDINGS OF THE 2017 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE
   AND UBIQUITOUS COMPUTING AND PROCEEDINGS OF THE 2017 ACM INTERNATIONAL
   SYMPOSIUM ON WEARABLE COMPUTERS (UBICOMP/ISWC `17 ADJUNCT)}},
Year = {{2017}},
Pages = {{121-124}},
Note = {{ACM International Joint Conference on Pervasive and Ubiquitous Computing
   (UBICOMP) / ACM International Symposium on Wearable Computers (ISWC),
   Maui, HI, SEP 11-15, 2017}},
Organization = {{Assoc Comp Machinery; ACM SIGCHI; ACM SIGMOBILE}},
Abstract = {{Analyzing the eye gaze to estimate the text understanding is a good way
   to overcome the drawbacks of classic question based assessment tests. In
   particular it does not suffer from random answers, question
   misunderstanding and can cover every parts of the text. In this paper we
   propose a method to estimate the objective understanding of a learner by
   analyzing his eye movements while reading. We conduct our experiment on
   Japanese texts and try to predict, by analyzing the eye gaze, how many
   questions about the texts the reader will be able to answer to. We show
   that we obtain 5.27\% of error in the number of correct answers
   estimation by using eye gaze features. As a comparison, we try to
   predict the number of correct answers by using the reader's self
   assessment understanding and show that it leads to 9.04\% error.}},
Publisher = {{ASSOC COMPUTING MACHINERY}},
Address = {{1515 BROADWAY, NEW YORK, NY 10036-9998 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Sanches, CL (Reprint Author), Osaka Prefecture Univ, Naka Ku, 1-1 Gakuen Cho, Sakai, Osaka 5998531, Japan.
   Sanches, Charles Lima; Augereau, Olivier; Kise, Koichi, Osaka Prefecture Univ, Naka Ku, 1-1 Gakuen Cho, Sakai, Osaka 5998531, Japan.}},
DOI = {{10.1145/3123024.3123092}},
ISBN = {{978-1-4503-5190-4}},
Keywords = {{Eye tracker; Gaze analysis; Reading understanding}},
Research-Areas = {{Computer Science; Telecommunications}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Computer Science,
   Cybernetics; Telecommunications}},
Author-Email = {{charles-ls@hotmail.fr
   augereau.o@gmail.fr
   kise@cs.osakafu-u.ac.jp}},
Funding-Acknowledgement = {{JST CRESTJapan Science \& Technology Agency (JST)Core Research for
   Evolutional Science and Technology (CREST) {[}JPMJCR16E1]; JSPSMinistry
   of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan
   Society for the Promotion of Science {[}15K12172]; JSPS KAKENHIMinistry
   of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan
   Society for the Promotion of ScienceGrants-in-Aid for Scientific
   Research (KAKENHI) {[}16K16089]; Key Project Grant Program of Osaka
   Prefecture University}},
Funding-Text = {{This work is in part supported by JST CREST (JPMJCR16E1), JSPS
   Grant-in-Aid for Scientific Research (15K12172), JSPS KAKENHI Grant
   Number (16K16089) and the Key Project Grant Program of Osaka Prefecture
   University.}},
Number-of-Cited-References = {{7}},
Times-Cited = {{1}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Doc-Delivery-Number = {{BJ6RT}},
Unique-ID = {{ISI:000426932500031}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000426932500055,
Author = {Yamada, Kento and Augereau, Olivier and Kise, Koichi},
Book-Group-Author = {{ACM}},
Title = {{Estimation of Confidence Based on Eye Gaze: an Application to
   Multiple-choice Questions}},
Booktitle = {{PROCEEDINGS OF THE 2017 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE
   AND UBIQUITOUS COMPUTING AND PROCEEDINGS OF THE 2017 ACM INTERNATIONAL
   SYMPOSIUM ON WEARABLE COMPUTERS (UBICOMP/ISWC `17 ADJUNCT)}},
Year = {{2017}},
Pages = {{217-220}},
Note = {{ACM International Joint Conference on Pervasive and Ubiquitous Computing
   (UBICOMP) / ACM International Symposium on Wearable Computers (ISWC),
   Maui, HI, SEP 11-15, 2017}},
Organization = {{Assoc Comp Machinery; ACM SIGCHI; ACM SIGMOBILE}},
Abstract = {{Multiple-choice questions are often used to assess the user's
   understanding. However, the system does not know if the user answered
   randomly, by chance, with confidence or without confidence. The user's
   confidence can be used as a feedback for teachers or smart interfaces
   that could understand that he is not sure of himself and then provide
   further assistance.
   We aim to create confidence aware interfaces such as pervasive display.
   As a first step, we propose an application in the context of
   multiple-choice question answering. By using an eye tracker, we analyze
   the reading and answering behavior of the user, extract some features
   and predict his confidence. We asked 11 participants to solve 80
   multiple-choice questions about English understanding, and estimated
   whether they answered confidently or not. We obtained 90.1\% average
   accuracy for this prediction.}},
Publisher = {{ASSOC COMPUTING MACHINERY}},
Address = {{1515 BROADWAY, NEW YORK, NY 10036-9998 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Yamada, K (Reprint Author), Osaka Prefecture Univ, Grad Sch Engn, Naka Ku, 1-1 Gakuen Cho, Sakai, Osaka, Japan.
   Yamada, Kento; Augereau, Olivier; Kise, Koichi, Osaka Prefecture Univ, Grad Sch Engn, Naka Ku, 1-1 Gakuen Cho, Sakai, Osaka, Japan.}},
DOI = {{10.1145/3123024.3123138}},
ISBN = {{978-1-4503-5190-4}},
Keywords = {{eye tracking; confidence estimation; study support; reading analysis;
   pervasive display}},
Research-Areas = {{Computer Science; Telecommunications}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Computer Science,
   Cybernetics; Telecommunications}},
Author-Email = {{kento@m.cs.osakafu-u.ac.jp
   augereau.o@gmail.com
   kise@cs.osakafu-u.ac.jp}},
Funding-Acknowledgement = {{JST CREST Grant {[}JPMJCR16E1]; JSPS KAKENHI GrantMinistry of Education,
   Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for
   the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)
   {[}25240028, 15K12172, 16K16089]}},
Funding-Text = {{This work is supported in part by JST CREST Grant Number JPMJCR16E1 and
   JSPS KAKENHI Grant Numbers 25240028, 15K12172 and 16K16089.}},
Number-of-Cited-References = {{8}},
Times-Cited = {{3}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{3}},
Doc-Delivery-Number = {{BJ6RT}},
Unique-ID = {{ISI:000426932500055}},
DA = {{2019-10-28}},
}

@article{ ISI:000415619600003,
Author = {Potocki, Anna and Ros, Christine and Vibert, Nicolas and Rouet,
   Jean-Francois},
Title = {{Children's Visual Scanning of Textual Documents: Effects of Document
   Organization, Search Goals, and Metatextual Knowledge}},
Journal = {{SCIENTIFIC STUDIES OF READING}},
Year = {{2017}},
Volume = {{21}},
Number = {{6}},
Pages = {{480-497}},
Abstract = {{This study examines children's strategies when scanning a document to
   answer a specific question. More specifically, we wanted to know whether
   they make use of organizers (i.e., headings) when searching and whether
   strategic search is related to their knowledge of reading strategies.
   Twenty-six French fifth graders were asked to search single-page
   documents presented on the screen of an eye tracker in order to respond
   to questions requiring the location of information in either a single
   paragraph (Location questions) or multiple paragraphs (Comparison
   questions). Location questions were easier and faster to answer than
   Comparison questions. The presence of headers led to more selective
   reading strategies but did not significantly speed up the search. Strong
   individual differences were observed in children's scanning strategies:
   Some systematically fixated headers, whereas others did not. These
   differences were not significantly related to the participants' decoding
   or comprehension skills but rather to their knowledge of reading
   strategies.}},
Publisher = {{ROUTLEDGE JOURNALS, TAYLOR \& FRANCIS LTD}},
Address = {{2-4 PARK SQUARE, MILTON PARK, ABINGDON OX14 4RN, OXON, ENGLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Potocki, A (Reprint Author), Univ Poitiers, CeRCA, UMR 7295, MSHS Bat A5,5 Rue Theodore Lefebvre,TSA 21103, F-86000 Poitiers, France.
   Ctr Rech Cognit \& Apprentissage, UMR 7295, Poitiers, France.
   Univ Poitiers, Poitiers, France.
   Univ Francois Rabelais Tours, Tours, France.
   CNRS, Paris, France.}},
DOI = {{10.1080/10888438.2017.1334060}},
ISSN = {{1088-8438}},
EISSN = {{1532-799X}},
Keywords-Plus = {{READING-COMPREHENSION; LOCATING INFORMATION; MODEL; DIFFICULTIES;
   ACQUISITION; STRATEGIES; PROFILES; ABILITY; TASKS; VIEW}},
Research-Areas = {{Education \& Educational Research; Psychology}},
Web-of-Science-Categories  = {{Education \& Educational Research; Psychology, Educational}},
Author-Email = {{anna.potocki@univ-poitiers.fr}},
Number-of-Cited-References = {{54}},
Times-Cited = {{2}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{6}},
Journal-ISO = {{Sci. Stud. Read.}},
Doc-Delivery-Number = {{FM9VK}},
Unique-ID = {{ISI:000415619600003}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000414246900004,
Author = {Melo, Jean and Narcizo, Fabricio Batista and Hansen, Dan Witzner and
   Brabrand, Claus and Wasowski, Andrzej},
Book-Group-Author = {{IEEE}},
Title = {{Variability through the Eyes of the Programmer}},
Booktitle = {{2017 IEEE/ACM 25TH INTERNATIONAL CONFERENCE ON PROGRAM COMPREHENSION
   (ICPC)}},
Series = {{International Conference on Program Comprehension}},
Year = {{2017}},
Pages = {{34-44}},
Note = {{25th IEEE/ACM International Conference on Program Comprehension (ICPC),
   Buenos Aires, ARGENTINA, MAY 22-23, 2017}},
Organization = {{IEEE; Assoc Comp Machinery; IEEE Comp Soc; IEEE Tech Council Software
   Engn; Special Interest Grp Software Engn; EXACTAS UBA; Soc Argentina
   Informatica; Sadosky Fundac}},
Abstract = {{Preprocessor directives (\#ifdefs) are often used to implement
   compile-time variability, despite the critique that they increase
   complexity, hamper maintainability, and impair code comprehensibility.
   Previous studies have shown that the time of bug finding increases
   linearly with variability. However, little is known about the cognitive
   process of debugging programs with variability. We carry out an
   experiment to understand how developers debug programs with variability.
   We ask developers to debug programs with and without variability, while
   recording their eye movements using an eye tracker. The results indicate
   that debugging time increases for code fragments containing variability.
   Interestingly, debugging time also seems to increase for code fragments
   without variability in the proximity of fragments that do contain
   variability. The presence of variability correlates with increase in the
   number of gaze transitions between definitions and usages for fields and
   methods. Variability also appears to prolong the ``initial scan{''} of
   the entire program that most developers initiate debugging with.}},
Publisher = {{IEEE}},
Address = {{345 E 47TH ST, NEW YORK, NY 10017 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Melo, J (Reprint Author), IT Univ Copenhagen, Copenhagen, Denmark.
   Melo, Jean; Narcizo, Fabricio Batista; Hansen, Dan Witzner; Brabrand, Claus; Wasowski, Andrzej, IT Univ Copenhagen, Copenhagen, Denmark.}},
DOI = {{10.1109/ICPC.2017.34}},
ISSN = {{1092-8138}},
ISBN = {{978-1-5386-0535-6}},
Keywords = {{Variability; Preprocessors; Debugging; Eye Tracking; Highly-Configurable
   Systems}},
Research-Areas = {{Computer Science}},
Web-of-Science-Categories  = {{Computer Science, Software Engineering}},
Author-Email = {{jeanmelo@itu.dk
   fabn@itu.dk
   witzner@itu.dk
   brabrand@itu.dk
   wasowski@itu.dk}},
ResearcherID-Numbers = {{Narcizo, Fabricio Batista/H-9426-2014}},
ORCID-Numbers = {{Narcizo, Fabricio Batista/0000-0003-1319-5153}},
Funding-Acknowledgement = {{Brazilian Science without Borders Programme, CNPq {[}249020/2013-0,
   229760/2013-9]; Danish Council for Independent Research, Sapere Aude
   {[}0602-02327B]}},
Funding-Text = {{We thank Thao for carrying out a pilot study in connection with her
   Master's Thesis. This work is supported by Brazilian Science without
   Borders Programme, CNPq grants no. 249020/2013-0 and no. 229760/2013-9.
   Wasowski is partially funded by The Danish Council for Independent
   Research, Sapere Aude grant no. 0602-02327B, VARIETE project.}},
Number-of-Cited-References = {{37}},
Times-Cited = {{5}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Doc-Delivery-Number = {{BI7GI}},
Unique-ID = {{ISI:000414246900004}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000403392300026,
Author = {Pappusetty, Deepti and Chinta, Vishnu Vardhan Reddy and Kalva, Hari},
Editor = {{Pescador, F and Lee, JH and Sanchez, DD}},
Title = {{Using Pupillary Response to Assess Video Quality}},
Booktitle = {{2017 IEEE INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS (ICCE)}},
Series = {{International Conference on Consumer Electronics}},
Year = {{2017}},
Note = {{IEEE International Conference on Consumer Electronics (ICCE), Las Vegas,
   NV, JAN 08-10, 2017}},
Organization = {{IEEE}},
Abstract = {{Pupil response can be measured non-intrusively using an eye tracker and
   offers a potentially new approach to understanding video structure and
   content. An analysis of pupil response to quality variations in a video
   is reported in this paper. Experiments were conducted under free viewing
   conditions and pupillary response of subjects was analyzed. Video clip
   encoded with AVC/H.264 at various qualities and durations were used to
   assess user response. Results show pupillary constrictions at points of
   quality transitions.}},
Publisher = {{IEEE}},
Address = {{345 E 47TH ST, NEW YORK, NY 10017 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Pappusetty, D (Reprint Author), Florida Atlantic Univ, Multimedia Lab, Boca Raton, FL 33431 USA.
   Pappusetty, Deepti; Chinta, Vishnu Vardhan Reddy; Kalva, Hari, Florida Atlantic Univ, Multimedia Lab, Boca Raton, FL 33431 USA.}},
ISBN = {{978-1-5090-5544-9}},
Research-Areas = {{Engineering}},
Web-of-Science-Categories  = {{Engineering, Electrical \& Electronic}},
Author-Email = {{dpappuse@fau.edu
   vchinta@fau.edu
   hkalva@fau.edu}},
Number-of-Cited-References = {{8}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Doc-Delivery-Number = {{BH8JB}},
Unique-ID = {{ISI:000403392300026}},
DA = {{2019-10-28}},
}

@article{ ISI:000396656700021,
Author = {Bi, Youyi and Reid, Tahira},
Title = {{Evaluating Students' Understanding of Statics Concepts Using Eye Gaze
   Data}},
Journal = {{INTERNATIONAL JOURNAL OF ENGINEERING EDUCATION}},
Year = {{2017}},
Volume = {{33}},
Number = {{1, A}},
Pages = {{225-235}},
Abstract = {{In engineering courses, exams and homework assignments are among the
   standard tools used to assess students' performance and comprehension of
   course material. However, they do not always provide opportunities to
   reveal whether students truly understand related engineering concepts.
   This paper seeks to bridge that research gap by using eye-tracking
   technology to observe how students solve statics problems. In a
   within-subject experiment, twenty participants were asked to solve nine
   statics problems shown on a computer display. A non-invasive eye-tracker
   was used to record participants' eye movements during the problem
   solving process. Participants were then asked to explain how they solved
   three representative problems. The results show that different eye gaze
   patterns exist between those who solved problems correctly and those who
   solved them incorrectly. For the specific concepts involved in solving
   these problems, those who correctly understood the concepts also
   exhibited different eye gaze patterns than those who did not. We also
   found that students' spatial visualization skills positively correlate
   with their performance when solving statics problems. This investigation
   showed that eye gaze data has the potential to serve as a diagnostic
   tool to discern how students solve statics problems and understand
   related engineering concepts. These results may provide insight into
   students' problem-solving strategies and difficulties, and help
   instructors choose more adaptive teaching methods for students.}},
Publisher = {{TEMPUS PUBLICATIONS}},
Address = {{IJEE , ROSSMORE,, DURRUS, BANTRY, COUNTY CORK 00000, IRELAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Bi, YY (Reprint Author), Purdue Univ, Sch Mech Engn, 585 Purdue Mall, W Lafayette, IN 47907 USA.
   Bi, Youyi; Reid, Tahira, Purdue Univ, Sch Mech Engn, 585 Purdue Mall, W Lafayette, IN 47907 USA.}},
ISSN = {{0949-149X}},
Keywords = {{statics; problem-solving; concept inventory; eye-tracking}},
Keywords-Plus = {{COMPUTER-BASED ASSESSMENT; CONCEPT INVENTORY; VISUAL-ATTENTION; VERBAL
   REPORTS; MOVEMENTS; PERFORMANCE; FIXATIONS; TRACKING; STRATEGIES;
   INSPECTION}},
Research-Areas = {{Education \& Educational Research; Engineering}},
Web-of-Science-Categories  = {{Education, Scientific Disciplines; Engineering, Multidisciplinary}},
Author-Email = {{bi9@purdue.edu}},
Number-of-Cited-References = {{66}},
Times-Cited = {{2}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{11}},
Journal-ISO = {{Int. J. Eng. Educ}},
Doc-Delivery-Number = {{EO4HT}},
Unique-ID = {{ISI:000396656700021}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000424196300007,
Author = {De Abreu, Ana and Ozcinar, Cagri and Smolic, Aljosa},
Book-Group-Author = {{IEEE}},
Title = {{Look Around You: Saliency Maps for Omnidirectional Images in VR
   Applications}},
Booktitle = {{2017 NINTH INTERNATIONAL CONFERENCE ON QUALITY OF MULTIMEDIA EXPERIENCE
   (QOMEX)}},
Series = {{International Workshop on Quality of Multimedia Experience}},
Year = {{2017}},
Note = {{9th International Conference on Quality of Multimedia Experience
   (QoMEX), Erfurt, GERMANY, MAY 29-JUN 02, 2017}},
Organization = {{You Tube; Netflix; Disney Res; Tech Univ Ilmenau; Tech Univ Berlin;
   Telekom; QUALINET; QoENET; IEEE; IEEE Signal Proc Soc}},
Abstract = {{Understanding visual attention has always been a topic of great interest
   in the graphics, image/video processing, robotics and human-computer
   interaction communities. By understanding salient image regions, the
   compression, transmission and rendering algorithms can be optimized.
   This is particularly important in omnidirectional images (ODIs) viewed
   with a head-mounted display (HMD), where only a fraction of the captured
   scene is displayed at a time, namely viewport. In order to predict
   salient image regions, saliency maps are estimated either by using an
   eye tracker to collect eye fixations during subjective tests or by using
   computational models of visual attention. However, eye tracking
   developments for ODIs are still in the early stages and although a large
   list of saliency models are available, no particular attention has been
   dedicated to ODIs. Therefore, in this paper, we consider the problem of
   estimating saliency maps for ODIs viewed with HMDs, when the use of an
   eye tracker device is not possible. We collected viewport center
   trajectories (VCTs) of 32 participants for 21 ODIs and propose a method
   to transform the gathered data into saliency maps. The obtained saliency
   maps are compared in terms of image exposition time used to display each
   ODI in the subjective tests. Then, motivated by the equator bias
   tendency in ODIs, we propose a post-processing method, namely fused
   saliency maps (FSM), to adapt current saliency models to ODIs
   requirements. We show that the use of FSM on current models improves
   their performance by up to 20\%. The developed database and testbed are
   publicly available with this paper.}},
Publisher = {{IEEE}},
Address = {{345 E 47TH ST, NEW YORK, NY 10017 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{De Abreu, A (Reprint Author), Trinity Coll Dublin, Dublin, Ireland.
   De Abreu, Ana; Ozcinar, Cagri; Smolic, Aljosa, Trinity Coll Dublin, Dublin, Ireland.}},
ISSN = {{2372-7179}},
EISSN = {{2472-7814}},
ISBN = {{978-1-5386-4024-1}},
Keywords = {{Fixations; head-mounted display (HMD); omnidirectional images (ODIs);
   saliency maps; viewport; virtual reality (VR)}},
Keywords-Plus = {{VISUAL-ATTENTION}},
Research-Areas = {{Computer Science; Engineering}},
Web-of-Science-Categories  = {{Computer Science, Information Systems; Computer Science, Software
   Engineering; Engineering, Electrical \& Electronic}},
ORCID-Numbers = {{Ozcinar, Cagri/0000-0003-4915-2251}},
Number-of-Cited-References = {{21}},
Times-Cited = {{2}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Doc-Delivery-Number = {{BJ3QG}},
Unique-ID = {{ISI:000424196300007}},
DA = {{2019-10-28}},
}

@article{ ISI:000389239000002,
Author = {Fu, Bo and Noy, Natalya F. and Storey, Margaret-Anne},
Title = {{Eye tracking the user experience - An evaluation of ontology
   visualization techniques}},
Journal = {{SEMANTIC WEB}},
Year = {{2017}},
Volume = {{8}},
Number = {{1}},
Pages = {{23-41}},
Abstract = {{Various ontology visualization techniques have been developed over the
   years, offering essential interfaces to users for browsing and
   interacting with ontologies, in an effort to assist with ontology
   understanding. Yet few studies have focused on evaluating the usability
   of existing ontology visualization techniques. This paper presents an
   eye-tracking user study that evaluates two commonly used ontology
   visualization techniques, namely, indented list and graph. The
   eye-tracking experiment and analysis presented in this paper complements
   the set of existing evaluation protocols for ontology visualization. In
   addition, the results found from this study contribute to a greater
   understanding of the strengths and weaknesses of the two visualization
   techniques, and in particular, how and why one is more effective than
   the other. Based on approximately 500MB of eye movement data containing
   around 30 million rows of gaze data generated by a Tobii eye tracker, we
   found evidence suggesting indented lists are more efficient at
   supporting information searches and graphs are more efficient at
   supporting information processing.}},
Publisher = {{IOS PRESS}},
Address = {{NIEUWE HEMWEG 6B, 1013 BG AMSTERDAM, NETHERLANDS}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Fu, B (Reprint Author), Univ Victoria, Dept Comp Sci, Victoria, BC V8W 2Y2, Canada.
   Fu, Bo; Storey, Margaret-Anne, Univ Victoria, Dept Comp Sci, Victoria, BC V8W 2Y2, Canada.
   Noy, Natalya F., Stanford Univ, Stanford Ctr Biomed Informat Res, Stanford, CA 94305 USA.
   Noy, Natalya F., Google Inc, Menlo Pk, CA USA.}},
DOI = {{10.3233/SW-140163}},
ISSN = {{1570-0844}},
EISSN = {{2210-4968}},
Keywords = {{Eye tracking; ontology visualization evaluation; usability study; gaze
   data processing; statistical analysis}},
Keywords-Plus = {{MOVEMENTS; BROWSER; TOOL}},
Research-Areas = {{Computer Science}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Theory \& Methods}},
Author-Email = {{bofu@uvic.ca}},
Funding-Acknowledgement = {{National Center for Biomedical Ontology (NCBO) from the National
   Institutes of Health {[}U54 HG004028]}},
Funding-Text = {{This research is supported by the National Center for Biomedical
   Ontology (NCBO) under grant U54 HG004028 from the National Institutes of
   Health.}},
Number-of-Cited-References = {{74}},
Times-Cited = {{10}},
Usage-Count-Last-180-days = {{6}},
Usage-Count-Since-2013 = {{31}},
Journal-ISO = {{Semant. Web}},
Doc-Delivery-Number = {{EE0BR}},
Unique-ID = {{ISI:000389239000002}},
DA = {{2019-10-28}},
}

@article{ ISI:000389010500002,
Author = {Li, Wen-Chin and Yu, Chung-San and Braithwaite, Graham and Greaves,
   Matthew},
Title = {{Pilots' Attention Distributions Between Chasing a Moving Target and a
   Stationary Target}},
Journal = {{Aerospace Medicine and Human Performance}},
Year = {{2016}},
Volume = {{87}},
Number = {{12}},
Pages = {{989-995}},
Month = {{DEC}},
Abstract = {{INTRODUCTION: Attention plays a central role in cognitive processing;
   ineffective attention may induce accidents in flight operations. The
   objective of the current research was to examine military pilots'
   attention distributions between chasing a moving target and a stationary
   target.
   METHOD: In the current research, 37 mission-ready F-16 pilots
   participated. Subjects' eye movements were collected by a portable
   head-mounted eye-tracker during tactical training in a flight simulator.
   The scenarios of chasing a moving target (air-to-air) and a stationary
   target (air-to-surface) consist of three operational phases: searching,
   aiming, and lock-on to the targets.
   RESULTS: The findings demonstrated significant differences in pilots'
   percentage of fixation during the searching phase between air-to-air (M
   = 37.57, SD = 5.72) and air-to-surface (M = 33.54, SD = 4.68). Fixation
   duration can indicate pilots' sustained attention to the trajectory of a
   dynamic target during air combat maneuvers. Aiming at the stationary
   target resulted in larger pupil size (M = 27,105, SD = 6565), reflecting
   higher cognitive loading than aiming at the dynamic target (M = 23,864,
   SD = 8762).
   DISCUSSION: Pilots' visual behavior is not only closely related to
   attention distribution, but also significantly associated with task
   characteristics. Military pilots demonstrated various visual scan
   patterns for searching and aiming at different types of targets based on
   the research settings of a flight simulator.The findings will facilitate
   system designers' understanding of military pilots' cognitive processes
   during tactical operations. They will assist human-centered interface
   design to improve pilots' situational awareness. The application of an
   eye-tracking device integrated with a flight simulator is a feasible and
   cost-effective intervention to improve the efficiency and safety of
   tactical training.}},
Publisher = {{AEROSPACE MEDICAL ASSOC}},
Address = {{320 S HENRY ST, ALEXANDRIA, VA 22314-3579 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Li, WC (Reprint Author), Cranfield Univ, Safety \& Accid Invest Ctr, Martell House,Univ Way, Cranfield MK43 0TR, Beds, England.
   Li, Wen-Chin; Braithwaite, Graham; Greaves, Matthew, Cranfield Univ, Safety \& Accid Invest Ctr, Martell House,Univ Way, Cranfield MK43 0TR, Beds, England.
   Yu, Chung-San, Natl Tsing Hua Univ, Dept Ind Engn \& Engn Management, Hsinchu, Taiwan.}},
DOI = {{10.3357/AMHP.4617.2016}},
ISSN = {{2375-6314}},
EISSN = {{2375-6322}},
Keywords = {{attentional processes; eye movements; mental workload; simulation and
   training; situation awareness}},
Keywords-Plus = {{NATURALISTIC DECISION-MAKING; SITUATION AWARENESS; EYE-MOVEMENTS;
   TRACKING; PERFORMANCE; PERCEPTION}},
Research-Areas = {{Biophysics; Public, Environmental \& Occupational Health; Research \&
   Experimental Medicine}},
Web-of-Science-Categories  = {{Biophysics; Public, Environmental \& Occupational Health; Medicine,
   Research \& Experimental}},
Author-Email = {{wenchin.li@cranfield.ac.uk}},
ResearcherID-Numbers = {{Yu, Chung-San/P-2060-2019
   Braithwaite, Graham/K-7730-2018}},
ORCID-Numbers = {{Braithwaite, Graham/0000-0002-5120-755X}},
Number-of-Cited-References = {{24}},
Times-Cited = {{2}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{17}},
Journal-ISO = {{Aerosp. Med.Hum. Perform.}},
Doc-Delivery-Number = {{ED7BK}},
Unique-ID = {{ISI:000389010500002}},
OA = {{Green Accepted}},
DA = {{2019-10-28}},
}

@article{ ISI:000386204500047,
Author = {Mueller, Jana Annina and Wendt, Dorothea and Kollmeier, Birger and
   Brand, Thomas},
Title = {{Comparing Eye Tracking with Electrooculography for Measuring Individual
   Sentence Comprehension Duration}},
Journal = {{PLOS ONE}},
Year = {{2016}},
Volume = {{11}},
Number = {{10}},
Month = {{OCT 20}},
Abstract = {{The aim of this study was to validate a procedure for performing the
   audio-visual paradigm introduced by Wendt et al. (2015) with reduced
   practical challenges. The original paradigm records eye fixations using
   an eye tracker and calculates the duration of sentence comprehension
   based on a bootstrap procedure. In order to reduce practical challenges,
   we first reduced the measurement time by evaluating a smaller
   measurement set with fewer trials. The results of 16 listeners showed
   effects comparable to those obtained when testing the original full
   measurement set on a different collective of listeners. Secondly, we
   introduced electrooculography as an alternative technique for recording
   eye movements. The correlation between the results of the two recording
   techniques (eye tracker and electrooculography) was r = 0.97, indicating
   that both methods are suitable for estimating the processing duration of
   individual participants. Similar changes in processing duration arising
   from sentence complexity were found using the eye tracker and the
   electrooculography procedure. Thirdly, the time course of eye fixations
   was estimated with an alternative procedure, growth curve analysis,
   which is more commonly used in recent studies analyzing eye tracking
   data. The results of the growth curve analysis were compared with the
   results of the bootstrap procedure. Both analysis methods show similar
   processing durations.}},
Publisher = {{PUBLIC LIBRARY SCIENCE}},
Address = {{1160 BATTERY STREET, STE 100, SAN FRANCISCO, CA 94111 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Muller, JA (Reprint Author), Carl von Ossietzky Univ Oldenburg, Med Phys, Oldenburg, Germany.
   Muller, JA (Reprint Author), Carl von Ossietzky Univ Oldenburg, Cluster Excellence Hearing4all, Oldenburg, Germany.
   Mueller, Jana Annina; Kollmeier, Birger; Brand, Thomas, Carl von Ossietzky Univ Oldenburg, Med Phys, Oldenburg, Germany.
   Mueller, Jana Annina; Kollmeier, Birger; Brand, Thomas, Carl von Ossietzky Univ Oldenburg, Cluster Excellence Hearing4all, Oldenburg, Germany.
   Wendt, Dorothea, Tech Univ Denmark, Dept Elect Engn, Hearing Syst, Lyngby, Denmark.
   Wendt, Dorothea, Eriksholm Res Ctr, Snekkersten, Denmark.}},
DOI = {{10.1371/journal.pone.0164627}},
Article-Number = {{e0164627}},
ISSN = {{1932-6203}},
Keywords-Plus = {{SPOKEN WORD RECOGNITION; TIME-COURSE; INTELLIGIBILITY; MOVEMENTS;
   LANGUAGE; NOISE; OCULOGRAPHY; INTEGRATION; PARADIGM; DYNAMICS}},
Research-Areas = {{Science \& Technology - Other Topics}},
Web-of-Science-Categories  = {{Multidisciplinary Sciences}},
Author-Email = {{j.a.mueller@uni-oldenburg.de}},
Funding-Acknowledgement = {{Deutsche Forschungsgemeinschaft (DFG)German Research Foundation (DFG)
   {[}SFB/Transregio 31];  {[}BR 3668/1-2];  {[}RU 1494/2-2]}},
Funding-Text = {{This research was supported by the Deutsche Forschungsgemeinschaft
   (DFG), SFB/Transregio 31 ``Das aktive Gehor{''}, and grant numbers BR
   3668/1-2 and RU 1494/2-2. The funder had no role in study design, data
   collection and analysis, decision to publish, or preparation of the
   manuscript.}},
Number-of-Cited-References = {{39}},
Times-Cited = {{6}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{9}},
Journal-ISO = {{PLoS One}},
Doc-Delivery-Number = {{DZ9NZ}},
Unique-ID = {{ISI:000386204500047}},
OA = {{DOAJ Gold, Green Accepted, Green Published}},
DA = {{2019-10-28}},
}

@article{ ISI:000385204200001,
Author = {Takacs, Zsofia K. and Bus, Adriana G.},
Title = {{Benefits of Motion in Animated Storybooks for Children's Visual
   Attention and Story Comprehension. An Eye-Tracking Study}},
Journal = {{FRONTIERS IN PSYCHOLOGY}},
Year = {{2016}},
Volume = {{7}},
Month = {{OCT 13}},
Abstract = {{The present study provides experimental evidence regarding 4-6-year-old
   children's visual processing of animated versus static illustrations in
   storybooks. Thirty nine participants listened to an animated and a
   static book, both three times, while eye movements were registered with
   an eye-tracker. Outcomes corroborate the hypothesis that specifically
   motion is what attracts children's attention while looking at
   illustrations. It is proposed that animated illustrations that are well
   matched to the text of the story guide children to those parts of the
   illustration that are important for understanding the story. This may
   explain why animated books resulted in better comprehension than static
   books.}},
Publisher = {{FRONTIERS MEDIA SA}},
Address = {{PO BOX 110, EPFL INNOVATION PARK, BUILDING I, LAUSANNE, 1015,
   SWITZERLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Takacs, ZK (Reprint Author), Eotvos Lorand Univ, Fac Educ \& Psychol, Inst Educ, Budapest, Hungary.
   Takacs, ZK; Bus, AG (Reprint Author), Leiden Univ, Inst Educ \& Child Studies, Learning Problems \& Impairments, Leiden, Netherlands.
   Takacs, Zsofia K., Eotvos Lorand Univ, Fac Educ \& Psychol, Inst Educ, Budapest, Hungary.
   Takacs, Zsofia K.; Bus, Adriana G., Leiden Univ, Inst Educ \& Child Studies, Learning Problems \& Impairments, Leiden, Netherlands.}},
DOI = {{10.3389/fpsyg.2016.01591}},
Article-Number = {{1591}},
ISSN = {{1664-1078}},
Keywords = {{multimedia learning; electronic storybooks; animation; motion; story
   comprehension; vocabulary learning; eye-tracking; attention}},
Keywords-Plus = {{KINDERGARTEN-CHILDREN; TELEVISION; LITERACY; PROMISE; READERS; MEMORY;
   BOOKS; MEDIA}},
Research-Areas = {{Psychology}},
Web-of-Science-Categories  = {{Psychology, Multidisciplinary}},
Author-Email = {{bus@fsw.leidenuniv.nl
   takacs.zsofia@ppk.elte.hu}},
ResearcherID-Numbers = {{Takacs, Zsofia/H-8615-2017
   }},
ORCID-Numbers = {{Takacs, Zsofia/0000-0003-0190-0628
   Bus, Adriana/0000-0002-2836-176X}},
Funding-Acknowledgement = {{Netherlands Organisation for Scientific Research (NWO)Netherlands
   Organization for Scientific Research (NWO) {[}411-07-216]}},
Funding-Text = {{This study was supported by a Grant from The Netherlands Organisation
   for Scientific Research (NWO) to AB (Project 411-07-216).}},
Number-of-Cited-References = {{38}},
Times-Cited = {{15}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{36}},
Journal-ISO = {{Front. Psychol.}},
Doc-Delivery-Number = {{DY6GK}},
Unique-ID = {{ISI:000385204200001}},
OA = {{DOAJ Gold, Green Published}},
DA = {{2019-10-28}},
}

@article{ ISI:000385369100021,
Author = {Zganjar, Ivana and Kovacevic, Dorotea and Brozovic, Maja},
Title = {{UNDERSTANDING DIFFERENT GRAPHIC LEGENDS INFORMATION IN THE PROCESS OF
   LEARNING}},
Journal = {{TEHNICKI VJESNIK-TECHNICAL GAZETTE}},
Year = {{2016}},
Volume = {{23}},
Number = {{5}},
Pages = {{1395-1403}},
Month = {{OCT}},
Abstract = {{Students' perception of graphically shaped information is in large part
   determined by the design of the depicted information. By choosing the
   appropriate graphical elements one can facilitate easier comprehension
   of a system which is displaying new contents to the students. For the
   purpose of this study, a landscape architecture site plan was used with
   three different ways of depicting a legend (symbols, numerals, strokes).
   Quantitative data (response time and number of fixations) on the visual
   perception of 50 students studying the landscape architecture site plan
   was gathered using eye-tracker technology. The participants' task was to
   find the correct name of the plant displayed on the site plan by using
   the legend. The results showed that students interpret the graphical
   information more correctly when they are depicted with numbers. The
   students required more time and needed to pay closer attention to the
   planting plan when the information was depicted with strokes.}},
Publisher = {{UNIV OSIJEK, TECH FAC}},
Address = {{TRG IVANE BRLIC-MAZURANIC 2, SLAVONSKI BROD, HR-35000, CROATIA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Zganjar, I (Reprint Author), Univ Zagreb, Fac Graph Arts, Getaldiceva 2, Zagreb 10000, Croatia.
   Zganjar, Ivana; Kovacevic, Dorotea; Brozovic, Maja, Univ Zagreb, Fac Graph Arts, Getaldiceva 2, Zagreb 10000, Croatia.}},
DOI = {{10.17559/TV-20150310102017}},
ISSN = {{1330-3651}},
EISSN = {{1848-6339}},
Keywords = {{graphic design; legend; students; visual perception}},
Research-Areas = {{Engineering}},
Web-of-Science-Categories  = {{Engineering, Multidisciplinary}},
Author-Email = {{ivana.zganjar@grf.hr
   dorotea.kovacevic@grf.hr
   maja.brozovic@grf.hr}},
Number-of-Cited-References = {{23}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Journal-ISO = {{Teh. Vjesn.}},
Doc-Delivery-Number = {{DY8HD}},
Unique-ID = {{ISI:000385369100021}},
OA = {{DOAJ Gold}},
DA = {{2019-10-28}},
}

@article{ ISI:000385167700008,
Author = {Nikolaides, Alexandra and Miess, Susanne and Auvera, Isabella and
   Mueller, Ralf and Klosterkoetter, Joachim and Ruhrmann, Stephan},
Title = {{Restricted attention to social cues in schizophrenia patients}},
Journal = {{EUROPEAN ARCHIVES OF PSYCHIATRY AND CLINICAL NEUROSCIENCE}},
Year = {{2016}},
Volume = {{266}},
Number = {{7}},
Pages = {{649-661}},
Month = {{OCT}},
Abstract = {{Deficits of psychosocial functioning are a robust finding in
   schizophrenia. Research on social cognition may open a new avenue for
   the development of effective interventions. As a correlate of social
   perceptive information processing deficits, schizophrenia patients (SZP)
   show deviant gaze behavior (GB) while viewing emotional faces. As
   understanding of a social environment requires gathering complex social
   information, our study aimed at investigating the gaze behavior of SZP
   related to social interactions and its impact on the level of social and
   role functioning. GB of 32 SZP and 37 healthy control individuals (HCI)
   was investigated with a high-resolution eye tracker during an unguided
   viewing of 12 complex pictures of social interaction scenes. Regarding
   whole pictures, SZP showed a shorter scanpath length, fewer fixations
   and a shorter mean distance between fixations. Furthermore, SZP
   exhibited fewer and shorter fixations on faces, but not on the socially
   informative bodies nor on the background, suggesting a cue-specific
   abnormality. Logistic regression with bootstrapping yielded a model
   including two GB parameters; a subsequent ROC curve analysis indicated
   an excellent ability of group discrimination (AUC .85). Face-related GB
   aberrations correlated with lower social and role functioning and with
   delusional thinking, but not with negative symptoms. Training of
   spontaneous integration of face-related social information seems
   promising to enable a holistic perception of social information, which
   may in turn improve social and role functioning. The observed ability to
   discriminate SZP from HCI warrants further research on the predictive
   validity of GB in psychosis risk prediction.}},
Publisher = {{SPRINGER HEIDELBERG}},
Address = {{TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Ruhrmann, S (Reprint Author), Univ Cologne, Dept Psychiat \& Psychotherapy, Kerpener Str 62, D-50924 Cologne, Germany.
   Nikolaides, Alexandra; Miess, Susanne; Auvera, Isabella; Mueller, Ralf; Klosterkoetter, Joachim; Ruhrmann, Stephan, Univ Cologne, Dept Psychiat \& Psychotherapy, Kerpener Str 62, D-50924 Cologne, Germany.}},
DOI = {{10.1007/s00406-016-0705-6}},
ISSN = {{0940-1334}},
EISSN = {{1433-8491}},
Keywords = {{Psychosis; Schizophrenia; Gaze behavior; Visual scanpath; Eye tracking;
   Social perception; Social cognition; Functioning}},
Keywords-Plus = {{VISUAL SCAN PATHS; EMOTION RECOGNITION REMEDIATION; PSYCHOLOGICAL
   THERAPY IPT; FACIAL AFFECT RECOGNITION; EYE-MOVEMENTS; PERSECUTORY
   DELUSIONS; SPECTRUM DISORDERS; FACE RECOGNITION; BIPOLAR DISORDER;
   REGRESSION-MODEL}},
Research-Areas = {{Neurosciences \& Neurology; Psychiatry}},
Web-of-Science-Categories  = {{Clinical Neurology; Psychiatry}},
Author-Email = {{Stephan.ruhrmann@uk-koeln.de}},
Funding-Acknowledgement = {{Deutsche Forschungsgemeinschaft (DFG)German Research Foundation (DFG)
   {[}RU859/2-1]}},
Funding-Text = {{This project was supported by the Deutsche Forschungsgemeinschaft (DFG)
   (S.R., J.K.; grant number RU859/2-1). We thank all of the study
   participants and the clinical staff who contributed to this project,
   Prof. Roder (Bern, Switzerland) and his team for providing us with the
   IPT stimulus material and Dr. Michael Edmond (Munich, Germany) for
   language editing.}},
Number-of-Cited-References = {{99}},
Times-Cited = {{8}},
Usage-Count-Last-180-days = {{2}},
Usage-Count-Since-2013 = {{25}},
Journal-ISO = {{Eur. Arch. Psych. Clin. Neurosci.}},
Doc-Delivery-Number = {{DY5TY}},
Unique-ID = {{ISI:000385167700008}},
DA = {{2019-10-28}},
}

@article{ ISI:000390415200008,
Author = {Parodi, Giovanni and Julio, Cristobal},
Title = {{Where do eyes go when reading multisemiotic disciplinary texts?
   Processing words and graphs in an experimental study with eye tracker}},
Journal = {{REVISTA SIGNOS}},
Year = {{2016}},
Volume = {{49}},
Number = {{1}},
Pages = {{149-183}},
Month = {{OCT}},
Abstract = {{Despite the growing interest in the comprehension of multisemiotic
   texts, scientific knowledge about processing of specialized disciplinary
   written genres in Spanish, composed of words and graphics, investigated
   with Eye Tracking ( ET) methodologies, is still scarce or even
   nonexistent. The present study aims at investigating: a) if there is an
   order of presentation of textual information indicating particular
   reading paths (verbal-graph or graph-verbal); b) if there is a semiotic
   system over another that is provided greater attention in reading a
   specific rhetorical step; c) if disciplinarity in terms of university
   study area affects the Reading paths of a given genre passage. It also
   aims at d) understanding readers' opinions regarding multisemiotic
   texts. The experimental design used an intra and inter-subject mixed
   approach using ET. Forty-seven Chilean university students read eight
   textual segments of the Monetary Policy Report genre. The overall
   results indicate that: 1) readers processed with equal attention all the
   texts without the having the `order' of semiotic systems playing any
   difference; 2) the verbal segment of texts is mainly processed,
   regardless of university study area of students' origin; 3)
   disciplinarity only matters when the subjects from economy read the AOI
   Graph Projection; and 4) most subjects in the sample state that they
   spent more time reading the words. These findings can be explained in
   terms of the Logocentric Principle, with no attribution to
   disciplinarity. Data from cognitive processing and from declarative
   knowledge of the readers corroborate in this study.}},
Publisher = {{EDICIONES UNIV VALPARAISO}},
Address = {{CASILLA 1415, VALPARAISO, 00000, CHILE}},
Type = {{Article}},
Language = {{Spanish}},
Affiliation = {{Parodi, G (Reprint Author), Pontificia Univ Catolica Valparaiso, Valparaiso, Region De, Chile.
   Parodi, Giovanni; Julio, Cristobal, Pontificia Univ Catolica Valparaiso, Valparaiso, Region De, Chile.}},
DOI = {{10.4067/S0718-09342016000400008}},
ISSN = {{0718-0934}},
Keywords = {{Multisemiotic comprehension; Monetary Policy Report; Line graphs; eye
   tracker}},
Keywords-Plus = {{E-Z-READER; MOVEMENT CONTROL; MODEL; FIXATIONS; PREDICTABILITY;
   COMPREHENSION; CONSTRUCTION; CORPUS}},
Research-Areas = {{Linguistics}},
Web-of-Science-Categories  = {{Linguistics; Language \& Linguistics}},
Author-Email = {{giovanni.parodi@pucv.cl
   cristobal.julio@pucv.cl}},
ResearcherID-Numbers = {{Julio, Cristobal/P-7265-2019}},
ORCID-Numbers = {{Julio, Cristobal/0000-0002-4723-0271}},
Number-of-Cited-References = {{108}},
Times-Cited = {{6}},
Usage-Count-Last-180-days = {{4}},
Usage-Count-Since-2013 = {{17}},
Journal-ISO = {{Rev. Signos}},
Doc-Delivery-Number = {{EF6BR}},
Unique-ID = {{ISI:000390415200008}},
OA = {{Bronze, Other Gold}},
DA = {{2019-10-28}},
}

@article{ ISI:000383389300001,
Author = {Habicht, Julia and Kollmeier, Birger and Neher, Tobias},
Title = {{Are Experienced Hearing Aid Users Faster at Grasping the Meaning of a
   Sentence Than Inexperienced Users? An Eye-Tracking Study}},
Journal = {{TRENDS IN HEARING}},
Year = {{2016}},
Volume = {{20}},
Month = {{SEP 5}},
Abstract = {{This study assessed the effects of hearing aid (HA) experience on how
   quickly a participant can grasp the meaning of an acoustic
   sentence-in-noise stimulus presented together with two similar pictures
   that either correctly (target) or incorrectly (competitor) depict the
   meaning conveyed by the sentence. Using an eye tracker, the time taken
   by the participant to start fixating the target (the processing time)
   was measured for two levels of linguistic complexity (low vs. high) and
   three HA conditions: clinical linear amplification (National Acoustic
   Laboratories-Revised), single-microphone noise reduction with National
   Acoustic Laboratories-Revised, and linear amplification ensuring a
   sensation level of >= 15 dB up to at least 4 kHz for the speech material
   used here. Timed button presses to the target stimuli after the end of
   the sentences (offline reaction times) were also collected. Groups of
   experienced (eHA) and inexperienced (iHA) HA users matched in terms of
   age, hearing loss, and working memory capacity took part (N = 15 each).
   For the offline reaction times, no effects were found. In contrast,
   processing times increased with linguistic complexity. Furthermore, for
   all HA conditions, processing times were longer (poorer) for the iHA
   group than for the eHA group, despite comparable speech recognition
   performance. Taken together, these results indicate that processing
   times are more sensitive to speech processing-related factors than
   offline reaction times. Furthermore, they support the idea that HA
   experience positively impacts the ability to process noisy speech
   quickly, irrespective of the precise gain characteristics.}},
Publisher = {{SAGE PUBLICATIONS INC}},
Address = {{2455 TELLER RD, THOUSAND OAKS, CA 91320 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Habicht, J (Reprint Author), Carl von Ossietzky Univ Oldenburg, Dept Med Phys \& Acoust, D-26111 Oldenburg, Germany.
   Habicht, Julia; Kollmeier, Birger; Neher, Tobias, Carl von Ossietzky Univ Oldenburg, Med Phys \& Cluster Excellence Hearing4all, D-26111 Oldenburg, Germany.}},
DOI = {{10.1177/2331216516660966}},
Article-Number = {{2331216516660966}},
ISSN = {{2331-2165}},
Keywords = {{hearing aids; speech comprehension; eye-tracking; acclimatization;
   linguistic complexity}},
Keywords-Plus = {{OLDER-ADULTS; SPOKEN LANGUAGE; WORKING-MEMORY; SPEECH COMPREHENSION;
   SYNTACTIC COMPLEXITY; AMPLIFIED SPEECH; LISTENING EFFORT;
   NOISE-REDUCTION; TIME; BENEFIT}},
Research-Areas = {{Audiology \& Speech-Language Pathology; Otorhinolaryngology}},
Web-of-Science-Categories  = {{Audiology \& Speech-Language Pathology; Otorhinolaryngology}},
Author-Email = {{julia.habicht@uni-oldenburg.de}},
ORCID-Numbers = {{Neher, Tobias/0000-0002-1107-9274}},
Funding-Acknowledgement = {{DFG Cluster of Excellence ``Hearing4all{''} {[}EXC 1077/1]}},
Funding-Text = {{The authors disclosed receipt of the following financial support for the
   research, authorship, and/or publication of this article: This research
   was funded by the DFG Cluster of Excellence EXC 1077/1
   ``Hearing4all.{''}}},
Number-of-Cited-References = {{43}},
Times-Cited = {{5}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{10}},
Journal-ISO = {{Trends Hear.}},
Doc-Delivery-Number = {{DW1FV}},
Unique-ID = {{ISI:000383389300001}},
OA = {{DOAJ Gold, Green Published}},
DA = {{2019-10-28}},
}

@article{ ISI:000382159600001,
Author = {Yorzinski, Jessica L.},
Title = {{Eye blinking in an avian species is associated with gaze shifts}},
Journal = {{SCIENTIFIC REPORTS}},
Year = {{2016}},
Volume = {{6}},
Month = {{AUG 30}},
Abstract = {{Even when animals are actively monitoring their environment, they lose
   access to visual information whenever they blink. They can strategically
   time their blinks to minimize information loss and improve visual
   functioning but we have little understanding of how this process
   operates in birds. This study therefore examined blinking in
   freely-moving peacocks (Pavo cristatus) to determine the relationship
   between their blinks, gaze shifts, and context. Peacocks wearing a
   telemetric eye-tracker were exposed to a taxidermy predator (Vulpes
   vulpes) and their blinks and gaze shifts were recorded. Peacocks blinked
   during the majority of their gaze shifts, especially when gaze shifts
   were large, thereby timing their blinks to coincide with periods when
   visual information is already suppressed. They inhibited their blinks
   the most when they exhibited high rates of gaze shifts and were thus
   highly alert. Alternative hypotheses explaining the link between blinks
   and gaze shifts are discussed.}},
Publisher = {{NATURE PUBLISHING GROUP}},
Address = {{MACMILLAN BUILDING, 4 CRINAN ST, LONDON N1 9XW, ENGLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Yorzinski, JL (Reprint Author), Texas A\&M Univ, Dept Wildlife \& Fisheries Sci, College Stn, TX 77843 USA.
   Yorzinski, Jessica L., Texas A\&M Univ, Dept Wildlife \& Fisheries Sci, College Stn, TX 77843 USA.}},
DOI = {{10.1038/srep32471}},
Article-Number = {{32471}},
ISSN = {{2045-2322}},
Keywords-Plus = {{SELECTIVE ATTENTION; MOVEMENTS; VISION; SUPPRESSION; ACTIVATION;
   EYEBLINKS; VIGILANCE; MONKEYS; PEAHENS; RATES}},
Research-Areas = {{Science \& Technology - Other Topics}},
Web-of-Science-Categories  = {{Multidisciplinary Sciences}},
Author-Email = {{jyorzinski@tamu.edu}},
Funding-Acknowledgement = {{College of Agriculture and Life Sciences at Texas A\&M University, Texas
   A\&M AgriLife Research; National Science Foundation graduate research
   fellowshipNational Science Foundation (NSF); Animal Behavior Society
   Student Research Grant; Animal Behavior Graduate Group at UC Davis;
   Chapman Memorial Fund; National Academy of Sciences; Philanthropic
   Educational Organization Scholar Award; National Geographic
   Society/Waitt Foundation grantNational Geographic Society}},
Funding-Text = {{This research was funded by the College of Agriculture and Life Sciences
   at Texas A\&M University, Texas A\&M AgriLife Research, a National
   Science Foundation graduate research fellowship, an Animal Behavior
   Society Student Research Grant, the Animal Behavior Graduate Group at UC
   Davis, the Chapman Memorial Fund, a Grant-In-Aid of Research from the
   National Academy of Sciences (administered by Sigma-Xi, The Scientific
   Research Society), a Philanthropic Educational Organization Scholar
   Award, and a National Geographic Society/Waitt Foundation grant. I thank
   David Guyton for valuable discussion and Michael Platt for loaning
   equipment. Michael Land provided useful feedback on the manuscript.}},
Number-of-Cited-References = {{48}},
Times-Cited = {{5}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{12}},
Journal-ISO = {{Sci Rep}},
Doc-Delivery-Number = {{DU4CN}},
Unique-ID = {{ISI:000382159600001}},
OA = {{DOAJ Gold, Green Published}},
DA = {{2019-10-28}},
}

@article{ ISI:000381456200003,
Author = {Lin, Yu-Tzu and Wu, Cheng-Chih and Hou, Ting-Yun and Lin, Yu-Chih and
   Yang, Fang-Ying and Chang, Chia-Hu},
Title = {{Tracking Students' Cognitive Processes During Program Debugging-An
   Eye-Movement Approach}},
Journal = {{IEEE TRANSACTIONS ON EDUCATION}},
Year = {{2016}},
Volume = {{59}},
Number = {{3}},
Pages = {{175-186}},
Month = {{AUG}},
Abstract = {{This study explores students' cognitive processes while debugging
   programs by using an eye tracker. Students' eye movements during
   debugging were recorded by an eye tracker to investigate whether and how
   high- and low-performance students act differently during debugging.
   Thirty-eight computer science undergraduates were asked to debug two C
   programs. The path of students' gaze while following program codes was
   subjected to sequential analysis to reveal significant sequences of
   areas examined. These significant gaze path sequences were then compared
   to those of students with different debugging performances. The results
   show that, when debugging, high-performance students traced programs in
   a more logical manner, whereas low-performance students tended to stick
   to a line-by-line sequence and were unable to quickly derive the
   program's higher-level logic. Low-performance students also often jumped
   directly to certain suspected statements to find bugs, without following
   the program's logic. They also often needed to trace back to prior
   statements to recall information, and spent more time on manual
   computation. Based on the research results, adaptive instructional
   strategies and materials can be developed for students of different
   performance levels, to improve associated cognitive activities during
   debugging, which can foster learning during debugging and programming.}},
Publisher = {{IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC}},
Address = {{445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Lin, YT (Reprint Author), Natl Taiwan Normal Univ, Grad Inst Informat \& Comp Educ, Taipei, Taiwan.
   Lin, Yu-Tzu; Wu, Cheng-Chih; Hou, Ting-Yun, Natl Taiwan Normal Univ, Grad Inst Informat \& Comp Educ, Taipei, Taiwan.
   Lin, Yu-Chih, Yuanpei Univ, Dept Biomed Engn, Hsinchu 30015, Taiwan.
   Yang, Fang-Ying, Natl Taiwan Normal Univ, Grad Inst Sci Educ, Taipei 10607, Taiwan.
   Chang, Chia-Hu, Natl Taiwan Univ, Grad Inst Networking \& Multimedia, Taipei 10617, Taiwan.}},
DOI = {{10.1109/TE.2015.2487341}},
ISSN = {{0018-9359}},
EISSN = {{1557-9638}},
Keywords = {{Cognition analysis; eye-movement analysis; program debugging;
   programming instruction}},
Keywords-Plus = {{WORKING-MEMORY; COMPREHENSION PROCESSES; STRATEGIES}},
Research-Areas = {{Education \& Educational Research; Engineering}},
Web-of-Science-Categories  = {{Education, Scientific Disciplines; Engineering, Electrical \& Electronic}},
Author-Email = {{linyt0802@gmail.com}},
Number-of-Cited-References = {{62}},
Times-Cited = {{9}},
Usage-Count-Last-180-days = {{5}},
Usage-Count-Since-2013 = {{33}},
Journal-ISO = {{IEEE Trans. Educ.}},
Doc-Delivery-Number = {{DT4MW}},
Unique-ID = {{ISI:000381456200003}},
DA = {{2019-10-28}},
}

@article{ ISI:000383771600010,
Author = {Kreuzmair, Christina and Siegrist, Michael and Keller, Carmen},
Title = {{High Numerates Count Icons and Low Numerates Process Large Areas in
   Pictographs: Results of an Eye-Tracking Study}},
Journal = {{RISK ANALYSIS}},
Year = {{2016}},
Volume = {{36}},
Number = {{8, SI}},
Pages = {{1599-1614}},
Month = {{AUG}},
Abstract = {{In two experiments, we investigated the influence of numeracy on
   individuals' information processing of pictographs depending on numeracy
   via an eye-tracker. In two conditions, participants from the general
   population were presented with a scenario depicting the risk of having
   cancer and were asked to indicate their perceived risk. The risk level
   was high (63\%) in experiment 1 (N = 70) and low (6\%) in experiment 2
   (N = 69). In the default condition, participants were free to use their
   default strategy for information processing. In the
   guiding-toward-the-number condition, they were prompted to count icons
   in the pictograph by answering with an explicit number. We used
   eye-tracking parameters related to the distance between sequential
   fixations to analyze participants' strategies for processing numerical
   information. In the default condition, the higher the numeracy was, the
   shorter the distances traversed in the pictograph were, indicating that
   participants counted the icons. People lower in numeracy performed
   increased large-area processing by comparing highlighted and
   nonhighlighted parts of the pictograph. In the guiding-toward-the-number
   condition, participants used short distances regardless of their
   numeracy, supporting the notion that short distances represent counting.
   Despite the different default processing strategies, participants
   processed the pictograph with a similar depth and derived similar risk
   perceptions. The results show that pictographs are beneficial for
   communicating medical risk. Pictographs make the gist salient by making
   the part-to-whole relationship visually available, and they facilitate
   low numerates' non-numeric processing of numerical information.
   Contemporaneously, pictographs allow high numerates to numerically
   process and rely on the number depicted in the pictograph.}},
Publisher = {{WILEY}},
Address = {{111 RIVER ST, HOBOKEN 07030-5774, NJ USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Kreuzmair, C (Reprint Author), ETH, IED, Consumer Behav, Univ Str 22,CHN J78, CH-8092 Zurich, Switzerland.
   Kreuzmair, Christina; Siegrist, Michael; Keller, Carmen, ETH, IED, Consumer Behav, Univ Str 22,CHN J78, CH-8092 Zurich, Switzerland.}},
DOI = {{10.1111/risa.12531}},
ISSN = {{0272-4332}},
EISSN = {{1539-6924}},
Keywords = {{Eye tracking; information processing strategies; medical risk
   communication; numeracy; pictograph}},
Keywords-Plus = {{FUZZY-TRACE THEORY; MEDICAL DECISION-MAKING; RISK COMMUNICATION;
   DUAL-PROCESSES; HEALTH-RISK; COMPREHENSION; INFORMATION; PERCEPTION;
   FORMAT; MEMORY}},
Research-Areas = {{Public, Environmental \& Occupational Health; Mathematics; Mathematical
   Methods In Social Sciences}},
Web-of-Science-Categories  = {{Public, Environmental \& Occupational Health; Mathematics,
   Interdisciplinary Applications; Social Sciences, Mathematical Methods}},
Author-Email = {{ckreuzmair@ethz.ch}},
ResearcherID-Numbers = {{Siegrist, Michael/A-1032-2008}},
ORCID-Numbers = {{Siegrist, Michael/0000-0002-6139-7190}},
Funding-Acknowledgement = {{Swiss National Science FoundationSwiss National Science Foundation
   (SNSF) {[}100014\_143335]}},
Funding-Text = {{This study was supported by the Swiss National Science Foundation, grant
   number 100014\_143335. Ethical approval for the presented study was
   given by the ethics commission of ETH Zurich. We thank Philipp Kurpiers
   for his help in programming the script for the analysis of the
   eye-tracking data.}},
Number-of-Cited-References = {{58}},
Times-Cited = {{9}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{8}},
Journal-ISO = {{Risk Anal.}},
Doc-Delivery-Number = {{DW6OU}},
Unique-ID = {{ISI:000383771600010}},
DA = {{2019-10-28}},
}

@article{ ISI:000382325100002,
Author = {Wolpin, Seth and Halpenny, Barbara and Sorrentino, Erica and Stewart,
   Mark and McReynolds, Justin and Cvitkovic, Ivan and Chang, Peter and
   Berry, Donna},
Title = {{Usability Testing the ``Personal Patient Profile-Prostate{''} in a
   Sample of African American and Hispanic Men}},
Journal = {{CIN-COMPUTERS INFORMATICS NURSING}},
Year = {{2016}},
Volume = {{34}},
Number = {{7}},
Pages = {{288-296}},
Month = {{JUL}},
Abstract = {{Shared treatment decision making in a cancer setting requires a
   patient's understanding of the potential benefits and risks of each
   treatment option. Graphical display of risk information is one approach
   to improving understanding. Little is known about how patients engage
   with infographics in the context of health education materials and
   whether interactions vary with health literacy levels. We conducted an
   observational study, using an eye tracker device, of how men with newly
   diagnosed localized prostate cancer visually engaged with an on-screen
   infographic depicting risk information in the Personal Patient
   Profile-Prostate. Health literacy was measured with the Short Assessment
   of Health Literacy-English. Gaze patterns on an exemplar screens
   containing infographics about survival were analyzed and explored with
   respect to sociodemographic and health literacy data. Acceptability of
   Personal Patient Profile-Prostate was measured with the Acceptability
   E-scale. Twenty-six English-speaking men participated, and eye tracking
   data were collected for 12 men on the exemplar page of risk information
   that we analyzed. We found preliminary evidence of visual scanning and
   of participants with lower literacy focusing sooner on infographics
   versus text. Acceptability for Personal Patient Profile-Prostate was
   high. These findings suggest that infographics may be of higher relative
   value to participants with low health literacy. Eye trackers may provide
   valuable information on how people visually engage with infographics and
   may inform development of health education materials, although care must
   be taken to minimize data loss.}},
Publisher = {{LIPPINCOTT WILLIAMS \& WILKINS}},
Address = {{TWO COMMERCE SQ, 2001 MARKET ST, PHILADELPHIA, PA 19103 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Wolpin, S (Reprint Author), Univ Washington, Biobehav Nursing \& Hlth Syst, Seattle, WA 98195 USA.
   Wolpin, Seth; Stewart, Mark; McReynolds, Justin; Cvitkovic, Ivan; Berry, Donna, Univ Washington, Biobehav Nursing \& Hlth Syst, Seattle, WA 98195 USA.
   Halpenny, Barbara; Sorrentino, Erica; Berry, Donna, Dana Farber Canc Inst, Phyllis F Cantor Ctr, Boston, MA 02115 USA.
   Chang, Peter, Beth Israel Deaconess Med Ctr, Div Urol, Boston, MA 02215 USA.}},
DOI = {{10.1097/CIN.0000000000000239}},
ISSN = {{1538-2931}},
EISSN = {{1538-9774}},
Keywords = {{Decision support; Health literacy; Infographics; nursing informatics;
   Prostate cancer}},
Keywords-Plus = {{DECISION-SUPPORT-SYSTEM; CANCER; LITERACY; GRAPHS}},
Research-Areas = {{Computer Science; Medical Informatics; Nursing}},
Web-of-Science-Categories  = {{Computer Science, Interdisciplinary Applications; Medical Informatics;
   Nursing}},
Author-Email = {{swolpin@uw.edu}},
Funding-Acknowledgement = {{A. David Mazzone Research Award at Dana-Farber/Harvard Cancer Center}},
Funding-Text = {{This study was funded by an A. David Mazzone Research Award at
   Dana-Farber/Harvard Cancer Center.}},
Number-of-Cited-References = {{23}},
Times-Cited = {{3}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{7}},
Journal-ISO = {{CIN-Comput. Inform. Nurs.}},
Doc-Delivery-Number = {{DU6LC}},
Unique-ID = {{ISI:000382325100002}},
DA = {{2019-10-28}},
}

@article{ ISI:000376212900009,
Author = {Tsai, Meng-Jung and Huang, Li-Ju and Hou, Huei-Tse and Hsu, Chung-Yuan
   and Chiou, Guo-Li},
Title = {{Visual behavior, flow and achievement in game-based learning}},
Journal = {{COMPUTERS \& EDUCATION}},
Year = {{2016}},
Volume = {{98}},
Pages = {{115-129}},
Month = {{JUL}},
Abstract = {{This study utilized eye-tracking technology to explore the differences
   between high- and low-conceptual-comprehension players' visual behaviors
   and game flows in game-based learning (GBL). A total of 22 university
   students participated in this study and their eye movements while
   playing a physics game were recorded by an eye tracker. Along with eye
   tracking measures, the participants' prior knowledge, flow and
   comprehension test scores were collected. Multiple data analysis methods
   including MWU tests, correlation analyses, heat map analyses and lag
   sequential analyses were employed in this study. The results indicated
   that the players in the high comprehension group demonstrated an
   efficient text-reading strategy and better metacognitive controls of
   visual attention during game plays; while those in the low comprehension
   group could have some difficulties in decoding the conceptual
   representations. In addition, the players with higher comprehension
   expressed a higher level of game flow in two aspects: the sense of
   control and concentration. Furthermore, the percentages of fixation for
   the main task and prompt messages were associated with the players' game
   flow experience, especially the time distortion feeling. This study
   successfully applied eye-tracking technology to find learners' visual
   behavioral patterns in GBL environment and confirmed the flow construct
   for GBL, which may provide some insights for the learning mechanism of
   GBL. Future studies have been suggested in this paper. (C) 2016 Elsevier
   Ltd. All rights reserved.}},
Publisher = {{PERGAMON-ELSEVIER SCIENCE LTD}},
Address = {{THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Tsai, MJ (Reprint Author), 43,Sec 4,Keelung Rd, Taipei 106, Taiwan.
   Tsai, Meng-Jung; Huang, Li-Ju; Chiou, Guo-Li, Natl Taiwan Univ Sci \& Technol, Grad Inst Digital Learning \& Educ, Taipei, Taiwan.
   Hou, Huei-Tse, Natl Taiwan Univ Sci \& Technol, Grad Inst Appl Sci \& Technol, Taipei, Taiwan.
   Hsu, Chung-Yuan, Natl Pingtung Univ Sci \& Technol, Dept Child Care, Pingtung 91201, Taiwan.}},
DOI = {{10.1016/j.compedu.2016.03.011}},
ISSN = {{0360-1315}},
EISSN = {{1873-782X}},
Keywords = {{Eye-tracking; Game-based learning; Visual patterns; Visual attention;
   Flow; Conceptual learning}},
Keywords-Plus = {{EYE-MOVEMENTS; SCIENCE TEXT; COMPUTER; TRACKING; STUDENTS;
   COMPREHENSION; PERFORMANCE; ENGAGEMENT; STRATEGIES; MOTIVATION}},
Research-Areas = {{Computer Science; Education \& Educational Research}},
Web-of-Science-Categories  = {{Computer Science, Interdisciplinary Applications; Education \&
   Educational Research}},
Author-Email = {{mjtsai99@mail.ntust.edu.tw
   D10411001@mail.ntust.edu.tw
   hthou@mail.ntust.edu.tw
   jackohio@gmail.com
   glchiou@mail.ntust.edu.tw}},
ORCID-Numbers = {{Tsai, Meng-Jung/0000-0002-8994-861X}},
Funding-Acknowledgement = {{Ministry of Science and Technology in TaiwanMinistry of Science and
   Technology, Taiwan {[}MOST103-2511-S-011-005-MY3,
   MOST103-2511-S-011-002-MY3]}},
Funding-Text = {{This paper was financially supported by the research projects provided
   by the Ministry of Science and Technology in Taiwan under the following
   grant numbers: MOST103-2511-S-011-005-MY3 and
   MOST103-2511-S-011-002-MY3.}},
Number-of-Cited-References = {{61}},
Times-Cited = {{23}},
Usage-Count-Last-180-days = {{12}},
Usage-Count-Since-2013 = {{128}},
Journal-ISO = {{Comput. Educ.}},
Doc-Delivery-Number = {{DM2YB}},
Unique-ID = {{ISI:000376212900009}},
DA = {{2019-10-28}},
}

@article{ ISI:000385299900005,
Author = {Zhang, Bing and Zhang, Min and Ananos, Elena},
Title = {{Effectiveness of the Auditory and Visual effects of Chinese e-Magazine
   on the Graduate Students' Reading Process}},
Journal = {{ANALISI-QUADERNS DE COMUNICACIO I CULTURA}},
Year = {{2016}},
Number = {{54}},
Pages = {{58-74}},
Month = {{JUN}},
Abstract = {{We are in a digital society where new technologies are constantly
   emerging and old technologies are being upgraded faster than we can take
   in. Digital reading has become an important means of reading, but the
   visual process for digital and paper reading is different. With the eye
   tracking technique, the orthogonal design and the regression model, this
   paper, analyzes audiences' eye movement when reading Chinese e-magazine
   under visual and auditory stimulations. An experimental e-magazine was
   created and 80 graduate students were randomly selected. All
   participants underwent the same setup but under different stimuli; they
   had to read short sentences while an eye tracker recorded their eye
   movement; after completion, participants answered a comprehension
   questionnaire. The most important results, analyzed in terms of eye
   fixations, are that text dubbing and background colors have a
   significant effect on reading comprehension, which means that using text
   dubbing and cool colors can lead to a better reading comprehension. The
   findings obtained reveal a better knowledge of the e-magazine reading
   behavior and it can be applied to the design of digital magazines.
   However, it is necessary to perform specific cross-cultural studies and
   to unify the units of measurement of eye tracker and the units of
   measure of reading effectiveness.}},
Publisher = {{UNIV AUTONOMA BARCELONA}},
Address = {{SERVEI PUBLICACIONS, EDIFICI A, BELLATERRA, CARDANYOLA DEL VALLES,
   BARCELONA, 08193, SPAIN}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Zhang, B (Reprint Author), Autonomous Univ Barcelona, Dept Journalism \& Commun, E-08193 Barcelona, Spain.
   Zhang, Bing, Autonomous Univ Barcelona, Dept Journalism \& Commun, E-08193 Barcelona, Spain.
   Zhang, Min, Univ Shanghai Sci \& Technol, Dept Publishing \& Art Design, Shanghai, Peoples R China.
   Ananos, Elena, Autonomous Univ Barcelona, Dept Advertising, Publ Relat \& Audiovisual Commun, E-08193 Barcelona, Spain.}},
DOI = {{10.7238/a.v0i54.2552}},
ISSN = {{0211-2175}},
EISSN = {{2340-5236}},
Keywords = {{e-magazines; attention; eye tracker; digital context; reading;
   multimedia resources}},
Keywords-Plus = {{EYE-TRACKING; PICTURES; ONLINE}},
Research-Areas = {{Communication}},
Web-of-Science-Categories  = {{Communication}},
Author-Email = {{zhangbing1219@gmail.com
   zhangmin0210@usst.edu.cn
   Elena.ananos@uab.cat}},
Number-of-Cited-References = {{45}},
Times-Cited = {{1}},
Usage-Count-Last-180-days = {{2}},
Usage-Count-Since-2013 = {{8}},
Journal-ISO = {{Analisi}},
Doc-Delivery-Number = {{DY7HI}},
Unique-ID = {{ISI:000385299900005}},
OA = {{DOAJ Gold}},
DA = {{2019-10-28}},
}

@article{ ISI:000372767300006,
Author = {Samant, Shilpa S. and Seo, Han-Seok},
Title = {{Effects of label understanding level on consumers' visual attention
   toward sustainability and process-related label claims found on chicken
   meat products}},
Journal = {{FOOD QUALITY AND PREFERENCE}},
Year = {{2016}},
Volume = {{50}},
Pages = {{48-56}},
Month = {{JUN}},
Abstract = {{Consumers' food purchase behavior is influenced by label information
   displayed on the product. However, consumers vary in their label
   knowledge, affecting their individual product quality perception and
   purchase intent. An eye-tracking is an objective methodology that has
   been used to analyze consumers' purchase behavior in terms of their
   visual attention to labels. The aim of this study was to determine the
   effect of label education on consumers' purchase behavior, measured with
   respect to their visual attention to label claims on chicken products.
   Twenty-nine participants with prior food label education were chosen to
   represent the High Label Understanding (HLU) group. Additionally, 29 new
   participants without any kind of prior label education were selected as
   a Control (CNTL) group. Each participant viewed packaging images of
   chicken meat products using the eye-tracker and answered additional
   subjective questions relating to their potential purchase behavior
   (purchase intent, trust in product, and overall liking). Participants in
   the CNTL group examined the appearance of the meat product more often
   and longer than did those in the HLU group. In contrast, participants in
   the HLU group looked at the label claims associated with sustainability
   and process more often and longer than did those in the CNTL group.
   Furthermore, participants in the HLU group liked and trusted the chicken
   products significantly more than did those in the CNTL group. In
   conclusion, our findings provide empirical evidence that enhanced
   label-knowledge increases consumers' visual attention to labels with a
   possibility of translation into positive purchase behavior. Published by
   Elsevier Ltd.}},
Publisher = {{ELSEVIER SCI LTD}},
Address = {{THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Seo, HS (Reprint Author), Univ Arkansas, Dept Food Sci, 2650 North Young Ave, Fayetteville, AR 72704 USA.
   Samant, Shilpa S.; Seo, Han-Seok, Univ Arkansas, Dept Food Sci, 2650 North Young Ave, Fayetteville, AR 72704 USA.}},
DOI = {{10.1016/j.foodqual.2016.01.002}},
ISSN = {{0950-3293}},
EISSN = {{1873-6343}},
Keywords = {{Label understanding; Sustainability; Visual attention; Eye-tracking;
   Chicken meat products; Education}},
Keywords-Plus = {{WILLINGNESS-TO-PAY; NUTRITION INFORMATION; QUALITY PERCEPTION; FOOD;
   PURCHASE; LIKING; INTENTIONS; ATTITUDES; RATINGS; VARY}},
Research-Areas = {{Food Science \& Technology}},
Web-of-Science-Categories  = {{Food Science \& Technology}},
Author-Email = {{hanseok@uark.edu}},
Funding-Acknowledgement = {{University of Arkansas Division of Agriculture}},
Funding-Text = {{This research was partially supported by the start-up funding from the
   University of Arkansas Division of Agriculture.}},
Number-of-Cited-References = {{46}},
Times-Cited = {{11}},
Usage-Count-Last-180-days = {{2}},
Usage-Count-Since-2013 = {{51}},
Journal-ISO = {{Food. Qual. Prefer.}},
Doc-Delivery-Number = {{DH4PG}},
Unique-ID = {{ISI:000372767300006}},
DA = {{2019-10-28}},
}

@article{ ISI:000370932800027,
Author = {Eivazi, Shahram and Bednarik, Roman and Leinonen, Ville and Fraunberg,
   Mikael von Und Zu and Jaaskelainen, Juha E.},
Title = {{Embedding an Eye Tracker Into a Surgical Microscope: Requirements,
   Design, and Implementation}},
Journal = {{IEEE SENSORS JOURNAL}},
Year = {{2016}},
Volume = {{16}},
Number = {{7}},
Pages = {{2070-2078}},
Month = {{APR 1}},
Abstract = {{Eye tracking has long been known as a tool for attention tracking,
   however, the understanding of gaze in the critical domains such as
   surgery is still in its infancy. In image-guided surgery, studying the
   role that visual attention plays in eye-hand coordination, situation
   awareness, and instrumentation control is critical in order to
   understand the nature of expertise and explore the possibilities for
   gaze-based interaction. To date, the eye-tracking technology has not
   been embedded into an operation room microscope and thus limited
   knowledge is available about the role of attention in real-life
   image-guided surgery. To advance the state-of-the-art, we adopted an
   optical solution for eye tracking and embedded a binocular eye tracker
   into a surgical microscope. We present the design principles and
   development evaluation cycles, as well as highlight the technical
   challenges encountered when embedding an eye tracker for a surgical
   microscope. The developed solution can be applied for other types of
   microscopes and ocular-based optical devices, for example,
   ophthalmology, otolaryngology, plastic and reconstructive surgery, and
   astronomical devices.}},
Publisher = {{IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC}},
Address = {{445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Eivazi, S; Bednarik, R (Reprint Author), Univ Eastern Finland, Sch Comp, Lansikatu 15,3rd Floor, Joensuu 80110, Finland.
   Leinonen, V; Fraunberg, MVZ; Jaaskelainen, JE (Reprint Author), Univ Eastern Finland, Neurosurg Kuopio Univ Hosp Kuopio, Joensuu 80130, Finland.
   Eivazi, Shahram; Bednarik, Roman, Univ Eastern Finland, Sch Comp, Lansikatu 15,3rd Floor, Joensuu 80110, Finland.
   Leinonen, Ville; Fraunberg, Mikael von Und Zu; Jaaskelainen, Juha E., Univ Eastern Finland, Neurosurg Kuopio Univ Hosp Kuopio, Joensuu 80130, Finland.}},
DOI = {{10.1109/JSEN.2015.2501237}},
ISSN = {{1530-437X}},
EISSN = {{1558-1748}},
Keywords = {{Eye tracking; microsurgery; operating room}},
Keywords-Plus = {{GAZE; EXPERTISE}},
Research-Areas = {{Engineering; Instruments \& Instrumentation; Physics}},
Web-of-Science-Categories  = {{Engineering, Electrical \& Electronic; Instruments \& Instrumentation;
   Physics, Applied}},
Author-Email = {{shahram.eivazi@uef.fi
   roman.bednarik@uef.fi
   ville.leinonen@kuh.fi
   mikael.fraunberg@kuh.fi
   juha.e.jaaskelainen@kuh.fi}},
ORCID-Numbers = {{Bednarik, Roman/0000-0003-1726-3520}},
Number-of-Cited-References = {{48}},
Times-Cited = {{5}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{10}},
Journal-ISO = {{IEEE Sens. J.}},
Doc-Delivery-Number = {{DE9CK}},
Unique-ID = {{ISI:000370932800027}},
DA = {{2019-10-28}},
}

@article{ ISI:000375157000005,
Author = {Loewen, Shawn and Inceoglu, Solene},
Title = {{The effectiveness of visual input enhancement on the noticing and L2
   development of the Spanish past tense}},
Journal = {{STUDIES IN SECOND LANGUAGE LEARNING AND TEACHING}},
Year = {{2016}},
Volume = {{6}},
Number = {{1}},
Pages = {{89-110}},
Month = {{MAR}},
Abstract = {{Textual manipulation is a common pedagogic tool used to emphasize
   specific features of a second language (L2) text, thereby facilitating
   noticing and, ideally, second language development. Visual input
   enhancement has been used to investigate the effects of highlighting
   specific grammatical structures in a text. The current study uses a
   quasi-experimental design to determine the extent to which textual
   manipulation increase (a) learners' perception of targeted forms and (b)
   their knowledge of the forms. Input enhancement was used to highlight
   the Spanish preterit and imperfect verb forms and an eye tracker
   measured the frequency and duration of participants' fixation on the
   targeted items. In addition, pretests and posttests of the Spanish past
   tense provided information about participants' knowledge of the targeted
   forms. Results indicate that learners were aware of the highlighted
   grammatical forms in the text; however, there was no difference in the
   amount of attention between the enhanced and unenhanced groups. In
   addition, both groups improved in their knowledge of the L2 forms;
   however, again, there was no differential improvement between the two
   groups.}},
Publisher = {{ADAM MICKIEWICZ UNIV, KALISZ}},
Address = {{FAC PEDAGOGY \& FINE ARTS, DEPT ENGLISH STUDIES, UL NOWY SWIAT 28-30,
   KALISZ, 62-800, POLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Loewen, S (Reprint Author), Michigan State Univ, Dept Linguist \& Languages, B255 Wells Hall,619 Red Cedar Rd, E Lansing, MI 48824 USA.
   Inceoglu, S (Reprint Author), Rochester Inst Technol, Dept Modern Languages \& Cultures, 92 Lomb Mem Dr, Rochester, NY 14623 USA.
   Loewen, Shawn, Michigan State Univ, Dept Linguist \& Languages, B255 Wells Hall,619 Red Cedar Rd, E Lansing, MI 48824 USA.
   Inceoglu, Solene, Rochester Inst Technol, Dept Modern Languages \& Cultures, 92 Lomb Mem Dr, Rochester, NY 14623 USA.}},
DOI = {{10.14746/ssllt.2016.6.1.5}},
ISSN = {{2083-5205}},
EISSN = {{2084-1965}},
Keywords = {{input enhancement; eye tracking; attention; L2 reading; L2 Spanish}},
Keywords-Plus = {{TEXTUAL ENHANCEMENT; TOPIC FAMILIARITY; EYE-MOVEMENTS; COMPREHENSION;
   2ND-LANGUAGE; GRAMMAR; FORM; INSTRUCTION; ATTENTION; FEEDBACK}},
Research-Areas = {{Linguistics}},
Web-of-Science-Categories  = {{Linguistics}},
Author-Email = {{loewens@msu.edu
   inceoglu.solene@gmail.com}},
ResearcherID-Numbers = {{Inceoglu, Solene/H-9882-2019
   }},
ORCID-Numbers = {{Inceoglu, Solene/0000-0002-9571-4684
   Loewen, Shawn/0000-0002-2195-313X}},
Number-of-Cited-References = {{59}},
Times-Cited = {{6}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{5}},
Journal-ISO = {{Stud. Second Lang. Learn. Teach.}},
Doc-Delivery-Number = {{DK8DI}},
Unique-ID = {{ISI:000375157000005}},
OA = {{DOAJ Gold}},
DA = {{2019-10-28}},
}

@article{ ISI:000371936900015,
Author = {Stuart, S. and Alcock, L. and Godfrey, A. and Lord, S. and Rochester, L.
   and Galna, B.},
Title = {{Accuracy and re-test reliability of mobile eye-tracking in Parkinson's
   disease and older adults}},
Journal = {{MEDICAL ENGINEERING \& PHYSICS}},
Year = {{2016}},
Volume = {{38}},
Number = {{3}},
Pages = {{308-315}},
Month = {{MAR}},
Abstract = {{Mobile eye-tracking is important for understanding the role of vision
   during real-world tasks in older adults (OA) and people with Parkinson's
   disease (PD). However, accuracy and reliability of such devices have not
   been established in these populations. We used a novel protocol to
   quantify accuracy and reliability of a mobile eye-tracker in OA and PD.
   A mobile eye-tracker (Dikablis) measured the saccade amplitudes of 20 OA
   and 14 PD on two occasions. Participants made saccades between targets
   placed 5 degrees, 10 degrees and 15 degrees apart. Impact of visual
   correction (glasses) on saccadic amplitude measurement was also
   investigated in 10 OA.
   Saccade amplitude accuracy (median bias) was 1.21 degrees but a wide
   range of bias (-7.73 degrees to 5.81 degrees) was seen in OA and PD,
   with large vertical saccades (15 degrees) being least accurate.
   Reliability assessment showed a median difference between sessions of <1
   degrees for both groups, with poor to good relative agreement (Spearman
   rho: 0.14 to 0.85). Greater accuracy and reliability was observed in
   people without visual correction.
   Saccade amplitude can be measured with variable accuracy and reliability
   using a mobile eye-tracker in OA and PD. Human, technological and
   study-specific protocol factors may introduce error and are discussed
   along with methodological recommendations. (C) 2015 IPEM. Published by
   Elsevier Ltd. All rights reserved.}},
Publisher = {{ELSEVIER SCI LTD}},
Address = {{THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Stuart, S (Reprint Author), Newcastle Univ, Inst Ageing, Inst Neurosci, Clin Ageing Res Unit, Newcastle Upon Tyne NE1 7RU, Tyne \& Wear, England.
   Stuart, S.; Alcock, L.; Godfrey, A.; Lord, S.; Rochester, L.; Galna, B., Newcastle Univ, Inst Ageing, Inst Neurosci, Clin Ageing Res Unit, Newcastle Upon Tyne NE1 7RU, Tyne \& Wear, England.}},
DOI = {{10.1016/j.medengphy.2015.12.001}},
ISSN = {{1350-4533}},
EISSN = {{1873-4030}},
Keywords = {{Parkinson's disease; Mobile eye-tracking; Accuracy; Reliability;
   Saccades; Walking}},
Keywords-Plus = {{TEST-RETEST RELIABILITY; HEAD COORDINATION; SACCADES; MOVEMENT;
   ACCELERATIONS; WALKING}},
Research-Areas = {{Engineering}},
Web-of-Science-Categories  = {{Engineering, Biomedical}},
Author-Email = {{sam.stuart@newcastle.ac.uk}},
ResearcherID-Numbers = {{Stuart, Samuel/I-6449-2019
   }},
ORCID-Numbers = {{Stuart, Samuel/0000-0001-6846-9372
   Rochester, Lynn/0000-0001-5774-9272
   Godfrey, Alan/0000-0003-4049-9291
   Alcock, Lisa/0000-0002-8364-9803}},
Funding-Acknowledgement = {{NIHR Newcastle CRF Infrastructure funding; National Institute for Health
   Research (NIHR) Newcastle Biomedical Research Unit/Centre (BRU/BRC)
   based at Newcastle-upon-Tyne Hospitals NHS Foundation Trust and
   Newcastle University}},
Funding-Text = {{This research is supported by the National Institute for Health Research
   (NIHR) Newcastle Biomedical Research Unit/Centre (BRU/BRC) based at
   Newcastle-upon-Tyne Hospitals NHS Foundation Trust and Newcastle
   University. The research was also supported by NIHR Newcastle CRF
   Infrastructure funding. The views expressed are solely those of the
   authors.}},
Number-of-Cited-References = {{45}},
Times-Cited = {{10}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{16}},
Journal-ISO = {{Med. Eng. Phys.}},
Doc-Delivery-Number = {{DG2YO}},
Unique-ID = {{ISI:000371936900015}},
OA = {{Green Published}},
DA = {{2019-10-28}},
}

@article{ ISI:000366782900021,
Author = {Antunez, Lucia and Ares, Gaston and Gimenez, Ana and Jaeger, Sara R.},
Title = {{Do individual differences in visual attention to CATA questions affect
   sensory product characterization? A case study with plain crackers}},
Journal = {{FOOD QUALITY AND PREFERENCE}},
Year = {{2016}},
Volume = {{48}},
Number = {{A}},
Pages = {{185-194}},
Month = {{MAR}},
Abstract = {{CATA questions are gaining prominence as a quick and easy method for
   obtaining consumer-derived sensory product characterizations. The
   present research contributes further methodological understanding by
   exploring individual differences in visual attention to CATA questions
   and investigating the relationship between visual attention throughout
   the task and consumers' ability to discriminate among samples. A
   consumer study was carried out in which 113 participants were asked to
   evaluate six plain crackers and to answer a check-all-that-apply
   question composed of 20 sensory terms, which was presented on a computer
   screen. While consumers completed the CATA tasks their eye movements
   were recorded using a remote eye-tracker. Considerable heterogeneity
   among participants existed with regard to the number of fixations made
   when completing the CATA questions. Three groups of consumers who
   differed in their degree of visual processing (more thorough to less
   thorough) were identified. These groups also differed in their responses
   to the CATA question. Consumers who performed the most thorough visual
   processing of the CATA question, used, on average, a significantly
   larger number of CATA terms to describe the products than consumers in
   the other two groups. Meanwhile, consumers who engaged in the least
   detailed visual processing showed lesser discriminative ability.
   However, sensory spaces from the three consumer groups were highly
   similar, suggesting that consumers identified the same similarities and
   differences among samples despite differing in the degree of visual
   attention directed to the CATA question. (C) 2015 Elsevier Ltd. All
   rights reserved.}},
Publisher = {{ELSEVIER SCI LTD}},
Address = {{THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Ares, G (Reprint Author), Univ Republica, Fac Quim, Gral Flores 2124, Montevideo 11800, Uruguay.
   Antunez, Lucia; Ares, Gaston; Gimenez, Ana, Univ Republica, Fac Quim, Montevideo 11800, Uruguay.
   Antunez, Lucia; Ares, Gaston; Gimenez, Ana, Univ Republica, Fac Psicol, Ctr Invest Basica Psicol, Montevideo 11200, Uruguay.
   Jaeger, Sara R., New Zealand Inst Plant \& Food Res Ltd, Auckland, New Zealand.}},
DOI = {{10.1016/j.foodqual.2015.09.009}},
ISSN = {{0950-3293}},
EISSN = {{1873-6343}},
Keywords = {{Check-all-that-apply; Sensory characterization; Segmentation;
   Eye-tracking; Visual processing}},
Keywords-Plus = {{THAT-APPLY QUESTIONS; THINKING STYLES; EYE-TRACKING; CONSUMERS;
   REPRODUCIBILITY; COGNITION; INSIGHTS; NUMBER; NEED}},
Research-Areas = {{Food Science \& Technology}},
Web-of-Science-Categories  = {{Food Science \& Technology}},
Author-Email = {{gares@fq.edu.uy}},
ResearcherID-Numbers = {{Jaeger, Sara R/R-2648-2016
   }},
ORCID-Numbers = {{Jaeger, Sara/0000-0002-4960-5233
   Antunez, Lucia/0000-0003-2497-6609}},
Funding-Acknowledgement = {{New Zealand Institute for Plant \& Food Research Limited; Comision
   Sectorial de Investigacion Cientifica (Universidad de la Republica,
   Uruguay)}},
Funding-Text = {{The authors are indebted to the New Zealand Institute for Plant \& Food
   Research Limited and Comision Sectorial de Investigacion Cientifica
   (Universidad de la Republica, Uruguay) for financial support.}},
Number-of-Cited-References = {{44}},
Times-Cited = {{4}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{25}},
Journal-ISO = {{Food. Qual. Prefer.}},
Doc-Delivery-Number = {{CZ0GK}},
Unique-ID = {{ISI:000366782900021}},
DA = {{2019-10-28}},
}

@article{ ISI:000368540100005,
Author = {Sharma, Chandresh and Bhavsar, Punitkumar and Srinivasan, Babji and
   Srinivasan, Rajagopalan},
Title = {{Eye gaze movement studies of control room operators: A novel approach to
   improve process safety}},
Journal = {{COMPUTERS \& CHEMICAL ENGINEERING}},
Year = {{2016}},
Volume = {{85}},
Pages = {{43-57}},
Month = {{FEB 2}},
Abstract = {{Process industries continue to suffer from accidents despite significant
   regulatory intervention since the mid-1980s. Human error is widely
   considered to be the major cause for most accidents today. Detailed
   analysis of various incidents indicates that reduced staffing levels in
   control rooms and inadequate operator training with complex automation
   strategies as common reasons for human errors. Therefore, there is a
   need to develop deeper understanding of human errors as well as
   strategies to prevent them. However, similar to hardware failures,
   traditionally human error has been quantified using likelihood
   approaches; this viewpoint abnegates the role of the cognitive abilities
   of the operators. Recent studies in other safety critical domains (
   aviation, health-care) show that operator's level of situation awareness
   as inferred by eye tracking is a good online indicator of human error.
   In this work, a novel attempt is made to understand the behavior of the
   operator in a typical chemical plant control room using the information
   obtained from eye tracker. Experimental studies conducted on 72
   participants reveal that fixation patterns contain signatures about the
   operators learning and awareness at various situations. Implications of
   these findings on human error in process plant operations them are
   discussed. (C) 2015 Elsevier Ltd. All rights reserved.}},
Publisher = {{PERGAMON-ELSEVIER SCIENCE LTD}},
Address = {{THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Srinivasan, B; Srinivasan, R (Reprint Author), Indian Inst Technol Gandhinagar, Dept Chem Engn, VGEC Campus, Chandkheda 382424, Ahmedabad, India.
   Sharma, Chandresh; Bhavsar, Punitkumar; Srinivasan, Babji, Indian Inst Technol Gandhinagar, Dept Elect Engn, VGEC Campus, Chandkheda 382424, Ahmedabad, India.
   Srinivasan, Babji; Srinivasan, Rajagopalan, Indian Inst Technol Gandhinagar, Dept Chem Engn, VGEC Campus, Chandkheda 382424, Ahmedabad, India.}},
DOI = {{10.1016/j.compchemeng.2015.09.012}},
ISSN = {{0098-1354}},
EISSN = {{1873-4375}},
Keywords = {{Cognitive engineering; Human error; Eye tracking; Process safety}},
Keywords-Plus = {{TRACKING; WORKLOAD}},
Research-Areas = {{Computer Science; Engineering}},
Web-of-Science-Categories  = {{Computer Science, Interdisciplinary Applications; Engineering, Chemical}},
Author-Email = {{babji.srinivasan@iitgn.ac.in
   raj@iitgn.ac.in}},
ResearcherID-Numbers = {{Srinivasan, Rajagopalan/B-5322-2010}},
ORCID-Numbers = {{Srinivasan, Rajagopalan/0000-0002-8790-4349}},
Funding-Acknowledgement = {{Indian Institute of Technology Gandhinagar}},
Funding-Text = {{We would like to thank Dr. Krishna Prasad Miyapuram, Indian Institute of
   Technology Gandhinagar for his constructive suggestions. Financial
   support from Indian Institute of Technology Gandhinagar to pursue this
   work is also gratefully acknowledged.}},
Number-of-Cited-References = {{62}},
Times-Cited = {{14}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{28}},
Journal-ISO = {{Comput. Chem. Eng.}},
Doc-Delivery-Number = {{DB5FO}},
Unique-ID = {{ISI:000368540100005}},
DA = {{2019-10-28}},
}

@article{ ISI:000368655300015,
Author = {Demberg, Vera and Sayeed, Asad},
Title = {{The Frequency of Rapid Pupil Dilations as a Measure of Linguistic
   Processing Difficulty}},
Journal = {{PLOS ONE}},
Year = {{2016}},
Volume = {{11}},
Number = {{1}},
Month = {{JAN 22}},
Abstract = {{While it has long been known that the pupil reacts to cognitive load,
   pupil size has received little attention in cognitive research because
   of its long latency and the difficulty of separating effects of
   cognitive load from the light reflex or effects due to eye movements. A
   novel measure, the Index of Cognitive Activity (ICA), relates cognitive
   effort to the frequency of small rapid dilations of the pupil. We report
   here on a total of seven experiments which test whether the ICA reliably
   indexes linguistically induced cognitive load: three experiments in
   reading (a manipulation of grammatical gender match / mismatch, an
   experiment of semantic fit, and an experiment comparing locally
   ambiguous subject versus object relative clauses, all in German), three
   dual-task experiments with simultaneous driving and spoken language
   comprehension (using the same manipulations as in the single-task
   reading experiments), and a visual world experiment comparing the
   processing of causal versus concessive discourse markers. These
   experiments are the first to investigate the effect and time course of
   the ICA in language processing. All of our experiments support the idea
   that the ICA indexes linguistic processing difficulty. The effects of
   our linguistic manipulations on the ICA are consistent for reading and
   auditory presentation. Furthermore, our experiments show that the ICA
   allows for usage within a multi-task paradigm. Its robustness with
   respect to eye movements means that it is a valid measure of processing
   difficulty for usage within the visual world paradigm, which will allow
   researchers to assess both visual attention and processing difficulty at
   the same time, using an eye-tracker. We argue that the ICA is indicative
   of activity in the locus caeruleus area of the brain stem, which has
   recently also been linked to P600 effects observed in psycholinguistic
   EEG experiments.}},
Publisher = {{PUBLIC LIBRARY SCIENCE}},
Address = {{1160 BATTERY STREET, STE 100, SAN FRANCISCO, CA 94111 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Demberg, V (Reprint Author), Univ Saarland, Cluster Excellence MMCI, D-66123 Saarbrucken, Germany.
   Demberg, Vera; Sayeed, Asad, Univ Saarland, Cluster Excellence MMCI, D-66123 Saarbrucken, Germany.}},
DOI = {{10.1371/journal.pone.0146194}},
Article-Number = {{e0146194}},
ISSN = {{1932-6203}},
Keywords-Plus = {{LOCUS-COERULEUS; LOAD; MODULATION; RESPONSES; STATE}},
Research-Areas = {{Science \& Technology - Other Topics}},
Web-of-Science-Categories  = {{Multidisciplinary Sciences}},
Author-Email = {{vera@coli.uni-saarland.de}},
Funding-Acknowledgement = {{German Research Foundation (DFG)German Research Foundation (DFG);
   Cluster of Excellence Multimodal Computing and Interaction {[}EXC284]}},
Funding-Text = {{This work was supported by German Research Foundation (DFG); both
   authors were supported by the Cluster of Excellence Multimodal Computing
   and Interaction EXC284. The funders had no role in study design, data
   collection and analysis, decision to publish, or preparation of the
   manuscript.}},
Number-of-Cited-References = {{40}},
Times-Cited = {{13}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{19}},
Journal-ISO = {{PLoS One}},
Doc-Delivery-Number = {{DB6WB}},
Unique-ID = {{ISI:000368655300015}},
OA = {{DOAJ Gold, Green Published}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000406771301049,
Author = {Augereau, Olivier and Fujiyoshi, Hiroki and Kise, Koichi},
Book-Group-Author = {{IEEE}},
Title = {{Towards an Automated Estimation of English Skill via TOEIC Score Based
   on Reading Analysis}},
Booktitle = {{2016 23RD INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)}},
Series = {{International Conference on Pattern Recognition}},
Year = {{2016}},
Pages = {{1285-1290}},
Note = {{23rd International Conference on Pattern Recognition (ICPR), Mexican
   Assoc Comp Vis Robot \& Neural Comp, Cancun, MEXICO, DEC 04-08, 2016}},
Organization = {{Int Assoc Pattern Recognit; Int Conf Pattern Recognit, Org Comm;
   Elsevier; IBM Res; INTEL; CONACYT}},
Abstract = {{Estimating automatically the degree of language skill by analyzing the
   eye movements is a promising way to help people from all over the world
   to learn a new language. In this study, we focus on the English skills
   of non-native speakers. Our aim is to provide an algorithm that can
   assess accurately and automatically the TOEIC score after reading
   English texts for few minutes. As a first step towards this direction,
   we propose an algorithm that can predict accurately this score after
   reading and answering some questions about the comprehension of few
   English texts. We use an eye tracker in order to record the eye gaze,
   i.e. the positions where the reader is looking at. Then we extract
   several features to characterize the behavior, and consequently the
   skill of the reader. We also add a feature based on the number of
   correct answers to the questions. By using a machine learning based on
   multivariate regression, the score is estimated user independently. A
   backward stepwise feature selection is used to select the relevant
   features and to optimize the estimation. As a main result, the TOEIC
   score is estimated with 21.7 points of mean absolute error for 21
   subjects after reading and answering the questions of only 3 documents.}},
Publisher = {{IEEE COMPUTER SOC}},
Address = {{10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Augereau, O (Reprint Author), Osaka Prefecture Univ, Naka Ku, 1-1 Gakuencho, Sakai, Osaka 5998531, Japan.
   Augereau, Olivier; Fujiyoshi, Hiroki; Kise, Koichi, Osaka Prefecture Univ, Naka Ku, 1-1 Gakuencho, Sakai, Osaka 5998531, Japan.}},
ISSN = {{1051-4651}},
ISBN = {{978-1-5090-4847-2}},
Keywords-Plus = {{EYE-MOVEMENTS}},
Research-Areas = {{Computer Science}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence}},
Author-Email = {{augereau.o@gmail.com
   fujiyoshi@m.cs.osakafu-u.ac.jp
   kise@cs.osakafu-u.ac.jp}},
Funding-Acknowledgement = {{JST CRESTJapan Science \& Technology Agency (JST)Core Research for
   Evolutional Science and Technology (CREST); JSPS KAKENHIMinistry of
   Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan
   Society for the Promotion of ScienceGrants-in-Aid for Scientific
   Research (KAKENHI) {[}25240028, 15K12172]}},
Funding-Text = {{This work is supported in part by JST CREST and JSPS KAKENHI Grant
   Numbers 25240028 and 15K12172.}},
Number-of-Cited-References = {{19}},
Times-Cited = {{8}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Doc-Delivery-Number = {{BI1RM}},
Unique-ID = {{ISI:000406771301049}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000391174500105,
Author = {Slovackova, Tereza and Soucek, Martin},
Book-Group-Author = {{Slovak Univ Agr Nitra}},
Title = {{The influence of product placement in Czech movies and TV shows among
   generation Y}},
Booktitle = {{INTERNATIONAL SCIENTIFIC DAYS 2016: THE AGRI-FOOD VALUE CHAIN:
   CHALLENGES FOR NATURAL RESOURCES MANAGEMENT AND SOCIETY}},
Year = {{2016}},
Pages = {{874-884}},
Note = {{International Scientific Days (ISD) Conference on Agri Food Value Chain
   - Challenges for Natural Resources Management Society, Nitra, SLOVAKIA,
   MAY 19-20, 2016}},
Organization = {{Slovak Univ Agr Nitra, Fac Econ \& Management; Slovak Univ Agr Nitra,
   Soc Agr Experts; Visegrad Univ Assoc; Soc Agr Experts; Assoc Agr
   Economists Slovakia; PACAgro; FOODCOST; KLUB POLNOHOSPODARSKYCH SPU}},
Abstract = {{This paper is focused on how product placement is perceived by
   generation Y. The data were collected through eye tracking using the SMI
   RED 250 device. In-depth interviews were used to acquire a more detailed
   understanding of the opinion of the respondents. The research was
   conducted during December 2014 and January 2015 in the Eye Tracking
   Laboratory at Mendel University. 35 respondents representing generation
   Y were involved in this research. The main objective was to determine
   their attention to product placement, brand recall and their attitudes
   to product placement. Several parts of movies and TV shows were shown to
   the participants and their attention was measured through the eye
   tracker. Although the viewers looked at the products, they often were
   not able to recall them afterwards. The brand recall was higher when
   they were familiar with the brand. The results show that the audience is
   more strongly affected by prominent product placement than by subtle
   placement. The audience is able to remember an audio-visual product
   placement better than an audio or visual placement. Men are more
   sensitive to product placement related to male product categories (cars,
   alcohol and sexual health products). Women on the other hand are more
   sensitive to product placement related to fashion products.}},
Publisher = {{SLOVAK UNIV AGRICULTURE NITRA}},
Address = {{TR A HLINKU2, NITRA, 94976, SLOVAKIA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Slovackova, T (Reprint Author), Mendel Univ Brno, Fac Business \& Econ, Zemedelska 1, Brno, Czech Republic.
   Slovackova, Tereza; Soucek, Martin, Mendel Univ Brno, Fac Business \& Econ, Zemedelska 1, Brno, Czech Republic.}},
DOI = {{10.15414/isd2016.s11.03}},
ISBN = {{978-80-552-1503-7}},
Keywords = {{product placement; consumer behaviour; consumer perception; brand
   recall; eye tracking; generation Y}},
Keywords-Plus = {{PLOT CONNECTION}},
Research-Areas = {{Agriculture; Business \& Economics; Food Science \& Technology}},
Web-of-Science-Categories  = {{Agricultural Economics \& Policy; Business; Food Science \& Technology}},
Author-Email = {{xslovack@mendelu.cz
   martin.soucek@mendelu.cz}},
Number-of-Cited-References = {{13}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{7}},
Doc-Delivery-Number = {{BG7BT}},
Unique-ID = {{ISI:000391174500105}},
OA = {{Other Gold}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000389279905039,
Author = {Hasanzadeh, Sogand and Esmaeili, Behzad and Dodd, Michael D.},
Editor = {{Perdomo-Rivera, JL and Gonzalez-Quevedo, A and Lopez DelPuerto, C and Maldonado-Fortunet, F and Molina-Bas, OI}},
Title = {{Measuring Construction Workers' Real-Time Situation Awareness Using
   Mobile Eye-Tracking}},
Booktitle = {{CONSTRUCTION RESEARCH CONGRESS 2016: OLD AND NEW CONSTRUCTION
   TECHNOLOGIES CONVERGE IN HISTORIC SAN JUAN}},
Year = {{2016}},
Pages = {{2894-2904}},
Note = {{Construction Research Congress, Univ Puerto Rico Mayaguez, Dept Civil
   Engn \& Surveying, Construct Engn \& M, San Juan, PR, MAY 31-JUN 02,
   2016}},
Organization = {{Amer Soc Civil Engineers, Construct Inst, Construct Res Council}},
Abstract = {{Eye-tracking technology is an emerging methodology that has the
   potential to transform current practices in measuring workers' situation
   awareness (SA). Since eye movements are assumed to indicate where a
   worker's attention is directed, tracking such movements provides a
   practical way to measure workers' attention and comprehension of
   construction hazards. Considering this technology's potential
   applications, some researchers have started to use remote eye trackers
   to investigate workers' abilities to identify hazards; however, these
   studies lack process capabilities to assess real-time situation
   awareness. In order to address this limitation, the main objective of
   this study is to use a mobile eye-tracker to measure workers' SA in
   different scenarios within a real-world construction site. Using direct
   measures of SA (eye tracking) in parallel with subjective SA measures,
   this research resulted in some interesting associations between these
   two types of SA measurements. It also found that situation awareness and
   visual attention allocations of workers vary significantly according to
   the scenario's workload, the state of the area of interest, and the
   workers' level of experience. Results from this study may help to
   identify workers with lower SA and therefore, pinpoint opportunities to
   provide proactive training and develop guidelines for workers that will
   reduce human error and accidents in construction sites. Subsequently,
   this approach can measure the same workers' SA level after training to
   determine whether their SA improved.}},
Publisher = {{AMER SOC CIVIL ENGINEERS}},
Address = {{UNITED ENGINEERING CENTER, 345 E 47TH ST, NEW YORK, NY 10017-2398 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Hasanzadeh, S (Reprint Author), Univ Nebraska, Durham Sch Architectural Engn \& Construct, Lincoln, NE 68588 USA.
   Hasanzadeh, Sogand; Esmaeili, Behzad, Univ Nebraska, Durham Sch Architectural Engn \& Construct, Lincoln, NE 68588 USA.
   Dodd, Michael D., Univ Nebraska, Dept Psychol, Lincoln, NE 68588 USA.}},
ISBN = {{978-0-7844-7982-7}},
Keywords-Plus = {{SEARCH; SAFETY}},
Research-Areas = {{Construction \& Building Technology; Education \& Educational Research;
   Engineering}},
Web-of-Science-Categories  = {{Construction \& Building Technology; Education, Scientific Disciplines;
   Engineering, Industrial; Engineering, Civil}},
Author-Email = {{smohammadhasanzadeh@huskers.unl.edu
   besmaeili2@unl.edu
   mdodd2@unl.edu}},
Number-of-Cited-References = {{24}},
Times-Cited = {{8}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{7}},
Doc-Delivery-Number = {{BG4ZU}},
Unique-ID = {{ISI:000389279905039}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000390309900019,
Author = {Khodambashi, Soudabeh and Gilstad, Heidi and Nytro, Oystein},
Editor = {{Hoerbst, A and Hackl, WO and DeKeizer, N and Prokosch, HU and HercigonjaSzekeres, M and DeLusignan, S}},
Title = {{Usability Evaluation of Clinical Guidelines on the Web Using Eye-Tracker}},
Booktitle = {{EXPLORING COMPLEXITY IN HEALTH: AN INTERDISCIPLINARY SYSTEMS APPROACH}},
Series = {{Studies in Health Technology and Informatics}},
Year = {{2016}},
Volume = {{228}},
Pages = {{95-99}},
Note = {{Medical Informatics Europe (MIE) Conference at Conference on Health -
   Exploring Complexity (HEC) - An Interdisciplinary Systems Approach,
   Munich, GERMANY, AUG 28-SEP 02, 2016}},
Abstract = {{Publishing clinical guidelines (GLs) on the web increases their
   accessibility. However, evaluating their usability and understanding how
   users interact with the websites has been neglected. In this study we
   used Tobii eye-tracker to analyse users' interaction with five
   commercial and public GL sites popular in Norway (four in Norwegian and
   one English of US origin (UpToDate)). We measured number of clicks and
   usage rate for search functions, task completion time, users' objective
   and perception of task success rate. We also measured learning effect
   for inexperienced users. We found a direct correlation between
   participant's satisfaction regarding website usability and the time
   spent, number of mouse clicks and use of search function to obtain the
   desired results. Our study showed that users' perceived success rate was
   not reliable and GL publishers should evaluate their website regarding
   presentation format, layout, navigation bar and search function.}},
Publisher = {{IOS PRESS}},
Address = {{NIEUWE HEMWEG 6B, 1013 BG AMSTERDAM, NETHERLANDS}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Khodambashi, S (Reprint Author), Norwegian Univ Sci \& Technol, Trondheim, Norway.
   Khodambashi, Soudabeh; Gilstad, Heidi; Nytro, Oystein, Norwegian Univ Sci \& Technol, Trondheim, Norway.}},
DOI = {{10.3233/978-1-61499-678-1-95}},
ISSN = {{0926-9630}},
ISBN = {{978-1-61499-678-1; 978-1-61499-677-4}},
Keywords = {{Clinical guidelines; usability evaluation; Tobii eye-tracker; UpToDate}},
Keywords-Plus = {{QUESTIONS}},
Research-Areas = {{Health Care Sciences \& Services}},
Web-of-Science-Categories  = {{Health Care Sciences \& Services}},
Author-Email = {{soudabeh@idi.ntnu.no}},
ORCID-Numbers = {{Nytro, Oystein/0000-0002-8163-2362}},
Number-of-Cited-References = {{12}},
Times-Cited = {{4}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{1}},
Doc-Delivery-Number = {{BG6IU}},
Unique-ID = {{ISI:000390309900019}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000390309900117,
Author = {Klausen, Andreas and Roehrig, Rainer and Lipprandt, Myriam},
Editor = {{Hoerbst, A and Hackl, WO and DeKeizer, N and Prokosch, HU and HercigonjaSzekeres, M and DeLusignan, S}},
Title = {{Feasibility of Eyetracking in Critical Care Environments - A Systematic
   Review}},
Booktitle = {{EXPLORING COMPLEXITY IN HEALTH: AN INTERDISCIPLINARY SYSTEMS APPROACH}},
Series = {{Studies in Health Technology and Informatics}},
Year = {{2016}},
Volume = {{228}},
Pages = {{604-608}},
Note = {{Medical Informatics Europe (MIE) Conference at Conference on Health -
   Exploring Complexity (HEC) - An Interdisciplinary Systems Approach,
   Munich, GERMANY, AUG 28-SEP 02, 2016}},
Abstract = {{Achieving a good understanding of the socio-technical system in critical
   or emergency situations is important for patient safety. Research in
   human-computer interaction in the field of anesthesia or surgery has the
   potential to improve usability of the user interfaces and enhance
   patient safety. Therefore eye-tracking is a technology for analyzing
   gaze patterns. It can also measure what is being perceived by the
   physician during medical procedures. The aim of this review is the
   applicability of eye-tracker in the domain of simulated or real
   environments of anesthesia, surgery or intensive care. We carried out a
   literature research in PubMed. Two independent researchers screened the
   titles and abstracts. The remaining 8 full-papers were analyzed based on
   the applicability of eye-trackers. The articles contain topics like
   training of surgeons, novice vs. experts or the cognitive workload. None
   of the publications address our goal. The applicability or limitations
   of the eye-tracker technology were stated incidentally.}},
Publisher = {{IOS PRESS}},
Address = {{NIEUWE HEMWEG 6B, 1013 BG AMSTERDAM, NETHERLANDS}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Lipprandt, M (Reprint Author), Carl von Ossietzky Univ Oldenburg, Dept Med Informat, Oldenburg, Germany.
   Klausen, Andreas; Roehrig, Rainer; Lipprandt, Myriam, Carl von Ossietzky Univ Oldenburg, Dept Med Informat, Oldenburg, Germany.}},
DOI = {{10.3233/978-1-61499-678-1-604}},
ISSN = {{0926-9630}},
ISBN = {{978-1-61499-678-1; 978-1-61499-677-4}},
Keywords = {{Eye Tracking; Feasibility; Usability; Medical Informatics Applications;
   Equipment and Supplies Keyword; Information System; cognitive workload;
   Surgery; Critical Care; Anaesthesia; Intensive Care; Patient Safety}},
Keywords-Plus = {{ATTENTION; GAZE}},
Research-Areas = {{Health Care Sciences \& Services}},
Web-of-Science-Categories  = {{Health Care Sciences \& Services}},
Author-Email = {{Myriam.Lipprandt@uni-oldenburg.de}},
Number-of-Cited-References = {{11}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Doc-Delivery-Number = {{BG6IU}},
Unique-ID = {{ISI:000390309900117}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000389727300051,
Author = {Wu, Tian-Yu and Liu, Yan},
Editor = {{Stephanidis, C}},
Title = {{Comparative Study on Visual Differences of Poster Designs Based on
   Design Psychology}},
Booktitle = {{HCI INTERNATIONAL 2016 - POSTERS' EXTENDED ABSTRACTS, PT I}},
Series = {{Communications in Computer and Information Science}},
Year = {{2016}},
Volume = {{617}},
Pages = {{302-307}},
Note = {{18th International Conference on Human-Computer Interaction (HCI
   International), Toronto, CANADA, JUL 17-22, 2016}},
Abstract = {{Objective. This paper is to offer relevant theoretical basis and
   experimental method for future Chinese ink poster designs through the
   information on differentiation obtained from an in-depth analysis of the
   concept of design psychology combined with objective examination of the
   visual perception of Chinese ink poster designs of both professionals
   and nonprofessionals by means of eye movement experiments. Method.
   Methods of questionnaire and eye movement experiments were adopted to
   extract the differentiated information on the visual perceptions of
   Chinese ink posters of both design professionals and nonprofessionals
   with the aim of studying and understanding their visual focus and visual
   perception of poster designs and realizing the ``customer-oriented{''}
   concept of Chinese ink poster designs. Conclusion. Through data
   gathering, assessment and analysis, conclusion has been reached that
   professionals are more concerned about the cultural connotation and
   aesthetic imagery while nonprofessionals focus more on the information
   transmission through the hieroglyphs in the posters. The paper argues
   that Chinese ink poster designs are based on certain psychological
   theories and tested and evaluated via scientific experiment.}},
Publisher = {{SPRINGER INT PUBLISHING AG}},
Address = {{GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Wu, TY (Reprint Author), Nanjing Univ Sci \& Technol, Sch Design Art \& Media, Nanjing, Jiangsu, Peoples R China.
   Wu, Tian-Yu; Liu, Yan, Nanjing Univ Sci \& Technol, Sch Design Art \& Media, Nanjing, Jiangsu, Peoples R China.}},
DOI = {{10.1007/978-3-319-40548-3\_51}},
ISSN = {{1865-0929}},
ISBN = {{978-3-319-40548-3; 978-3-319-40547-6}},
Keywords = {{Eye tracker; Poster; Design; Visual; Psychology}},
Research-Areas = {{Computer Science; Engineering}},
Web-of-Science-Categories  = {{Computer Science, Cybernetics; Computer Science, Hardware \&
   Architecture; Computer Science, Software Engineering; Ergonomics}},
Author-Email = {{1031063471@qq.com}},
Number-of-Cited-References = {{7}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{4}},
Doc-Delivery-Number = {{BG5SU}},
Unique-ID = {{ISI:000389727300051}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000389412900040,
Author = {Li, Wen-Chin and White, James and Braithwaite, Graham and Greaves, Matt
   and Lin, Jr-Hung},
Editor = {{Harris, D}},
Title = {{The Evaluation of Pilot's Situational Awareness During Mode Changes on
   Flight Mode Annunciators}},
Booktitle = {{Engineering Psychology and Cognitive Ergonomics, (EPCE 2016)}},
Series = {{Lecture Notes in Artificial Intelligence}},
Year = {{2016}},
Volume = {{9736}},
Pages = {{409-418}},
Note = {{13th International Conference on Engineering Psychology and Cognitive
   Ergonomics (EPCE) held as part of 18th International Conference on
   Human-Computer Interaction (HCI International), Toronto, CANADA, JUL
   17-22, 2016}},
Abstract = {{Current research investigates automation feedback design compared with a
   potential design solution that may increase pilot's situation awareness
   of the Flight Mode Annunciators (FMAs) to reduce pilot workload and
   improve human-automation coordination. The research tools include an Eye
   Tracker and B747 flight simulator. This research evaluated two types of
   FMAs; a proposed glareshield mounted FMAs against the baseline FMA
   design mounted on the Primary Flight Display using an objective eye
   tracker. There are 19 participants including professional and private
   pilots and aerospace engineers. The results suggest that proposed
   glareshield design is the better design compared with the baseline
   design which demonstrated larger mean pupil sizes related to the higher
   workload. A design solution was proposed that moved the FMAs to a MCP
   position, taking into account EASA and FAA design guidance, as well as
   several design principles including positioning to increase salience and
   the proximity compatibility principle. The results of the experiment
   found that FMAs on the MCP could increase pilot SA and reduced the mean
   fixation duration compared to the PFD position. Although the study used
   a small sample size, it demonstrates the value of further research to
   evaluate the proposed design.}},
Publisher = {{SPRINGER INT PUBLISHING AG}},
Address = {{GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Li, WC (Reprint Author), Cranfield Univ, Safety \& Accid Invest Ctr, Cranfield, Beds, England.
   Li, Wen-Chin; White, James; Braithwaite, Graham; Greaves, Matt, Cranfield Univ, Safety \& Accid Invest Ctr, Cranfield, Beds, England.
   Lin, Jr-Hung, Lund Univ, Humanities \& Technol Lab, Helgonabacken 12, Lund, Sweden.}},
DOI = {{10.1007/978-3-319-40030-3\_40}},
ISSN = {{0302-9743}},
ISBN = {{978-3-319-40030-3; 978-3-319-40029-7}},
Keywords = {{Attention distribution; Eye movement; Flight deck design; Mode
   confusion; Proximity compatibility principle}},
Keywords-Plus = {{EYE-MOVEMENTS; COMPREHENSION; OPERATIONS; FEEDBACK; TRACKING; DESIGN}},
Research-Areas = {{Computer Science; Engineering}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Computer Science,
   Cybernetics; Engineering, Electrical \& Electronic; Ergonomics}},
Author-Email = {{wenchin.li@cranfield.ac.uk}},
Number-of-Cited-References = {{27}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{3}},
Doc-Delivery-Number = {{BG5DR}},
Unique-ID = {{ISI:000389412900040}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000387733800009,
Author = {Liu, Sidi and Lv, Jinglei and Hou, Yimin and Shoemaker, Ting and Dong,
   Qinglin and Li, Kaiming and Liu, Tianming},
Book-Group-Author = {{ACM}},
Title = {{What Makes a Good Movie Trailer? Interpretation from Simultaneous EEG
   and Eyetracker Recording}},
Booktitle = {{MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE}},
Year = {{2016}},
Pages = {{82-86}},
Note = {{24th ACM Multimedia Conference (MM), Amsterdam, NETHERLANDS, OCT 15-19,
   2016}},
Organization = {{ACM; ACM SIGMM}},
Abstract = {{What makes a good movie trailer? It's a big challenge to answer this
   question because of the complexity of multimedia in both low level
   sensory features and high level semantic features. However, human
   perception and reactivity could be straightforward evidence for
   evaluation. Modern Electroencephalography (EEG) technology provides
   measurement of consequential brain neural activity to external stimuli.
   Meanwhile, visual perception and attention could be captured and
   interpreted by Eye Tracking technology. Intuitively, simultaneous EEG
   and Eye Tracker recording of human audience with multimedia stimuli
   could bridge the gap between human comprehension and multimedia
   analysis, and provide a new way for movie trailer evaluation. In this
   paper, we propose a novel platform to simultaneously record EEG and eye
   movement for participants with video stimuli by integrating 256-channel
   EEG, Eye Tracker and video display device as a system. Based on the
   proposed system a novel experiment has been designed, in which
   independent and joint features of EEG and Eye tracking data were mined
   to evaluate the movie trailer. Our analysis has shown interesting
   features that are corresponding with trailer quality and video shoot
   changes.}},
Publisher = {{ASSOC COMPUTING MACHINERY}},
Address = {{1515 BROADWAY, NEW YORK, NY 10036-9998 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Liu, SD (Reprint Author), Univ Georgia, Dept Comp, Cort Architecture Imaging \& Discovery Lab, Athens, GA 30602 USA.
   Liu, SD (Reprint Author), Univ Georgia, Bioimaging Res Ctr, Athens, GA 30602 USA.
   Liu, Sidi; Lv, Jinglei; Shoemaker, Ting; Dong, Qinglin; Liu, Tianming, Univ Georgia, Dept Comp, Cort Architecture Imaging \& Discovery Lab, Athens, GA 30602 USA.
   Liu, Sidi; Lv, Jinglei; Shoemaker, Ting; Dong, Qinglin; Liu, Tianming, Univ Georgia, Bioimaging Res Ctr, Athens, GA 30602 USA.
   Hou, Yimin, Northeast Dianli Univ, Sch Automat Engn, Jilin, Peoples R China.
   Li, Kaiming, Sichuan Univ, West China Hosp, Huaxi MR Res Ctr, Chengdu, Peoples R China.}},
DOI = {{10.1145/2964284.2967187}},
ISBN = {{978-1-4503-3603-1}},
Keywords = {{Electroencephalography; Eye-Tracker; Multimedia evaluation}},
Research-Areas = {{Computer Science; Engineering}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Engineering, Electrical \&
   Electronic}},
Author-Email = {{chrisliu8709@gmail.com
   lvjinglei@gmail.com
   ymh7821@163.com
   workspacext@gmail.com
   lynnqd2000@gmail.com
   likaiming@gmail.com
   tianming.liu@gmail.com}},
ResearcherID-Numbers = {{Dong, Qinglin/V-9204-2019}},
Number-of-Cited-References = {{11}},
Times-Cited = {{4}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{6}},
Doc-Delivery-Number = {{BG2WX}},
Unique-ID = {{ISI:000387733800009}},
DA = {{2019-10-28}},
}

@article{ ISI:000378916500013,
Author = {Bongiorno, Nicola and Bosurgi, Gaetano and Pellegrino, Orazio},
Title = {{A PROCEDURE FOR EVALUATING THE INFLUENCE OF ROAD CONTEXT ON DRIVERS'
   VISUAL BEHAVIOUR}},
Journal = {{TRANSPORT}},
Year = {{2016}},
Volume = {{31}},
Number = {{2, SI}},
Pages = {{233-241}},
Abstract = {{In this paper, we investigated drivers' visual behaviour while
   travelling a road regularly opened to traffic in order to evaluate the
   effectiveness of the traditional scientific models and propose, at the
   same time, further measures useful for understanding the complex
   phenomenon. As is known, drivers acquire the necessary information for
   knowing the road geometry by visually detecting certain areas of the
   surrounding context. Some models in the literature have shown in a
   simple and convincing way these mechanisms, but they are valid only with
   specific assumptions, often very restrictive, such as a two-lane road,
   horizontal sign clearly visible and no interaction with other vehicles.
   For this reason, in this study we wanted to investigate different
   conditions, by estimating the visual strategy of some regular drivers on
   a three-lane road in presence of other vehicles. The visual behaviour
   was surveyed with the Tobii Glasses Eye Tracker (R) and the resulting
   raw data were further manipulated by us to extract more useful
   information for our purposes. In particular, we quantified the driver's
   dedicated attention to the various elements present inside the
   environmental context, both static (road edges, road signs, dashboard,
   etc.) and dynamic (other vehicles), meaning by this term those that
   could potentially collide with the trajectories of our vehicle. The
   achieved results, highlighting the limits of validity of some recent
   studies, contain some proposed indexes useful to give a better
   understanding of the visual behaviour in order to detect any eventual
   weakness of the road.}},
Publisher = {{VILNIUS GEDIMINAS TECH UNIV}},
Address = {{SAULETEKIO AL 11, VILNIUS, LT-10223, LITHUANIA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Pellegrino, O (Reprint Author), Univ Messina, Dept Civil Engn, I-98100 Messina, Italy.
   Bongiorno, Nicola; Bosurgi, Gaetano; Pellegrino, Orazio, Univ Messina, Dept Civil Engn, I-98100 Messina, Italy.}},
DOI = {{10.3846/16484142.2016.1188852}},
ISSN = {{1648-4142}},
EISSN = {{1648-3480}},
Keywords = {{highway; information infrastructure; performance evaluation; advanced
   technologies; analysis; visibility}},
Keywords-Plus = {{FUZZY TECHNIQUES; WORKLOAD; VISION; MODEL}},
Research-Areas = {{Transportation}},
Web-of-Science-Categories  = {{Transportation Science \& Technology}},
Author-Email = {{opellegrino@unime.it}},
Number-of-Cited-References = {{23}},
Times-Cited = {{1}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{6}},
Journal-ISO = {{Transport}},
Doc-Delivery-Number = {{DQ0UJ}},
Unique-ID = {{ISI:000378916500013}},
OA = {{Bronze}},
DA = {{2019-10-28}},
}

@article{ ISI:000370329100009,
Author = {Ni Choisdealbha, Aine and Westermann, Gert and Dunn, Kirsty and Reid,
   Vincent},
Title = {{Dissociating associative and motor aspects of action understanding:
   Processing of dual-ended tools by 16-month-old infants}},
Journal = {{BRITISH JOURNAL OF DEVELOPMENTAL PSYCHOLOGY}},
Year = {{2016}},
Volume = {{34}},
Number = {{1, SI}},
Pages = {{115-131}},
Abstract = {{When learning about the functions of novel tools, it is possible that
   infants may use associative and motoric processes. This study
   investigated the ability of 16-month-olds to associate the orientation
   in which an actor held a dual-function tool with the actor's prior
   demonstrated interest in one of two target objects, and their use of the
   tool on that target. The actors' hand posture did not differ between
   conditions. The infants were shown stimuli in which two actors acted
   upon novel objects with a novel tool, each actor employing a different
   function of the tool. Using an eye-tracker, infants' looking time at
   images depicting the actors holding the tool in an orientation congruent
   or incongruent with the actor's goal was measured. Infants preferred to
   look at the specific part of the tool that was incongruent with the
   actor's goal. Results show that the association formed involves the
   specific part of the tool, the actor, and the object the actor acted
   upon, but not the orientation of the tool. The capacity to form such
   associations is demonstrated in this study in the absence of motor
   information that would allow 16-month-olds to generate a specific
   representation of how the tool should be held for each action via
   mirroring processes.}},
Publisher = {{WILEY}},
Address = {{111 RIVER ST, HOBOKEN 07030-5774, NJ USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Ni Choisdealbha, A (Reprint Author), Univ Lancaster, Fylde Coll, Dept Psychol, Lancaster LA1 3BX, England.
   Ni Choisdealbha, Aine; Westermann, Gert; Dunn, Kirsty; Reid, Vincent, Univ Lancaster, Dept Psychol, Lancaster LA1 3BX, England.}},
DOI = {{10.1111/bjdp.12116}},
ISSN = {{0261-510X}},
EISSN = {{2044-835X}},
Keywords = {{tool use; action mirroring; action semantics; infancy}},
Keywords-Plus = {{OBJECT; GAZE; ACTIVATION; PREDICTION; ATTENTION; GOALS; EXPERIENCE;
   RESONANCE; CHILDREN; ADULTS}},
Research-Areas = {{Psychology}},
Web-of-Science-Categories  = {{Psychology, Developmental}},
Author-Email = {{a.nichoisdealbha@lancaster.ac.uk}},
ResearcherID-Numbers = {{Westermann, Gert/B-1699-2008
   }},
ORCID-Numbers = {{Westermann, Gert/0000-0003-2803-1872
   Ni Choisdealbha, Aine/0000-0002-8740-167X
   Reid, Vincent/0000-0002-2774-6975
   Dunn, Kirsty Jayne/0000-0003-0863-8757}},
Funding-Acknowledgement = {{FP7 Marie Curie ITN `ACT' {[}289404]}},
Funding-Text = {{We would like to express our gratitude to all of the families who
   participated in this research. This work was supported by FP7 Marie
   Curie ITN `ACT', Grant Number 289404.}},
Number-of-Cited-References = {{35}},
Times-Cited = {{1}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{3}},
Journal-ISO = {{Br. J. Dev. Psychol.}},
Doc-Delivery-Number = {{DE0QJ}},
Unique-ID = {{ISI:000370329100009}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000382923300134,
Author = {Silva, Ines Garganta and Lopes, Carla Teixeira and Ellison, Maria},
Editor = {{Rocha, A and Reis, LP and Cota, MP and Suarez, OS and Goncalves, R}},
Title = {{Can we detect English proficiency through reading behavior? A
   preliminary study}},
Booktitle = {{2016 11TH IBERIAN CONFERENCE ON INFORMATION SYSTEMS AND TECHNOLOGIES
   (CISTI)}},
Series = {{Iberian Conference on Information Systems and Technologies}},
Year = {{2016}},
Note = {{11th Iberian Conference on Information Systems and Technologies (CISTI),
   SPAIN, JUN 15-18, 2016}},
Organization = {{Asociac Iberica Sistemas Tecnologias Informac; Univ Las Palmas Gran
   Canaria}},
Abstract = {{If it were possible to automatically detect proficiency in languages
   using data from eye movements, new levels of customizing computer
   applications could possibly be achieved. An example in case is web
   searches where suggestions and results could be adjusted to the user's
   knowledge of the language. The objective of this study is to compare the
   reading habits of users with high and low English language proficiency,
   having in mind the possible automatic detection of the English
   proficiency level through reading. For this purpose, a study was
   conducted with two types of user, those with a high level of proficiency
   (Proficient Users), and those with low proficiency (Basic Users) in the
   English language. An eye-tracker was used to collect users' eye
   movements while reading a text in English. Results show that users with
   high proficiency engage in more careful reading. In contrast, low
   English proficiency users take more time to read, revisit sentences and
   paragraphs more often, have more and longer fixations and also a higher
   number of saccades. As expected, these users have more difficulties in
   understanding the text.}},
Publisher = {{IEEE}},
Address = {{345 E 47TH ST, NEW YORK, NY 10017 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Silva, IG (Reprint Author), Univ Porto, Fac Engn, INESC TEC, Oporto, Portugal.
   Silva, IG (Reprint Author), Univ Porto, Fac Engn, DEI, Oporto, Portugal.
   Silva, Ines Garganta; Lopes, Carla Teixeira, Univ Porto, Fac Engn, INESC TEC, Oporto, Portugal.
   Silva, Ines Garganta; Lopes, Carla Teixeira, Univ Porto, Fac Engn, DEI, Oporto, Portugal.
   Ellison, Maria, Univ Porto, Fac Arts, Oporto, Portugal.}},
ISSN = {{2166-0727}},
ISBN = {{978-989-98434-6-2}},
Keywords = {{human-computer interaction; eye-tracker; English proficiency; user study}},
Keywords-Plus = {{EYE-MOVEMENTS; COMPREHENSION}},
Research-Areas = {{Computer Science; Engineering}},
Web-of-Science-Categories  = {{Computer Science, Information Systems; Engineering, Electrical \&
   Electronic}},
Author-Email = {{ei10162@fe.up.pt
   ctl@fe.up.pt
   mellison@letras.up.pt}},
ResearcherID-Numbers = {{Lopes, Carla Teixeira/B-6731-2008}},
ORCID-Numbers = {{Lopes, Carla Teixeira/0000-0002-4202-791X}},
Number-of-Cited-References = {{13}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{2}},
Doc-Delivery-Number = {{BF6BA}},
Unique-ID = {{ISI:000382923300134}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000417330200016,
Author = {de Castell, S. and Larios, H. and Jenson, J.},
Editor = {{Chova, LG and Martinez, AL and Torres, IC}},
Title = {{IMPACTS OF GENDER AND VIDEO GAME EXPERIENCE ON NAVIGATION IN 3D VIRTUAL
   ENVIRONMENTS}},
Booktitle = {{ICERI2016: 9TH INTERNATIONAL CONFERENCE OF EDUCATION, RESEARCH AND
   INNOVATION}},
Series = {{ICERI Proceedings}},
Year = {{2016}},
Pages = {{96-105}},
Note = {{9th Annual International Conference of Education, Research and
   Innovation (iCERi), Seville, SPAIN, NOV 14-16, 2016}},
Abstract = {{Three-Dimensional (3D) virtual environments enable users to interact
   with information in a spatially realistic manner that can facilitate
   visualization and manipulation of information. Yet we still know little
   about how users navigate such environments, and in particular we know
   little about how past spatial experience influences navigation
   performance. In this study, we tested 82 participants in two experiments
   to examine their navigation performance in different versions of the
   virtual Morris Water Maze, analyzing eye fixations and individual
   differences in mental rotation ability. We assessed navigational
   competence by comparing participants' search times to locate a hidden
   target and their dwell times on the actual target area. In general, the
   results showed that past 3D video game experience was associated with
   better navigation performance in some, but not all, instances. We found
   similar sex differences as have been observed in prior research,
   confirming that there is a gender-differentiated uptake of distal and
   proximal cues, with navigators of high spatial ability being less
   reliant on salient proximal cues than navigators of low spatial ability
   -but we were also able to demonstrate how, with the provision of
   proximal cues, in the form of landmarks on the circumference of a
   virtual pool, these gender-based differences in navigational performance
   were significantly diminished. Given that spatial abilities have been
   correlated with positive educational outcomes in STEM (science,
   technology, engineering and mathematics) subjects, and that female
   students have been and remain under-represented in these subject areas
   despite their centrality to 21st century educational and occupational
   demands, understanding the factors that affect abilities to navigate
   virtual spaces, identifying the underlying processes that different
   users bring to bear when navigating 3D virtual environments, and knowing
   what design modifications support improvements in performance are
   important to the advancement of educational opportunities. This study
   contributes to realizing that larger goal in several respects:
   theoretically, it provides insights into how different types of
   environmental information impact 3D virtual navigation performance and
   how spatial learning is affected; methodologically, it demonstrates uses
   of novel research tools and designs to better understand virtual
   navigation, including the use of an eye tracker and virtual versions of
   the Morris maze. As 3D virtual environments proliferate, it will become
   increasingly important to understand how different types of cues affect
   navigational performance. Our work shows how manipulating the proximity
   and saliency of cues impacts spatial learning and goal completion.}},
Publisher = {{IATED-INT ASSOC TECHNOLOGY EDUCATION \& DEVELOPMENT}},
Address = {{LAURI VOLPI 6, VALENICA, BURJASSOT 46100, SPAIN}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{de Castell, S (Reprint Author), Univ Ontario, Inst Technol, Oshawa, ON, Canada.
   de Castell, S.; Larios, H., Univ Ontario, Inst Technol, Oshawa, ON, Canada.
   Jenson, J., York Univ, N York, ON, Canada.}},
DOI = {{10.21125/iceri.2016.1020}},
ISSN = {{2340-1095}},
ISBN = {{978-84-617-5895-1}},
Keywords = {{Virtual environments; spatial abilities; STEM education; videogames and
   learning; virtual Morris water maze; eye tracking}},
Keywords-Plus = {{SEX-DIFFERENCES; WATER-MAZE; SPATIAL ABILITY; MENTAL ROTATIONS;
   VISUALIZATION; ADOLESCENTS; PERFORMANCE; KNOWLEDGE; CHILDREN; MALES}},
Research-Areas = {{Education \& Educational Research}},
Web-of-Science-Categories  = {{Education \& Educational Research}},
Funding-Acknowledgement = {{Social Sciences and Humanities Research Council of CanadaSocial Sciences
   and Humanities Research Council of Canada (SSHRC)}},
Funding-Text = {{The authors are indebted to Walter R. Boot, David Harris Smith, and John
   Murray. This research was supported by the Social Sciences and
   Humanities Research Council of Canada.}},
Number-of-Cited-References = {{30}},
Times-Cited = {{1}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{5}},
Doc-Delivery-Number = {{BJ0ZM}},
Unique-ID = {{ISI:000417330200016}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000469278500025,
Author = {Dingier, Tilman and Rzayev, Rufat and Schwind, Valentin and Henze, Niels},
Book-Group-Author = {{Assoc Comp Machinery}},
Title = {{RSVP on the Go - Implicit Reading Support on Smart Watches Through Eye
   Tracking}},
Booktitle = {{ISWC'16 - PROCEEDINGS OF THE 2016 ACM INTERNATIONAL SYMPOSIUM ON
   WEARABLE COMPUTERS}},
Series = {{IEEE International Symposium on Wearable Computers}},
Year = {{2016}},
Pages = {{116-119}},
Note = {{ACM International Joint Conference on Pervasive and Ubiquitous Computing
   (UbiComp) / 20th ACM International Symposium on Wearable Computers
   (ISWC), Heidelberg, GERMANY, SEP 12-16, 2016}},
Organization = {{Assoc Comp Machinery; ACM SIGCHI; ACM SIGMOBILE}},
Abstract = {{While smartwatches have become common for mobile interaction, one of
   their main limitation is the limited screen size. To facilitate reading
   activities despite these limitations, reading with Rapid Serial Visual
   Presentation (RSVP) has been shown to be feasible. However, when text is
   presented in rapid sequence, single words are easily missed due to
   blinking or briefly glancing up from the screen. This gets worse the
   more the reader is engaged in a secondary task, such as walking. To give
   implicit control over the reading flow we combined an RSVP reading
   application on a smartwatch with a head-worn eye tracker. When the
   reading flow is briefly interrupted, the text presentation automatically
   pauses or backtracks. In a user study with 15 participants we show that
   using eye tracking in combination with RSVP increases users'
   comprehension compared to a touch-based Ul to control the text
   presentation. We argue that eye tracking will be a valuable extension
   for future smartwatch interaction.}},
Publisher = {{ASSOC COMPUTING MACHINERY}},
Address = {{1515 BROADWAY, NEW YORK, NY 10036-9998 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Dingier, T (Reprint Author), Univ Stuttgart, VIS, Stuttgart, Germany.
   Dingier, Tilman; Rzayev, Rufat; Schwind, Valentin; Henze, Niels, Univ Stuttgart, VIS, Stuttgart, Germany.}},
DOI = {{10.1145/2971763.2971794}},
ISSN = {{1550-4816}},
ISBN = {{978-1-4503-4460-9}},
Keywords = {{Reading interfaces; RSVP; eye-tracking; eye gaze interaction; mental
   load; comprehension}},
Research-Areas = {{Computer Science; Telecommunications}},
Web-of-Science-Categories  = {{Computer Science, Cybernetics; Telecommunications}},
Author-Email = {{tilman.dingier@vis.uni-stuttgart.de
   rufat.rzayev@vis.uni-stuttgart.de
   valentin.schwind@vis.uni-stuttgart.de
   niels.henze@vis.uni-stuttgart.de}},
Funding-Acknowledgement = {{Future and Emerging Technologies (FET) programme within the 7th
   Framework Programme for Research of the European Commission, under FET
   grant {[}612933]; German Research Foundation (DFG)German Research
   Foundation (DFG) {[}SFB/Transregio 161]}},
Funding-Text = {{We acknowledge the funding through the Future and Emerging Technologies
   (FET) programme within the 7th Framework Programme for Research of the
   European Commission, under FET grant number: 612933 (RECALL), as well as
   through the German Research Foundation (DFG) within the project C04 of
   SFB/Transregio 161.}},
Number-of-Cited-References = {{14}},
Times-Cited = {{1}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Doc-Delivery-Number = {{BM8JQ}},
Unique-ID = {{ISI:000469278500025}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000390298600116,
Author = {Orlov, Pavel and Gorshkova, Ksenia},
Book-Group-Author = {{ACM}},
Title = {{Gaze-Based Interactive Comics}},
Booktitle = {{PROCEEDINGS OF THE NORDICHI `16: THE 9TH NORDIC CONFERENCE ON
   HUMAN-COMPUTER INTERACTION - GAME CHANGING DESIGN}},
Year = {{2016}},
Note = {{9th Nordic Conference on Human-Computer Interaction (NordiCHI), Chalmers
   Univ Technol, Dept of Appl Informat Technol, Gothenburg, SWEDEN, OCT
   23-27, 2016}},
Organization = {{Univ Gothenburg; IT Univ Gothenburg; Visage Technologies;
   Studentlitteratur; Chalmers Area Advances ICT; Tobii Pro; Assoc Comp
   Machinery}},
Abstract = {{This extended abstract presents a gaze-interactive comics, where page
   design alters based on the coordinates of a user's gaze. We were
   analyzing the impact of gaze-based interactivity on the user's
   impression of the comics the plot comprehension. We found that subjects
   can use gaze-based interaction for comics viewing without prior
   training. The subjects who were using the gaze-based interactive
   technique tend to perceive the story in a slightly more optimistic way
   compared to those who watched the non-interactive version. However,
   subjects from both groups described the environment and the main
   character's emotional state similarly.}},
Publisher = {{ASSOC COMPUTING MACHINERY}},
Address = {{1515 BROADWAY, NEW YORK, NY 10036-9998 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Orlov, P (Reprint Author), Univ Eastern Finland, Sch Comp, POB 111, FI-80101 Joensuu, Finland.
   Orlov, P (Reprint Author), Peter Great St Petersburg Polytech Univ, Dept Engn Graph \& Design, Polytech Skaya 29, St Petersburg 195251, Russia.
   Orlov, Pavel, Univ Eastern Finland, Sch Comp, POB 111, FI-80101 Joensuu, Finland.
   Orlov, Pavel; Gorshkova, Ksenia, Peter Great St Petersburg Polytech Univ, Dept Engn Graph \& Design, Polytech Skaya 29, St Petersburg 195251, Russia.}},
DOI = {{10.1145/2971485.2996751}},
ISBN = {{978-1-4503-4763-1}},
Keywords = {{Eye tracker; gaze-interactive; comics; animation}},
Research-Areas = {{Computer Science; Engineering; Social Sciences - Other Topics}},
Web-of-Science-Categories  = {{Computer Science, Cybernetics; Ergonomics; Social Sciences,
   Interdisciplinary}},
Author-Email = {{paul.a.orlov@gmail.com
   95kseniia@gmail.com}},
ResearcherID-Numbers = {{Orlov, Pavel/K-8344-2018}},
ORCID-Numbers = {{Orlov, Pavel/0000-0001-6256-8134}},
Number-of-Cited-References = {{6}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Doc-Delivery-Number = {{BG6HM}},
Unique-ID = {{ISI:000390298600116}},
DA = {{2019-10-28}},
}

@article{ ISI:000365754900015,
Author = {Yorzinski, Jessica L. and Patricelli, Gail L. and Platt, Michael L. and
   Land, Michael F.},
Title = {{Eye and head movements shape gaze shifts in Indian peafowl}},
Journal = {{JOURNAL OF EXPERIMENTAL BIOLOGY}},
Year = {{2015}},
Volume = {{218}},
Number = {{23}},
Pages = {{3771-3776}},
Month = {{DEC}},
Abstract = {{Animals selectively direct their visual attention toward relevant
   aspects of their environments. They can shift their attention using a
   combination of eye, head and body movements. While we have a growing
   understanding of eye and head movements in mammals, we know little about
   these processes in birds. We therefore measured the eye and head
   movements of freely behaving Indian peafowl (Pavo cristatus) using a
   telemetric eye-tracker. Both eye and head movements contributed to gaze
   changes in peafowl. When gaze shifts were smaller, eye movements played
   a larger role than when gaze shifts were larger. The duration and
   velocity of eye and head movements were positively related to the size
   of the eye and head movements, respectively. In addition, the
   coordination of eye and head movements in peafowl differed from that in
   mammals; peafowl exhibited a near-absence of the vestibulo-ocular
   reflex, which may partly result from the peafowl's ability to move their
   heads as quickly as their eyes.}},
Publisher = {{COMPANY OF BIOLOGISTS LTD}},
Address = {{BIDDER BUILDING CAMBRIDGE COMMERCIAL PARK COWLEY RD, CAMBRIDGE CB4 4DL,
   CAMBS, ENGLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Yorzinski, JL (Reprint Author), Purdue Univ, Dept Biol Sci, 915 West State St, W Lafayette, IN 47907 USA.
   Yorzinski, Jessica L., Purdue Univ, Dept Biol Sci, W Lafayette, IN 47907 USA.
   Yorzinski, Jessica L., Purdue Univ, Dept Anim Sci, W Lafayette, IN 47907 USA.
   Yorzinski, Jessica L.; Patricelli, Gail L., Univ Calif Davis, Anim Behav Grad Grp, Davis, CA 95616 USA.
   Yorzinski, Jessica L.; Patricelli, Gail L., Univ Calif Davis, Dept Evolut \& Ecol, Davis, CA 95616 USA.
   Platt, Michael L., Univ Penn, Perelman Sch Med, Dept Neurosci, Philadelphia, PA 19104 USA.
   Platt, Michael L., Univ Penn, Dept Psychol, Philadelphia, PA 19104 USA.
   Platt, Michael L., Univ Penn, Wharton Sch, Dept Mkt, Philadelphia, PA 19104 USA.
   Land, Michael F., Univ Sussex, Sch Biol Sci, Brighton BN1 9QG, E Sussex, England.}},
DOI = {{10.1242/jeb.129544}},
ISSN = {{0022-0949}},
EISSN = {{1477-9145}},
Keywords = {{Eye movement; Eye tracking; Fixation; Vestibulo-ocular reflex; Pavo
   cristatus; Saccade}},
Keywords-Plus = {{PIGEON COLUMBA-LIVIA; SELECTIVE ATTENTION; COORDINATION; COURTSHIP;
   REFLEXES; MONKEYS; VISION; AVES}},
Research-Areas = {{Life Sciences \& Biomedicine - Other Topics}},
Web-of-Science-Categories  = {{Biology}},
Author-Email = {{jyorzinski@purdue.edu}},
ResearcherID-Numbers = {{Patricelli, Gail L/L-3730-2017
   }},
ORCID-Numbers = {{Patricelli, Gail/0000-0003-3397-8390}},
Funding-Acknowledgement = {{National Science Foundation graduate research fellowshipNational Science
   Foundation (NSF); Animal Behavior Graduate Group at UC Davis; Chapman
   Memorial Fund; National Academy of Sciences (Sigma-Xi, The Scientific
   Research Society); Philanthropic Educational Organization Scholar Award;
   National Geographic Society/Waitt Foundation grantNational Geographic
   Society; UC Davis; National Science Foundation grantNational Science
   Foundation (NSF) {[}IOS-0925038]; Alfred P. Sloan FoundationAlfred P.
   Sloan Foundation; Duke Lemur Center; Animal Behavior Society Student
   Research Grant}},
Funding-Text = {{This research was funded by a National Science Foundation graduate
   research fellowship, an Animal Behavior Society Student Research Grant,
   the Animal Behavior Graduate Group at UC Davis, the Chapman Memorial
   Fund, a Grant-In-Aid of Research from the National Academy of Sciences
   (administered by Sigma-Xi, The Scientific Research Society), a
   Philanthropic Educational Organization Scholar Award, and a National
   Geographic Society/Waitt Foundation grant to J.L.Y. Funding was provided
   by UC Davis and a National Science Foundation grant to G.L.P.
   (IOS-0925038). The Alfred P. Sloan Foundation and Duke Lemur Center
   provided funding to M.L.P.}},
Number-of-Cited-References = {{31}},
Times-Cited = {{7}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{17}},
Journal-ISO = {{J. Exp. Biol.}},
Doc-Delivery-Number = {{CX5PR}},
Unique-ID = {{ISI:000365754900015}},
OA = {{Bronze}},
DA = {{2019-10-28}},
}

@article{ ISI:000366067300006,
Author = {Lemonnier, Sophie and Bremond, Roland and Baccino, Thierry},
Title = {{Gaze behavior when approaching an intersection: Dwell time distribution
   and comparison with a quantitative prediction}},
Journal = {{TRANSPORTATION RESEARCH PART F-TRAFFIC PSYCHOLOGY AND BEHAVIOUR}},
Year = {{2015}},
Volume = {{35}},
Pages = {{60-74}},
Month = {{NOV}},
Abstract = {{The allocation of overt visual attention is investigated in a multi-task
   and dynamical situation: driving. The Expectancy-Value model of
   attention allocation stipulates that visual exploration depends on the
   expectancy and the value of the task-related information available in
   each Area Of Interest (AOI). We consider the approach to an intersection
   as a multi-task situation where two subtasks are involved: vehicle
   control and interactions with other drivers. Each of these subtasks is
   associated with some specific visual information present in the
   associated AOIs: the driver's lane and the intersecting road at the
   intersection. An experiment was conducted in a driving simulator,
   coupled with a head-mounted eye-tracker. The intersecting road's AOI's
   Expectancy was manipulated with the traffic density, and its Value was
   manipulated with the priority rule before the intersection (stop, yield,
   and priority). The distribution of visual attention and the dynamics of
   visual exploration were analyzed on 20 participants, taking into account
   the dwell time in the AOIs associated to the driving subtasks, and the
   gaze transitions between the AOIs. The results suggest that visual
   attention to intersecting roads varied with the priority rule, and
   impacted the visual attention associated with the vehicle control
   subtask. In addition, a quantitative model was used to improve the
   understanding of the Expectancy and Value factors. The comparison of the
   data with the model's predictions enables quantifying the observed
   differences between the experimental factors. Finally, the results
   associated with the traffic density are discussed in relation to the
   nature of the relevant information while approaching the intersection.
   (C) 2015 Elsevier Ltd. All rights reserved.}},
Publisher = {{ELSEVIER SCI LTD}},
Address = {{THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Bremond, R (Reprint Author), IFSTTAR, Lab Rd Operat Percept Simulators \& Simulat, Nantes, France.
   Lemonnier, Sophie; Bremond, Roland, IFSTTAR, Lab Rd Operat Percept Simulators \& Simulat, Nantes, France.
   Lemonnier, Sophie; Baccino, Thierry, Univ Paris 08, LUTIN, Lab Human \& Artificial Cognit, F-93526 St Denis 02, France.}},
DOI = {{10.1016/j.trf.2015.10.015}},
ISSN = {{1369-8478}},
EISSN = {{1873-5517}},
Keywords = {{Visual attention; Area of interest; Intersection; SEEV model}},
Keywords-Plus = {{ATTENTION ALLOCATION; VISUAL-ATTENTION; DRIVER BEHAVIOR; QUEUEING MODEL;
   TO-CONTACT; ENVIRONMENT; PERCEPTION}},
Research-Areas = {{Psychology; Transportation}},
Web-of-Science-Categories  = {{Psychology, Applied; Transportation}},
ResearcherID-Numbers = {{Bremond, Roland/B-6426-2013}},
ORCID-Numbers = {{Bremond, Roland/0000-0003-3150-7624}},
Number-of-Cited-References = {{67}},
Times-Cited = {{5}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{9}},
Journal-ISO = {{Transp. Res. Pt. F-Traffic Psychol. Behav.}},
Doc-Delivery-Number = {{CY0AF}},
Unique-ID = {{ISI:000366067300006}},
DA = {{2019-10-28}},
}

@article{ ISI:000362604600006,
Author = {Sharafi, Zohreh and Soh, Zephyrin and Gueheneuc, Yann-Gael},
Title = {{A systematic literature review on the usage of eye-tracking in software
   engineering}},
Journal = {{INFORMATION AND SOFTWARE TECHNOLOGY}},
Year = {{2015}},
Volume = {{67}},
Pages = {{79-107}},
Month = {{NOV}},
Abstract = {{Context: Eye-tracking is a mean to collect evidence regarding some
   participants' cognitive processes. Eye-trackers monitor participants'
   visual attention by collecting eye-movement data. These data are useful
   to get insights into participants' cognitive processes during reasoning
   tasks.
   Objective: The Evidence-based Software Engineering (EBSE) paradigm has
   been proposed in 2004 and, since then, has been used to provide detailed
   insights regarding different topics in software engineering research and
   practice. Systematic Literature Reviews (SLR) are also useful in the
   context of EBSE by bringing together all existing evidence of research
   and results about a particular topic. This SLR evaluates the current
   state of the art of using eye-trackers in software engineering and
   provides evidence on the uses and contributions of eye-trackers to
   empirical studies in software engineering.
   Method: We perform a SLR covering eye-tracking studies in software
   engineering published from 1990 up to the end of 2014. To search all
   recognised resources, instead of applying manual search, we perform an
   extensive automated search using Engineering Village. We identify 36
   relevant publications, including nine journal papers, two workshop
   papers, and 25 conference papers.
   Results: The software engineering community started using eye-trackers
   in the 1990s and they have become increasingly recognised as useful
   tools to conduct empirical studies from 2006. We observe that
   researchers use eye-trackers to study model comprehension, code
   comprehension, debugging, collaborative interaction, and traceability.
   Moreover, we find that studies use different metrics based on
   eye-movement data to obtain quantitative measures. We also report the
   limitations of current eye-tracking technology, which threaten the
   validity of previous studies, along with suggestions to mitigate these
   limitations.
   Conclusion: However, not withstanding these limitations and threats, we
   conclude that the advent of new eye-trackers makes the use of these
   tools easier and less obtrusive and that the software engineering
   community could benefit more from this technology. (C) 2015 Elsevier
   B.V. All rights reserved.}},
Publisher = {{ELSEVIER SCIENCE BV}},
Address = {{PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS}},
Type = {{Review}},
Language = {{English}},
Affiliation = {{Soh, Z (Reprint Author), Polytech Montreal, Ptidej Team, Dept Genie Informat \& Genie Logiciel, Quebec City, PQ, Canada.
   Sharafi, Zohreh; Soh, Zephyrin; Gueheneuc, Yann-Gael, Polytech Montreal, Ptidej Team, Dept Genie Informat \& Genie Logiciel, Quebec City, PQ, Canada.}},
DOI = {{10.1016/j.infsof.2015.06.008}},
ISSN = {{0950-5849}},
EISSN = {{1873-6025}},
Keywords = {{Eye-tracking; Software engineering; Experiment}},
Keywords-Plus = {{RESTRICTED FOCUS VIEWER; VISUAL-ATTENTION; MOVEMENTS; COMPREHENSION;
   FIXATIONS; VECTOR; TOOL}},
Research-Areas = {{Computer Science}},
Web-of-Science-Categories  = {{Computer Science, Information Systems; Computer Science, Software
   Engineering}},
Author-Email = {{zohreh.sharafi@polymtl.ca
   zephyrin.soh@polymtl.ca
   yann-gael.gueheneuc@polymtl.ca}},
ResearcherID-Numbers = {{Gueheneuc, Yann-Gael/K-9371-2019}},
Funding-Acknowledgement = {{Fonds de Recherche du Quebec Nature et Technologies (FQRNT); Canada
   Research Chairs on Software Patterns and Patterns of Software and on
   Software Change and Evolution}},
Funding-Text = {{We thank the editor and anonymous reviewers for their constructive
   comments, which helped us to improve the manuscript. This work has been
   partly funded by the Canada Research Chairs on Software Patterns and
   Patterns of Software and on Software Change and Evolution and by Fonds
   de Recherche du Quebec Nature et Technologies (FQRNT).}},
Number-of-Cited-References = {{53}},
Times-Cited = {{21}},
Usage-Count-Last-180-days = {{4}},
Usage-Count-Since-2013 = {{47}},
Journal-ISO = {{Inf. Softw. Technol.}},
Doc-Delivery-Number = {{CT2AO}},
Unique-ID = {{ISI:000362604600006}},
DA = {{2019-10-28}},
}

@article{ ISI:000363886100004,
Author = {Horvath, Klara and Myers, Kyle and Foster, Russell and Plunkett, Kim},
Title = {{Napping facilitates word learning in early lexical development}},
Journal = {{JOURNAL OF SLEEP RESEARCH}},
Year = {{2015}},
Volume = {{24}},
Number = {{5}},
Pages = {{503-509}},
Month = {{OCT}},
Abstract = {{Little is known about the role that night-time sleep and daytime naps
   play in early cognitive development. Our aim was to investigate how
   napping affects word learning in 16-month-olds. Thirty-four typically
   developing infants were assigned randomly to nap and wake groups. After
   teaching two novel object word pairs to infants, we tested their initial
   performance with an intermodal preferential looking task in which
   infants are expected to increase their target looking time compared to a
   distracter after hearing its auditory label. A second test session
   followed after approximately a 2-h delay. The delay contained sleep for
   the nap group or no sleep for the wake group. Looking behaviour was
   measured with an automatic eye-tracker. Vocabulary size was assessed
   using the Oxford Communicative Development Inventory. A significant
   interaction between group and session was found in preferential looking
   towards the target picture. The performance of the nap group increased
   after the nap, whereas that of the wake group did not change. The gain
   in performance correlated positively with the expressive vocabulary size
   in the nap group. These results indicate that daytime napping helps
   consolidate word learning in infancy.}},
Publisher = {{WILEY}},
Address = {{111 RIVER ST, HOBOKEN 07030-5774, NJ USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Horvath, K (Reprint Author), Univ Oxford, Dept Expt Psychol, S Parks Rd, Oxford OX1 3UD, England.
   Horvath, Klara; Myers, Kyle; Plunkett, Kim, Univ Oxford, Dept Expt Psychol, Oxford OX1 3UD, England.
   Foster, Russell, Univ Oxford, Nuffield Dept Clin Neurosci, Oxford OX1 3UD, England.}},
DOI = {{10.1111/jsr.12306}},
ISSN = {{0962-1105}},
EISSN = {{1365-2869}},
Keywords = {{daytime sleep; infancy; language acquisition; memory; sleep-dependent
   memory consolidation}},
Keywords-Plus = {{MEMORY CONSOLIDATION; LANGUAGE-DEVELOPMENT; SPOKEN WORDS; SLEEP;
   VOCABULARY; CHILDREN; INTEGRATION; COMPREHENSION}},
Research-Areas = {{Neurosciences \& Neurology}},
Web-of-Science-Categories  = {{Clinical Neurology; Neurosciences}},
Author-Email = {{klara.horvath@psy.ox.ac.uk}},
ResearcherID-Numbers = {{Horvath, Klara/B-9712-2012}},
ORCID-Numbers = {{Horvath, Klara/0000-0002-8198-9345}},
Number-of-Cited-References = {{34}},
Times-Cited = {{25}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{19}},
Journal-ISO = {{J. Sleep Res.}},
Doc-Delivery-Number = {{CU9ST}},
Unique-ID = {{ISI:000363886100004}},
OA = {{Bronze}},
DA = {{2019-10-28}},
}

@article{ ISI:000359880000002,
Author = {Chen, Yuping and Tsai, Meng-Jung},
Title = {{Eye-hand coordination strategies during active video game playing: An
   eye-tracking study}},
Journal = {{COMPUTERS IN HUMAN BEHAVIOR}},
Year = {{2015}},
Volume = {{51}},
Number = {{A}},
Pages = {{8-14}},
Month = {{OCT}},
Abstract = {{The purpose of this exploratory study was to examine the eye-hand
   coordination patterns while playing two virtual-reality active video
   games in healthy children and adults. Eleven children (mean age 8.09
   years) and ten adults participated in the study. Each participant played
   two digital games, Slap Stream and Kung Foo, from EyeToy Play software.
   Eye movements were recorded using Mobile Eye eye-tracker. Eye-hand
   coordination strategies and the time when virtual object appeared, the
   gaze shifted to the object, the reach started, the gaze shifted away,
   and the reach ended were coded from the video. The latencies between
   these events were computed and compared between adults and children and
   between games. The fixation duration, number of fixations, and number of
   gaze points were also computed for each game's areas of interests.
   Results showed that (1) all participants used multiple eye-hand
   strategies while playing active video games with some strategies more
   than others; (2) the Kung Foo game (with one target appearing on the
   screen) and the Slap Stream game (with potentially multiple targets
   appearing on the screen) induced different latencies and gaze points
   between children and adults; and (3) children had longer latencies and
   shorter fixation durations than adults. The study thus provides in-depth
   understanding of different patterns of eye-hand coordination in
   relations to active video game playing. The significant differences in
   coordinative control strategies found between adults and children as
   well as between game types provide a basis for further research in both
   child development and game-based learning fields. (C) 2015 Elsevier Ltd.
   All rights reserved.}},
Publisher = {{PERGAMON-ELSEVIER SCIENCE LTD}},
Address = {{THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Tsai, MJ (Reprint Author), Natl Taiwan Univ Sci \& Technol, Grad Inst Digital Learning \& Educ, 43,Sec 4,Keelung Rd, Taipei 106, Taiwan.
   Chen, Yuping, Georgia State Univ, Dept Phys Therapy, Atlanta, GA 30303 USA.
   Tsai, Meng-Jung, Natl Taiwan Univ Sci \& Technol, Grad Inst Digital Learning \& Educ, Taipei 106, Taiwan.}},
DOI = {{10.1016/j.chb.2015.04.045}},
ISSN = {{0747-5632}},
EISSN = {{1873-7692}},
Keywords = {{Children; Eye movements; Eye-hand coordination; Game-based learning;
   Visual strategy}},
Keywords-Plus = {{MOVEMENTS; SCIENCE; TECHNOLOGY; VISION}},
Research-Areas = {{Psychology}},
Web-of-Science-Categories  = {{Psychology, Multidisciplinary; Psychology, Experimental}},
Author-Email = {{mjtsai99@mail.ntust.edu.tw}},
ORCID-Numbers = {{Tsai, Meng-Jung/0000-0002-8994-861X
   Chen, Yu-ping/0000-0001-7127-8323}},
Funding-Acknowledgement = {{Ministry of Science and Technology in TaiwanMinistry of Science and
   Technology, Taiwan {[}MOST 103-2511-S-011-002-MY3, MOST
   103-2511-S-011-005-MY3]}},
Funding-Text = {{This study was supported by the Ministry of Science and Technology in
   Taiwan under the projects of MOST 103-2511-S-011-002-MY3 and MOST
   103-2511-S-011-005-MY3. We thank Amy Harrod and Youqun Luo for help in
   scoring the video data.}},
Number-of-Cited-References = {{29}},
Times-Cited = {{10}},
Usage-Count-Last-180-days = {{2}},
Usage-Count-Since-2013 = {{60}},
Journal-ISO = {{Comput. Hum. Behav.}},
Doc-Delivery-Number = {{CP4VF}},
Unique-ID = {{ISI:000359880000002}},
DA = {{2019-10-28}},
}

@article{ ISI:000358728200011,
Author = {Krogh-Jespersen, Sheila and Liberman, Zoe and Woodward, Amanda L.},
Title = {{Think fast! The relationship between goal prediction speed and social
   competence in infants}},
Journal = {{DEVELOPMENTAL SCIENCE}},
Year = {{2015}},
Volume = {{18}},
Number = {{5}},
Pages = {{815-823}},
Month = {{SEP}},
Abstract = {{Skilled social interactions require knowledge about others' intentions
   and the ability to implement this knowledge in real-time to generate
   appropriate responses to one's partner. Young infants demonstrate an
   understanding of other people's intentions (e.g. Woodward, Sommerville,
   Gerson, Henderson \& Buresh, 2009), yet it is not until the second year
   that infants seem to master the real-time implementation of their
   knowledge during social interactions (e.g. Warneken \& Tomasello, 2007).
   The current study investigates the possibility that developments in
   social competence during the second year are related to increases in the
   speed with which infants can employ their understanding of others'
   intentions. Twenty- to 22-month-old infants (N=23) viewed videos of
   goal-directed actions on a Tobii eye-tracker and then engaged in an
   interactive perspective-taking task. Infants who quickly and accurately
   anticipated another person's future behavior in the eye-tracking task
   were more successful at taking their partner's perspective in the social
   interaction. Success on the perspective-taking task was specifically
   related to the ability to correctly predict another person's intentions.
   These findings highlight the importance of not only being a smart'
   social partner but also a fast' social thinker.}},
Publisher = {{WILEY-BLACKWELL}},
Address = {{111 RIVER ST, HOBOKEN 07030-5774, NJ USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Krogh-Jespersen, S (Reprint Author), Univ Chicago, Dept Psychol, Chicago, IL 60637 USA.
   Krogh-Jespersen, Sheila; Liberman, Zoe; Woodward, Amanda L., Univ Chicago, Dept Psychol, Chicago, IL 60637 USA.}},
DOI = {{10.1111/desc.12249}},
ISSN = {{1363-755X}},
EISSN = {{1467-7687}},
Keywords-Plus = {{PERSPECTIVE-TAKING; DIRECTED ACTION; OBJECT; MIND; AGE; COOPERATION;
   ATTENTION; LOOKING; OTHERS; SEE}},
Research-Areas = {{Psychology}},
Web-of-Science-Categories  = {{Psychology, Developmental; Psychology, Experimental}},
Author-Email = {{skrogh@uchicago.edu}},
ResearcherID-Numbers = {{Liberman, Zoe/V-4580-2019
   }},
ORCID-Numbers = {{Liberman, Zoe/0000-0001-8923-2358}},
Funding-Acknowledgement = {{NICHDUnited States Department of Health \& Human ServicesNational
   Institutes of Health (NIH) - USANIH Eunice Kennedy Shriver National
   Institute of Child Health \& Human Development (NICHD) {[}P01 HD064653];
   NSFNational Science Foundation (NSF) {[}DLS 0951489]; NSF GRPF}},
Funding-Text = {{This research was supported by grants to A.L. Woodward from NICHD (P01
   HD064653) and NSF (DLS 0951489) and a NSF GRPF to Z. Liberman. We thank
   the families who participated in the current study, as well as Taylor
   Hayes for her help with data analysis.}},
Number-of-Cited-References = {{31}},
Times-Cited = {{12}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{15}},
Journal-ISO = {{Dev. Sci.}},
Doc-Delivery-Number = {{CN8XE}},
Unique-ID = {{ISI:000358728200011}},
OA = {{Green Accepted}},
DA = {{2019-10-28}},
}

@article{ ISI:000358971200001,
Author = {Mawad, Franco and Trias, Marcela and Gimenez, Ana and Maiche, Alejandro
   and Ares, Gaston},
Title = {{Influence of cognitive style on information processing and selection of
   yogurt labels: Insights from an eye-tracking study}},
Journal = {{FOOD RESEARCH INTERNATIONAL}},
Year = {{2015}},
Volume = {{74}},
Pages = {{1-9}},
Month = {{AUG}},
Abstract = {{Cognitive styles are characteristic and stable ways in which people
   acquire, organize and use information for solving problems and making
   decisions. Field dependence/independence is one of the most studied
   cognitive styles. Field independent subjects are characterized by having
   less difficulty in separating information from its contextual
   surroundings and being less likely to be influenced by external cues
   than field dependent individuals. The present work aimed at studying the
   influence of field dependence/independence cognitive style on consumers'
   visual processing and choice of yogurt labels. One hundred and thirty
   three consumers completed a choice conjoint task. They were asked to
   select their preferred yogurt label from each of 16 pairs of labels.
   While they completed the task their eye movements were recorded using an
   eye-tracker. Then, consumers were asked to complete the Group Embedded
   Figure Test to determine their cognitive style. Consumers were divided
   into two groups with different cognitive styles: 58\% of the sample was
   characterized as field dependent and 42\% as field independent. When
   making their choices, field dependent consumers tended to engage in less
   thoughtful information processing than field independent consumers and
   they made fewer fixations on traditional nutritional information.
   Besides, cognitive style significantly affected the relative importance
   of fat and sugar content on consumer choices and modulated the influence
   of the traffic light system. Field dependent consumers gave less
   importance to the nutritional composition of the yogurts than field
   independent consumers for selecting their preferred label. Results from
   this work suggest that studying the psychological underpinnings of
   consumers' decision making process when selecting food products has a
   great potential to contribute to a better understanding of how eating
   patterns and consumer preferences are shaped. (C) 2015 Elsevier Ltd. All
   rights reserved.}},
Publisher = {{ELSEVIER SCIENCE BV}},
Address = {{PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Ares, G (Reprint Author), Univ Republ UdelaR, Fac Psicol, Ctr Invest Basica Psicol CIBPsi, Tristan Narvaja 1674, Montevideo, Uruguay.
   Mawad, Franco; Trias, Marcela; Gimenez, Ana; Maiche, Alejandro; Ares, Gaston, Univ Republ UdelaR, Fac Psicol, Ctr Invest Basica Psicol CIBPsi, Montevideo, Uruguay.
   Gimenez, Ana; Ares, Gaston, Univ Republ UdelaR, Fac Quim, Dept Ciencia \& Tecnol Alimentos, Montevideo, Uruguay.}},
DOI = {{10.1016/j.foodres.2015.04.023}},
ISSN = {{0963-9969}},
EISSN = {{1873-7145}},
Keywords = {{Food choice; Food labels; Nutritional information; Front-of-pack;
   Traffic light system}},
Keywords-Plus = {{FIELD DEPENDENCE-INDEPENDENCE; FOOD LABELS; THINKING STYLES;
   RATIONALITY; PERCEPTION; PREVENTION; PSYCHOLOGY; STRATEGIES; DECISIONS;
   QUALITY}},
Research-Areas = {{Food Science \& Technology}},
Web-of-Science-Categories  = {{Food Science \& Technology}},
Author-Email = {{gares@fq.edu.uy}},
Funding-Acknowledgement = {{Espacio Interdisciplinario and Comision Sectorial de Investigacion
   Cientlfica (Universidad de la Republica, Uruguay)}},
Funding-Text = {{The authors are thankful to Espacio Interdisciplinario and Comision
   Sectorial de Investigacion Cientlfica (Universidad de la Republica,
   Uruguay) for financial support.}},
Number-of-Cited-References = {{61}},
Times-Cited = {{22}},
Usage-Count-Last-180-days = {{2}},
Usage-Count-Since-2013 = {{60}},
Journal-ISO = {{Food Res. Int.}},
Doc-Delivery-Number = {{CO2FJ}},
Unique-ID = {{ISI:000358971200001}},
DA = {{2019-10-28}},
}

@article{ ISI:000361253200018,
Author = {Hyun, Kyung Hoon and Lee, Ji-Hyun and Kim, Minki and Cho, Sulah},
Title = {{Style synthesis and analysis of car designs for style quantification
   based on product appearance similarities}},
Journal = {{ADVANCED ENGINEERING INFORMATICS}},
Year = {{2015}},
Volume = {{29}},
Number = {{3}},
Pages = {{483-494}},
Month = {{AUG}},
Abstract = {{Understanding how similar design appears is a key element to
   understanding companies' design strategies. However, it is difficult to
   evaluate companies' design strategies with conventional style
   measurement methods since they only taxonomically measure whether a
   specific characteristic is included in a specific style. This study
   numerically measured car design similarities to synthesize and analyze
   car brand styles, thereupon discovering the design trends among car
   brands for strategic design positioning. This paper aims to find methods
   for quantifying style differences and identifying unique design elements
   of car designs among 23 automobile manufacturers based on design
   similarities of a large quantity of car designs (N = 119). To achieve
   this goal, a hybrid style quantification methodology a mixture of
   Fourier decomposition, eye tracker, and shape grammar was created to
   evaluate similarities, visual significance, and combinations of 19 car
   design elements. Fourier decomposition was incorporated to find the
   quantifiable values of design similarities of car design elements.
   Visual significance analysis was also conducted for each car design
   element through eye tracker to measure the importance of certain design
   elements for weighting factors. Then, each combination of design
   elements was compared with car design elements of other cars for
   similarity calculations. Finally, car design alternatives were
   synthesized, and transitions of design positioning were analyzed based
   on the similarity values weighed by the visual significance results.
   Using the suggested methods, alternate designs can be synthesized while
   preserving brands' design styles, and design trends can be analyzed for
   strategic evaluation. (C) 2015 Elsevier Ltd. All rights reserved.}},
Publisher = {{ELSEVIER SCI LTD}},
Address = {{THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Lee, JH (Reprint Author), Korea Adv Inst Sci \& Technol, Grad Sch Culture Technol, Taejon 305701, South Korea.
   Hyun, Kyung Hoon; Lee, Ji-Hyun, Korea Adv Inst Sci \& Technol, Grad Sch Culture Technol, Taejon 305701, South Korea.
   Kim, Minki, Korea Adv Inst Sci \& Technol, Grad Sch Management, Seoul 130722, South Korea.
   Cho, Sulah, Korea Adv Inst Sci \& Technol, Dept Business \& Technol, Taejon 305701, South Korea.}},
DOI = {{10.1016/j.aei.2015.04.001}},
ISSN = {{1474-0346}},
EISSN = {{1873-5320}},
Keywords = {{Style analysis; Design management; Car design; Design Strategy; Brand
   association}},
Keywords-Plus = {{SHAPE; MODEL; TECHNOLOGY; INNOVATION}},
Research-Areas = {{Computer Science; Engineering}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Engineering,
   Multidisciplinary}},
Author-Email = {{jihyunlee@kaist.ac.kr}},
ResearcherID-Numbers = {{Kim, Minki/C-1851-2011
   Lee, Ji-Hyun/C-1685-2011
   }},
ORCID-Numbers = {{Hyun, Kyung Hoon/0000-0001-6379-9700}},
Number-of-Cited-References = {{48}},
Times-Cited = {{15}},
Usage-Count-Last-180-days = {{6}},
Usage-Count-Since-2013 = {{28}},
Journal-ISO = {{Adv. Eng. Inform.}},
Doc-Delivery-Number = {{CR3SK}},
Unique-ID = {{ISI:000361253200018}},
DA = {{2019-10-28}},
}

@article{ ISI:000353713200008,
Author = {Vine, Samuel J. and Uiga, Liis and Lavric, Aureliu and Moore, Lee J. and
   Tsaneva-Atanasova, Krasimira and Wilson, Mark R.},
Title = {{Individual reactions to stress predict performance during a critical
   aviation incident}},
Journal = {{ANXIETY STRESS AND COPING}},
Year = {{2015}},
Volume = {{28}},
Number = {{4}},
Pages = {{467-477}},
Month = {{JUL 4}},
Abstract = {{Background: Understanding the influence of stress on human performance
   is of theoretical and practical importance. An individual's reaction to
   stress predicts their subsequent performance; with a ``challenge{''}
   response to stress leading to better performance than a ``threat{''}
   response. However, this contention has not been tested in truly
   stressful environments with highly skilled individuals. Furthermore, the
   effect of challenge and threat responses on attentional control during
   visuomotor tasks is poorly understood. Design: Thus, this study aimed to
   examine individual reactions to stress and their influence on
   attentional control, among a cohort of commercial pilots performing a
   stressful flight assessment. Methods: Sixteen pilots performed an
   ``engine failure on take-off{''} scenario, in a high-fidelity flight
   simulator. Reactions to stress were indexed via self-report; performance
   was assessed subjectively (flight instructor assessment) and objectively
   (simulator metrics); gaze behavior data were captured using a mobile eye
   tracker, and measures of attentional control were subsequently
   calculated (search rate, stimulus driven attention, and entropy).
   Results: Hierarchical regression analyses revealed that a threat
   response was associated with poorer performance and disrupted
   attentional control. Conclusion: The findings add to previous research
   showing that individual reactions to stress influence performance and
   shed light on the processes through which stress influences performance.}},
Publisher = {{TAYLOR \& FRANCIS LTD}},
Address = {{4 PARK SQUARE, MILTON PARK, ABINGDON OX14 4RN, OXON, ENGLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Vine, SJ (Reprint Author), Univ Exeter, Coll Life \& Environm Sci, St Lukes Campus,Heavitree Rd, Exeter EX1 2LU, Devon, England.
   Vine, Samuel J.; Lavric, Aureliu; Wilson, Mark R., Univ Exeter, Coll Life \& Environm Sci, Exeter EX1 2LU, Devon, England.
   Uiga, Liis, Univ Hong Kong, Inst Human Performance, Hong Kong, Hong Kong, Peoples R China.
   Moore, Lee J., Univ Gloucestershire, Sch Sport \& Exercise, Gloucester GL2 9HW, England.
   Tsaneva-Atanasova, Krasimira, Univ Exeter, Coll Engn, Math \& Phys Sci, Exeter EX4 4QF, Devon, England.}},
DOI = {{10.1080/10615806.2014.986722}},
ISSN = {{1061-5806}},
EISSN = {{1477-2205}},
Keywords = {{biopsychosocial model; attention; eye tracking; challenge and threat;
   pilot}},
Keywords-Plus = {{THREAT STATES; CARDIOVASCULAR INDEXES; ATTENTIONAL CONTROL; CHALLENGE;
   ANXIETY; BEHAVIOR; EFFICIENCY; APPRAISAL; ERROR}},
Research-Areas = {{Neurosciences \& Neurology; Psychiatry; Psychology}},
Web-of-Science-Categories  = {{Neurosciences; Psychiatry; Psychology, Multidisciplinary}},
Author-Email = {{s.j.vine@exeter.ac.uk}},
ResearcherID-Numbers = {{Moore, Lee/X-4774-2019
   }},
ORCID-Numbers = {{Moore, Lee/0000-0001-7394-7762
   Tsaneva-Atanasova, Krasimira/0000-0002-6294-7051
   Vine, Samuel/0000-0001-9329-1262
   Wilson, Mark/0000-0001-8145-6971
   Uiga, Liis/0000-0002-5371-9428}},
Funding-Acknowledgement = {{Higher Education Funding Council for England (HEFCE)Higher Education
   Funding Council for England}},
Funding-Text = {{This research was funded by a grant from the Higher Education Funding
   Council for England (HEFCE) awarded to the first and third authors.}},
Number-of-Cited-References = {{34}},
Times-Cited = {{12}},
Usage-Count-Last-180-days = {{2}},
Usage-Count-Since-2013 = {{47}},
Journal-ISO = {{Anxiety Stress Coping}},
Doc-Delivery-Number = {{CH0MB}},
Unique-ID = {{ISI:000353713200008}},
OA = {{Green Accepted}},
DA = {{2019-10-28}},
}

@article{ ISI:000351248400016,
Author = {Preisig, Basil C. and Eggenberger, Noemi and Zito, Giuseppe and
   Vanbellingen, Tim and Schumacher, Rahel and Hopfner, Simone and
   Nyffeler, Thomas and Gutbrod, Klemens and Annoni, Jean-Marie and
   Bohlhalter, Stephan and Mueri, Rene M.},
Title = {{Perception of co-speech gestures in aphasic patients: A visual
   exploration study during the observation of dyadic conversations}},
Journal = {{CORTEX}},
Year = {{2015}},
Volume = {{64}},
Pages = {{157-168}},
Month = {{MAR}},
Abstract = {{Background: Co-speech gestures are part of nonverbal communication
   during conversations. They either support the verbal message or provide
   the interlocutor with additional information. Furthermore, they prompt
   as nonverbal cues the cooperative process of turn taking. In the present
   study, we investigated the influence of co-speech gestures on the
   perception of dyadic dialogue in aphasic patients. In particular, we
   analysed the impact of co-speech gestures on gaze direction (towards
   speaker or listener) and fixation of body parts. We hypothesized that
   aphasic patients, who are restricted in verbal comprehension, adapt
   their visual exploration strategies.
   Methods: Sixteen aphasic patients and 23 healthy control subjects
   participated in the study. Visual exploration behaviour was measured by
   means of a contact-free infrared eye-tracker while subjects were
   watching videos depicting spontaneous dialogues between two individuals.
   Cumulative fixation duration and mean fixation duration were calculated
   for the factors co-speech gesture (present and absent), gaze direction
   (to the speaker or to the listener), and region of interest (ROI),
   including hands, face, and body.
   Results: Both aphasic patients and healthy controls mainly fixated the
   speaker's face. We found a significant co-speech gesture x ROI
   interaction, indicating that the presence of a co-speech gesture
   encouraged subjects to look at the speaker. Further, there was a
   significant gaze direction x ROI x group interaction revealing that
   aphasic patients showed reduced cumulative fixation duration on the
   speaker's face compared to healthy controls.
   Conclusion: Co-speech gestures guide the observer's attention towards
   the speaker, the source of semantic input. It is discussed whether an
   underlying semantic processing deficit or a deficit to integrate
   audio-visual information may cause aphasic patients to explore less the
   speaker's face. (C) 2014 Elsevier Ltd. All rights reserved.}},
Publisher = {{ELSEVIER MASSON}},
Address = {{VIA PALEOCAPA 7, 20121 MILANO, ITALY}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Muri, RM (Reprint Author), Univ Hosp Bern, Inselspital, Dept Neurol, Percept \& Eye Movement Lab, Freiburgstr 10, CH-3010 Bern, Switzerland.
   Preisig, Basil C.; Eggenberger, Noemi; Vanbellingen, Tim; Schumacher, Rahel; Hopfner, Simone; Nyffeler, Thomas; Bohlhalter, Stephan; Mueri, Rene M., Univ Hosp Bern, Inselspital, Dept Neurol, Percept \& Eye Movement Lab, CH-3010 Bern, Switzerland.
   Preisig, Basil C.; Eggenberger, Noemi; Vanbellingen, Tim; Schumacher, Rahel; Hopfner, Simone; Nyffeler, Thomas; Bohlhalter, Stephan; Mueri, Rene M., Univ Hosp Bern, Inselspital, Dept Clin Res, CH-3010 Bern, Switzerland.
   Preisig, Basil C.; Eggenberger, Noemi; Vanbellingen, Tim; Schumacher, Rahel; Hopfner, Simone; Nyffeler, Thomas; Gutbrod, Klemens; Bohlhalter, Stephan; Mueri, Rene M., Univ Bern, CH-3012 Bern, Switzerland.
   Zito, Giuseppe, Univ Bern, ARTORG Ctr Biomed Engn Res, CH-3012 Bern, Switzerland.
   Vanbellingen, Tim; Nyffeler, Thomas; Bohlhalter, Stephan, Luzerner Kantonsspital, Neurol \& Neurorehabilitat Ctr, Dept Internal Med, Kantonsspital, Switzerland.
   Gutbrod, Klemens; Mueri, Rene M., Univ Hosp Bern, Inselspital, Dept Neurol, Div Cognit \& Restorat Neurol, CH-3010 Bern, Switzerland.
   Annoni, Jean-Marie, Univ Fribourg, Fac Sci, Dept Med, Neurol Unit,Lab Cognit \& Neurol Sci, CH-1700 Fribourg, Switzerland.
   Mueri, Rene M., Univ Bern, Gerontechnol \& Rehabil Grp, Bern, Switzerland.
   Mueri, Rene M., Univ Bern, Ctr Cognit Learning \& Memory, Bern, Switzerland.}},
DOI = {{10.1016/j.cortex.2014.10.013}},
ISSN = {{0010-9452}},
EISSN = {{1973-8102}},
Keywords = {{Gestures; Visual exploration; Dialogue; Aphasia; Apraxia; Eye movements}},
Keywords-Plus = {{SUPERIOR TEMPORAL SULCUS; AUDITORY VIGILANCE; ICONIC GESTURES;
   EYE-MOVEMENTS; LIMB APRAXIA; HAND GESTURE; LANGUAGE; RECOGNITION;
   INDIVIDUALS; ATTENTION}},
Research-Areas = {{Behavioral Sciences; Neurosciences \& Neurology; Psychology}},
Web-of-Science-Categories  = {{Behavioral Sciences; Neurosciences; Psychology, Experimental}},
Author-Email = {{Rene.mueri@insel.ch}},
ResearcherID-Numbers = {{Preisig, Basil/P-6823-2019
   Muri, Rene/E-9320-2012
   }},
ORCID-Numbers = {{Preisig, Basil/0000-0002-8148-3555
   Muri, Rene/0000-0001-6990-4188
   Schumacher, Rahel/0000-0001-7500-7491
   Zito, Giuseppe/0000-0002-5027-884X
   Gutbrod, Klemens/0000-0002-5723-3468}},
Funding-Acknowledgement = {{Swiss National Science FoundationSwiss National Science Foundation
   (SNSF) {[}320030\_138532/1]}},
Funding-Text = {{This study was supported by the Swiss National Science Foundation
   {[}Grant no. 320030\_138532/1]. We also would like to thank Sandra
   Perny, Susanne Zurrer, Julia Renggli, Marianne Tschirren, Corina Wyss,
   Carmen Schmid, Gabriella Steiner, Monica Koenig-Bruhin Nicole Williams,
   Reto Hanni, Gianni Pauciello, Silvia Burren, Andreas Loffel, Michael
   Schraner, Nina Kohler, Anita Mani-Luginbuhl, Hans Witschi, Sarah
   Schaefer, Martin Zurcher, and Michael Rath for their assistance.}},
Number-of-Cited-References = {{68}},
Times-Cited = {{5}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{19}},
Journal-ISO = {{Cortex}},
Doc-Delivery-Number = {{CD7CM}},
Unique-ID = {{ISI:000351248400016}},
OA = {{Green Published}},
DA = {{2019-10-28}},
}

@article{ ISI:000353991300004,
Author = {Alsadoon, Reem and Heift, Trude},
Title = {{Textual Input Enhancement for Vowel Blindness: A Study with Arabic ESL
   Learners}},
Journal = {{MODERN LANGUAGE JOURNAL}},
Year = {{2015}},
Volume = {{99}},
Number = {{1}},
Pages = {{57-79}},
Month = {{SPR}},
Abstract = {{This study explores the impact of textual input enhancement on the
   noticing and intake of English vowels by Arabic L2 learners of English.
   Arabic L1 speakers are known to experience vowel blindness, commonly
   defined as a difficulty in the textual decoding and encoding of English
   vowels due to an insufficient decoding of the word form. Thirty beginner
   ESL learners participated in a training study during which the
   experimental group received textual input enhancement on English vowels.
   Students completed a pretest and an immediate and delayed posttest. An
   eye-tracker recorded students' eye fixations during the treatment phase.
   Results indicate that vowel blindness was significantly reduced for the
   experimental group who received vowel training in the form of textual
   input enhancement. This might be due to a longer focus on the target
   words as suggested by our eye-tracking data.}},
Publisher = {{WILEY}},
Address = {{111 RIVER ST, HOBOKEN 07030-5774, NJ USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Alsadoon, R (Reprint Author), Simon Fraser Univ, Dept Linguist, Univ Dr 8888, Burnaby, BC V5A 1S6, Canada.
   Alsadoon, Reem; Heift, Trude, Simon Fraser Univ, Dept Linguist, Burnaby, BC V5A 1S6, Canada.}},
DOI = {{10.1111/modl.12188}},
ISSN = {{0026-7902}},
EISSN = {{1540-4781}},
Keywords = {{Vowel blindness; textual input enhancement; eye tracking; Arabic ESL
   learners}},
Keywords-Plus = {{EYE-MOVEMENTS; CONSCIOUSNESS; COMPREHENSION; ACQUISITION; ATTENTION;
   FORM; L2}},
Research-Areas = {{Education \& Educational Research; Linguistics}},
Web-of-Science-Categories  = {{Education \& Educational Research; Linguistics}},
Author-Email = {{ralsadoo@sfu.ca
   heift@sfu.ca}},
Funding-Acknowledgement = {{Social Sciences and Humanities Research Council, Canada {[}632209]; Imam
   Mohammed bin Saud University}},
Funding-Text = {{The authors would like to thank the anonymous reviewers for their
   invaluable feedback on earlier drafts of the article. We express our
   thanks as well for funding support from the Social Sciences and
   Humanities Research Council, Canada, grant 632209 and from Imam Mohammed
   bin Saud University for their financial contribution to the development
   of VALE.}},
Number-of-Cited-References = {{61}},
Times-Cited = {{2}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{7}},
Journal-ISO = {{Mod. Lang. J.}},
Doc-Delivery-Number = {{CH4GG}},
Unique-ID = {{ISI:000353991300004}},
DA = {{2019-10-28}},
}

@article{ ISI:000346849800004,
Author = {Bolden, David and Barmby, Patrick and Raine, Stephanie and Gardner,
   Matthew},
Title = {{How young children view mathematical representations: a study using
   eye-tracking technology}},
Journal = {{EDUCATIONAL RESEARCH}},
Year = {{2015}},
Volume = {{57}},
Number = {{1}},
Pages = {{59-79}},
Month = {{JAN 2}},
Abstract = {{Background: It has been shown that mathematical representations can aid
   children's understanding of mathematical concepts but that children can
   sometimes have difficulty in interpreting them correctly. New advances
   in eye-tracking technology can help in this respect because it allows
   data to be gathered concerning children's focus of attention and so
   indicate on what aspects of the representations they are focussing.
   However, recent eye-tracking technology has not been used to any great
   degree in investigating the way children view and interpret mathematical
   representations.
   Purpose: This research explored the use of new advances in eye-tracking
   technology in investigating how young children view and interpret
   mathematical representations of multiplication.
   Sample: Nine Year 5 children (four boys, five girls, aged 9-10years of
   age) from a local primary (elementary) school in the North-East of
   England were asked to complete the test during school time. The children
   represented a range of attainment levels across the mathematical domain
   (three higher-, three middle- and three lower-attaining children) and
   were selected accordingly by their class teacher. We recognise that this
   study was only based on a small sample of children, however, this number
   still allowed us to make meaningful comparisons in particular between
   the different types of representations presented.
   Design and methods: The study consisted of each child looking at 18
   static slides, one after the other, with each slide presenting a
   symbolic and a picture representation of multiplication problems. The
   data that was captured by the eye tracker and recorded was then analysed
   quantitatively (e.g. time on each slide, time on each area of interest
   specified within the software) and qualitatively (video recordings of
   each child's gaze trajectory during each representation was carried out,
   thereby allowing a categorisation of the different approaches adopted).
   Results: The study showed that (a) the particular form of the number
   line representation used in this study was less successful than the
   other picture representations used (equal groups, array) in promoting
   multiplicative thinking in children, and (b) the success of children to
   think multiplicatively with the `groups' and the array representation
   was related to their general mathematics attainment levels.
   Conclusion: These findings have implications for teacher practice in
   that teachers need to be clear about the possible drawbacks of
   particular representations. Even in using more successful
   representations, for lower-attaining children, the progression in their
   understanding of the representation needs to be taken into account by
   the teacher. The study also highlighted that the eye-tracking technology
   does have some limitations but is useful in investigating young
   children's focus of attention whilst undertaking a mathematics
   assessment task.}},
Publisher = {{ROUTLEDGE JOURNALS, TAYLOR \& FRANCIS LTD}},
Address = {{4 PARK SQUARE, MILTON PARK, ABINGDON OX14 4RN, OXFORDSHIRE, ENGLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Barmby, P (Reprint Author), Univ Witwatersrand, Sch Educ, Johannesburg, South Africa.
   Bolden, David; Raine, Stephanie; Gardner, Matthew, Univ Durham, Sch Educ, Durham, England.
   Barmby, Patrick, Univ Witwatersrand, Sch Educ, Johannesburg, South Africa.}},
DOI = {{10.1080/00131881.2014.983718}},
ISSN = {{0013-1881}},
EISSN = {{1469-5847}},
Keywords = {{multiplication; primary; representations; eye-tracking; mathematics}},
Keywords-Plus = {{ARITHMETIC WORD-PROBLEMS; MOVEMENTS; COMPREHENSION}},
Research-Areas = {{Education \& Educational Research}},
Web-of-Science-Categories  = {{Education \& Educational Research}},
Author-Email = {{patrick.barmby@wits.ac.za}},
ORCID-Numbers = {{Bolden, David/0000-0002-2717-7266}},
Number-of-Cited-References = {{49}},
Times-Cited = {{7}},
Usage-Count-Last-180-days = {{3}},
Usage-Count-Since-2013 = {{44}},
Journal-ISO = {{Educ. Res.}},
Doc-Delivery-Number = {{AX3OY}},
Unique-ID = {{ISI:000346849800004}},
OA = {{Green Accepted}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000387959201010,
Author = {Jiang, Ming and Huang, Shengsheng and Duan, Juanyong and Zhao, Qi},
Book-Group-Author = {{IEEE}},
Title = {{SALICON: Saliency in Context}},
Booktitle = {{2015 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)}},
Series = {{IEEE Conference on Computer Vision and Pattern Recognition}},
Year = {{2015}},
Pages = {{1072-1080}},
Note = {{IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
   Boston, MA, JUN 07-12, 2015}},
Organization = {{IEEE}},
Abstract = {{Saliency in Context (SALICON) is an ongoing effort that aims at
   understanding and predicting visual attention. This paper presents a new
   method to collect large-scale human data during natural explorations on
   images. While current datasets present a rich set of images and
   task-specific annotations such as category labels and object segments,
   this work focuses on recording and logging how humans shift their
   attention during visual exploration. The goal is to offer new
   possibilities to (I) complement task-specific annotations to advance the
   ultimate goal in visual understanding, and (2) understand visual
   attention and learn saliency models, all with human attentional data at
   a much larger scale.
   We designed a mouse-contingent multi-resolutional paradigm based on
   neurophysiological and psychophysical studies of peripheral vision, to
   simulate the natural viewing behavior of humans. The new paradigm
   allowed using a general-purpose mouse instead of an eye tracker to
   record viewing behaviors, thus enabling large-scale data collection. The
   paradigm was validated with controlled laboratory as well as large-scale
   online data. We report in this paper a proof-of-concept SALICON dataset
   of human `free viewing{''} data on 10,000 images from the Microsoft COCO
   (MS COCO) dataset with rich contextual information. We evaluated the use
   of the collected data in the context of saliency prediction, and
   demonstrated them a good source as ground truth for the evaluation of
   saliency algorithms.}},
Publisher = {{IEEE}},
Address = {{345 E 47TH ST, NEW YORK, NY 10017 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Zhao, Q (Reprint Author), Natl Univ Singapore, Dept Elect \& Comp Engn, Singapore 117548, Singapore.
   Jiang, Ming; Huang, Shengsheng; Duan, Juanyong; Zhao, Qi, Natl Univ Singapore, Dept Elect \& Comp Engn, Singapore 117548, Singapore.}},
ISSN = {{1063-6919}},
ISBN = {{978-1-4673-6964-0}},
Research-Areas = {{Computer Science}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence}},
Author-Email = {{mjiang@nus.edu.sg
   shane.huang@nus.edu.sg
   j.duan@u.nus.edu
   elegiz@nus.edu.sg}},
ResearcherID-Numbers = {{Jiang, Ming/I-1536-2016}},
ORCID-Numbers = {{Jiang, Ming/0000-0001-6439-5476}},
Number-of-Cited-References = {{38}},
Times-Cited = {{56}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{1}},
Doc-Delivery-Number = {{BG3KA}},
Unique-ID = {{ISI:000387959201010}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000383740305107,
Author = {Li, Wen-Chin and Yu, Chung-San and Greaves, Matthew and Braithwaite,
   Graham},
Editor = {{Ahram, T and Karwowski, W and Schmorrow, D}},
Title = {{How Cockpit Design Impacts Pilots' Attention Distribution and Perceived
   Workload during Aiming a Stationary Target}},
Booktitle = {{6TH INTERNATIONAL CONFERENCE ON APPLIED HUMAN FACTORS AND ERGONOMICS
   (AHFE 2015) AND THE AFFILIATED CONFERENCES, AHFE 2015}},
Series = {{Procedia Manufacturing}},
Year = {{2015}},
Volume = {{3}},
Pages = {{5663-5669}},
Note = {{6th International Conference on Applied Human Factors and Ergonomics
   (AHFE), Las Vegas, NV, JUL 26-30, 2015}},
Abstract = {{The eye movement data in five areas of interest (AOIs) were analyzed as
   follows, Head-up Display (HUD); Integrated Control Panel (ICP); Right
   Multiple Function Display (RMFD); Left Multiple Function Display (LMFD);
   and Outside of cockpit (OC). The scenario is performing an
   air-to-surface task to aim at a stationary target. The results show
   significant differences in pilots' percentage of fixation between two
   interface designs on the ICP (t=-3.36, p<.005, Cohen's d=-.98); RMFD
   (t=-4.85, p<.001, Cohen's d=-1.55) and LMFD (t=-2.56, p<.05, Cohen's
   d=-.79). There were significant differences in pilots' fixation duration
   between two interfaces on the HUD (t=2.64, p<.05, Cohen's d=.81); ICP
   (t=-3.00, p<.005, Cohen's d=-.94); RMFD (t=-5.32, p<.001, Cohen's
   d=-1.65) and LMFD (t=-2.77, p<.05, Cohen's d=-.92). By the application
   of eye tracker devices, interface designers can precisely evaluate
   pilots' visual behavior among interfaces of cockpit and SA performance.
   In addition, extra workload might have a negative impact on pilots' SA
   performance and increase the probability of operating hazards, and so
   there is the opportunity to compensate for the negative impact of
   workload through human-centered design. The current research uses
   eye-tracking devices to investigate pilots' visual behaviors and
   interface design and has potential to facilitate system designers'
   understanding of pilots' attention distribution and situational
   awareness for improving the integration of cockpit designs and
   ultimately aviation safety. (C) 2015 The Authors. Published by Elsevier
   B.V.}},
Publisher = {{ELSEVIER SCIENCE BV}},
Address = {{SARA BURGERHARTSTRAAT 25, PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Li, WC (Reprint Author), Cranfield Univ, Safety \& Accid Invest Ctr, Cranfield MK43 0TR, Beds, England.
   Li, Wen-Chin; Greaves, Matthew; Braithwaite, Graham, Cranfield Univ, Safety \& Accid Invest Ctr, Cranfield MK43 0TR, Beds, England.
   Yu, Chung-San, Natl Tsing Hua Univ, Dept Ind Engn \& Engn Management, Hsinchu 30013, Taiwan.}},
DOI = {{10.1016/j.promfg.2015.07.781}},
ISSN = {{2351-9789}},
Keywords = {{Attention Distribution; Interface Design; Situation Awareness; Perceived
   Workload; Visual Scan}},
Keywords-Plus = {{NATURALISTIC DECISION-MAKING; SITUATION AWARENESS; PERCEPTION; AVIATION}},
Research-Areas = {{Behavioral Sciences; Engineering}},
Web-of-Science-Categories  = {{Behavioral Sciences; Engineering, Multidisciplinary}},
Author-Email = {{wenchin.li@cranfield.ac.uk}},
Number-of-Cited-References = {{22}},
Times-Cited = {{1}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{6}},
Doc-Delivery-Number = {{BF6UZ}},
Unique-ID = {{ISI:000383740305107}},
OA = {{Bronze}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000380488000079,
Author = {Radecky, Michal and Vykopal, Jan and Smutny, Pavel},
Editor = {{Petras, I and Podlubny, I and Kacur, J and Vasarhelyi, J}},
Title = {{Analysis of Syntactic Elements and Structure of Web Pages Using
   Eye-tracking Technology}},
Booktitle = {{2015 16TH INTERNATIONAL CARPATHIAN CONTROL CONFERENCE (ICCC)}},
Year = {{2015}},
Pages = {{420-425}},
Note = {{2015 16th International Carpathian Control Conference (ICCC),
   Szilvasvarad, HUNGARY, MAY 27-30, 2015}},
Organization = {{IEEE Adv Technol Human; IEEE Ind Appl Soc; Dept Control Syst
   Instrumentat; Dept Proc Control; Inst Control Informat Product Proc;
   Dept Automatic Control; Univ Miskolc, Automat Info Communicat Fdn
   Supporting Educ Elect Engn}},
Abstract = {{As web pages are becoming increasingly complex, with images and
   different page layouts, understanding how users examine the page is
   important. We used eye-tracking analysis of user behavior and
   performance in the use of web. Using GP3 Eye Tracker we created tracking
   system, which take into account syntactic elements and content structure
   of the tested web pages.}},
Publisher = {{IEEE}},
Address = {{345 E 47TH ST, NEW YORK, NY 10017 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Radecky, M (Reprint Author), VSB Tech Univ Ostrava, Dept Comp Sci, FEECS, Ostrava, Czech Republic.
   Radecky, Michal; Vykopal, Jan, VSB Tech Univ Ostrava, Dept Comp Sci, FEECS, Ostrava, Czech Republic.
   Smutny, Pavel, VSB Tech Univ Ostrava, Dept Control Syst \& Instrumentat, FME, Ostrava, Czech Republic.}},
ISBN = {{978-1-4799-7370-5}},
Keywords = {{Keywords-eye-tracking; user inter/ace; syntactic analysis}},
Research-Areas = {{Automation \& Control Systems}},
Web-of-Science-Categories  = {{Automation \& Control Systems}},
Author-Email = {{michal.radecky@vsb.cz
   jan.vykopal.st@vsb.cz
   pavel.smutny@vsb.cz}},
ResearcherID-Numbers = {{Vykopal, Jan/Z-4832-2019
   Smutny, Pavel/U-9577-2017}},
ORCID-Numbers = {{Vykopal, Jan/0000-0002-3425-0951
   Smutny, Pavel/0000-0002-0885-5850}},
Number-of-Cited-References = {{11}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{2}},
Doc-Delivery-Number = {{BF2PL}},
Unique-ID = {{ISI:000380488000079}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000380393600055,
Author = {Gharbi, Mamoun and Paubel, Pierre-Vincent and Clodic, Aurelie and
   Carreras, Ophelie and Alami, Rachid and Cellier, Jean-Marie},
Book-Group-Author = {{IEEE}},
Title = {{Toward a better understanding of the communication cues involved in a
   human-robot object transfer.}},
Booktitle = {{2015 24TH IEEE INTERNATIONAL SYMPOSIUM ON ROBOT AND HUMAN INTERACTIVE
   COMMUNICATION (RO-MAN)}},
Year = {{2015}},
Pages = {{319-324}},
Note = {{24th IEEE International Symposium on Robot and Human Interactive
   Communication (RO-MAN), Kobe, JAPAN, AUG 31-SEP 04, 2015}},
Organization = {{IEEE; Robot Soc Japan; Korea Robot Soc; IEEE Robot \& Automat Soc}},
Abstract = {{Handing-over objects to humans (or taking objects from them) is a key
   capability for a service robot. Humans are efficient and natural while
   performing this action and the purpose of the studies on this topic is
   to bring human-robot handovers to an acceptable, efficient and natural
   level.
   This paper deals with the cues that allow to make a handover look as
   natural as possible, and more precisely we focus on where the robot
   should look while performing it. In this context we propose a user
   study, involving 33 volunteers, who judged video sequences where they
   see either a human or a robot giving them an object. They were presented
   with different sequences where the agents (robot or human) have
   different gaze behaviours, and were asked to give their feeling about
   the sequence naturalness. In addition to this subjective measure, the
   volunteers were equipped with an eye tracker which enabled us to have
   more accurate objective measures.}},
Publisher = {{IEEE}},
Address = {{345 E 47TH ST, NEW YORK, NY 10017 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Gharbi, M (Reprint Author), CNRS, LAAS, 7 Ave Colonel Roche, F-31400 Toulouse, France.
   Gharbi, Mamoun; Clodic, Aurelie; Alami, Rachid, CNRS, LAAS, 7 Ave Colonel Roche, F-31400 Toulouse, France.
   Gharbi, Mamoun, Univ Toulouse, INSA, LAAS, F-31400 Toulouse, France.
   Paubel, Pierre-Vincent; Carreras, Ophelie; Cellier, Jean-Marie, CLLE LTC UMR 5263 UT2J CNRS, Toulouse, France.
   Clodic, Aurelie; Alami, Rachid, Univ Toulouse, LAAS, F-31400 Toulouse, France.}},
ISBN = {{978-1-4673-6704-2}},
Research-Areas = {{Computer Science; Robotics}},
Web-of-Science-Categories  = {{Computer Science, Cybernetics; Robotics}},
Author-Email = {{mamoun.gharbi@laas.fr
   pierre.paubel@univ-tlse2.fr
   aurelie.clodic@laas.fr
   carreras@univ-tlse2.fr
   rachid.alami@laas.fr
   cellier@univ-tlse2.fr}},
Number-of-Cited-References = {{19}},
Times-Cited = {{5}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{1}},
Doc-Delivery-Number = {{BF1GF}},
Unique-ID = {{ISI:000380393600055}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000380481600055,
Author = {Yamaya, Akito and Topic, Goran and Martinez-Gomez, Pascual and Aizawa,
   Akiko},
Editor = {{NevesSilva, R and Jain, LC and Howlett, RJ}},
Title = {{Dynamic-Programming-Based Method for Fixation-to-Word Mapping}},
Booktitle = {{INTELLIGENT DECISION TECHNOLOGIES}},
Series = {{Smart Innovation Systems and Technologies}},
Year = {{2015}},
Volume = {{39}},
Pages = {{649-659}},
Note = {{7th KES International Conference on Intelligent Decision Technologies
   (KES-IDT), Sorrento, ITALY, JUN 17-19, 2015}},
Organization = {{KES Int}},
Abstract = {{Eye movements made when reading text are considered to be important
   clues for estimating both understanding and interest. To analyze gaze
   data captured by the eye tracker with respect to a text, we need a
   noise-robust mapping between a fixation point and a word in the text. In
   this paper, we propose a dynamic-programming-based method for effective
   fixation-to-word mappings that can reduce the vertical displacement in
   gaze location. The golden dataset is created using FixFix, our web-based
   manual annotation tool. We first divide the gaze data into a number of
   sequential reading segments, then attempt to find the best
   segment-to-line alignment. To determine the best alignment, we select
   candidates for each segment, and calculate the cost based on the length
   characteristics of both the segment and document lines. We compare our
   method with the naive mapping method, and show that it is capable of
   producing more accurate fixation-to-word mappings.}},
Publisher = {{SPRINGER-VERLAG BERLIN}},
Address = {{HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Yamaya, A (Reprint Author), Univ Tokyo, Bunkyo Ku, 7-3-1 Hongo, Tokyo 1138654, Japan.
   Yamaya, Akito; Aizawa, Akiko, Univ Tokyo, Bunkyo Ku, 7-3-1 Hongo, Tokyo 1138654, Japan.
   Topic, Goran; Martinez-Gomez, Pascual; Aizawa, Akiko, Natl Inst Informat, Tokyo 1018430, Japan.}},
DOI = {{10.1007/978-3-319-19857-6\_55}},
ISSN = {{2190-3018}},
ISBN = {{978-3-319-19857-6; 978-3-319-19856-9}},
Keywords = {{Eye tracking research; Error correction; Fixation-to-word mapping;
   Segment-to-line alignment; Dynamic programming; FixFix}},
Research-Areas = {{Computer Science}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Computer Science, Theory \&
   Methods}},
Author-Email = {{yamaya@nii.ac.jp
   goran\_topic@nii.ac.jp
   pascual@nii.ac.jp
   aizawa@nii.ac.jp}},
Number-of-Cited-References = {{10}},
Times-Cited = {{3}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Doc-Delivery-Number = {{BF2MX}},
Unique-ID = {{ISI:000380481600055}},
DA = {{2019-10-28}},
}

@article{ ISI:000367449400001,
Author = {Salmeron, Ladislao and Vidal-Abarca, Eduardo and Martinez, Tomas and
   Mana, Amelia and Gil, Laura and Naumann, Johannes},
Title = {{Strategic Decisions in Task-Oriented Reading}},
Journal = {{SPANISH JOURNAL OF PSYCHOLOGY}},
Year = {{2015}},
Volume = {{18}},
Abstract = {{Answering questions from texts are assessment and instructional
   activities that are frequently used in schools. Nevertheless, little is
   known about the strategic processes that students take while performing
   these tasks. We explored the amount and frequency that students
   initially read of a text before they answered questions pertaining to
   the material. In a procedure similar to the one used in the PISA
   (Program for International Students Assessment), one-hundred-seventy
   students between 7th and 9th grade read and answered several questions
   designed to assess task-oriented reading in three specific texts. We
   recorded on-line indexes that evaluated student behavior (e.g., the
   amount of text that students read before answering questions raised
   within a given text), performance, and comprehension skill. The results
   revealed that students skilled in comprehension initially read a high
   proportion of the texts, which in turn improved their overall
   performance in two of the three texts read (text 1: CI95\%: 0.01 to
   0.09; text 2: CI95\%: -0.01 to 0.05; text 3: CI95\%: 0.04 to 0.20).
   Therefore, we conclude that this strategic behavior should be considered
   during the assessment and instruction of reading literacy.}},
Publisher = {{CAMBRIDGE UNIV PRESS}},
Address = {{32 AVENUE OF THE AMERICAS, NEW YORK, NY 10013-2473 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Salmeron, L (Reprint Author), Univ Valencia, Ave Blasco Ibanez 21, Valencia 46010, Spain.
   Salmeron, Ladislao; Vidal-Abarca, Eduardo; Martinez, Tomas; Mana, Amelia; Gil, Laura, Univ Valencia, Valencia 46010, Spain.
   Naumann, Johannes, German Inst Int Educ Res, Frankfurt, Germany.}},
DOI = {{10.1017/sjp.2015.101}},
Article-Number = {{e102}},
ISSN = {{1138-7416}},
EISSN = {{1988-2904}},
Keywords = {{reading comprehension assessment; reading literacy; task-oriented
   reading}},
Keywords-Plus = {{QUESTION-ANSWERING TASKS; RESTRICTED FOCUS VIEWER; TEXT COMPREHENSION;
   LEXICAL QUALITY; MEMORY; INFORMATION; ATTENTION; RELEVANCE; TRACKING;
   ABILITY}},
Research-Areas = {{Psychology}},
Web-of-Science-Categories  = {{Psychology; Psychology, Multidisciplinary}},
Author-Email = {{ladislao.salmeron@uv.es}},
ResearcherID-Numbers = {{Mana Lloria, Amelia/L-6936-2014
   Gil, Laura/C-9911-2011
   Salmeron, Ladislao/C-7562-2011}},
ORCID-Numbers = {{Mana Lloria, Amelia/0000-0002-5304-5611
   Gil, Laura/0000-0002-0937-6245
   Salmeron, Ladislao/0000-0002-4283-4279}},
Funding-Acknowledgement = {{Ministerio de Ciencia e Innovacion (Secretaria General de Universidades)
   {[}EDU2008-03072]}},
Funding-Text = {{This research was funded by the Ministerio de Ciencia e Innovacion
   (Secretaria General de Universidades, Project EDU2008-03072). Johannes
   Naumann is currently at the Goethe-University Frankfurt am Main
   (Germany)}},
Number-of-Cited-References = {{40}},
Times-Cited = {{5}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{5}},
Journal-ISO = {{Span. J. Psychol.}},
Doc-Delivery-Number = {{CZ9WY}},
Unique-ID = {{ISI:000367449400001}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000364809400077,
Author = {Sharif, Bonita and Shaffer, Timothy},
Editor = {{Schmorrow, DD and Fidopiastis, CM}},
Title = {{The Use of Eye Tracking in Software Development}},
Booktitle = {{FOUNDATIONS OF AUGMENTED COGNITION, AC 2015}},
Series = {{Lecture Notes in Artificial Intelligence}},
Year = {{2015}},
Volume = {{9183}},
Pages = {{807-816}},
Note = {{9th International Conference on Augmented Cognition (AC) Held as Part of
   17th International Conference on Human-Computer Interaction (HCI
   International), Los Angeles, CA, AUG 02-07, 2015}},
Abstract = {{Eye trackers have been routinely used in psychology reading experiments
   and in website usability studies for many years. However, it is only
   recently that they have been used by more researchers in the software
   engineering community. In this paper, we categorize two broad areas in
   which eye tracking technology can benefit software development in a
   practical way. The first area includes using the eye tracker as an
   assessment tool for software artifacts, tools, and techniques. The
   second area deals with using eye tracking data from developers to inform
   certain software tools and software development tasks such as providing
   developer recommendations and software traceability tasks. Examples of
   experiments and studies done in each of these broad areas is presented
   and discussed along with future work. The results point towards many
   benefits that eye trackers provide to augment the daily lives of
   programmers during software development.}},
Publisher = {{SPRINGER-VERLAG BERLIN}},
Address = {{HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Sharif, B (Reprint Author), Youngstown State Univ, Youngstown, OH 44555 USA.
   Sharif, Bonita; Shaffer, Timothy, Youngstown State Univ, Youngstown, OH 44555 USA.}},
DOI = {{10.1007/978-3-319-20816-9\_77}},
ISSN = {{0302-9743}},
EISSN = {{1611-3349}},
ISBN = {{978-3-319-20816-9; 978-3-319-20815-2}},
Keywords = {{Eye tracking; Software development; Software traceability; Assessing
   software artifacts; Program comprehension}},
Keywords-Plus = {{COMPREHENSION; IMPACT}},
Research-Areas = {{Computer Science}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Computer Science, Information
   Systems}},
Author-Email = {{bsharif@ysu.edu
   trshaffer@student.ysu.edu}},
Number-of-Cited-References = {{28}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{2}},
Doc-Delivery-Number = {{BD9KA}},
Unique-ID = {{ISI:000364809400077}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000412403900040,
Author = {Neupane, Ajaya and Rahman, Md. Lutfor and Saxena, Nitesh and Hirshfield,
   Leanne},
Book-Group-Author = {{Assoc Comp Machinery}},
Title = {{A Multi-Modal Neuro-Physiological Study of Phishing Detection and
   Malware Warnings}},
Booktitle = {{CCS'15: PROCEEDINGS OF THE 22ND ACM SIGSAC CONFERENCE ON COMPUTER AND
   COMMUNICATIONS SECURITY}},
Year = {{2015}},
Pages = {{479-491}},
Note = {{22nd ACM SIGSAC Conference on Computer and Communications Security
   (CCS), Denver, CO, OCT 12-16, 2015}},
Organization = {{ACM SIGSAC; Assoc Comp Machinery}},
Abstract = {{Detecting phishing attacks (identifying fake vs. real websites) and
   heeding security warnings represent classical user-centered security
   tasks subjected to a series of prior investigations. However, our
   understanding of user behavior underlying these tasks is still not fully
   mature, motivating further work concentrating at the neurophysiological
   level governing the human processing of such tasks.
   We pursue a comprehensive three-dimensional study of phishing detection
   and malware warnings, focusing not only on what users' task performance
   is but also on how users process these tasks based on: (1) neural
   activity captured using Electroencephalogram (EEG) cognitive metrics,
   and (2) eye gaze patterns captured using an eye tracker. Our primary
   novelty lies in employing multi-modal neurophysiological measures in a
   single study and providing a near realistic set-up (in contrast to a
   recent neuro-study conducted inside an fMRI scanner). Our work serves to
   advance, extend and support prior knowledge in several significant ways.
   Specifically, in the context of phishing detection, we show that users
   do not spend enough time analyzing key phishing indicators and often
   fail at detecting these attacks, although they may be mentally engaged
   in the task and subconsciously processing real sites differently from
   fake sites. In the malware warning tasks, in contrast, we show that
   users are frequently reading, possibly comprehending, and eventually
   heeding the message embedded in the warning.
   Our study provides an initial foundation for building future mechanisms
   based on the studied real-time neural and eye gaze features, that can
   automatically infer a user's ``alertness{''} state, and determine
   whether or not the user's response should be relied upon.}},
Publisher = {{ASSOC COMPUTING MACHINERY}},
Address = {{1515 BROADWAY, NEW YORK, NY 10036-9998 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Neupane, A (Reprint Author), Univ Alabama Birmingham, Dept Comp \& Informat Sci, Birmingham, AL 35294 USA.
   Neupane, Ajaya; Saxena, Nitesh, Univ Alabama Birmingham, Dept Comp \& Informat Sci, Birmingham, AL 35294 USA.
   Rahman, Md. Lutfor, Aegis Foundry LLC, Birmingham, AL USA.
   Hirshfield, Leanne, Syracuse Univ, Newhouse Sch Publ Commun, Syracuse, NY 13244 USA.
   Rahman, Md. Lutfor, UAB, Birmingham, AL USA.}},
DOI = {{10.1145/2810103.2813660}},
ISBN = {{978-1-4503-3832-5}},
Keywords = {{Phishing Detection; Malware Warnings; EEG; Eye Tracking; Neuroscience}},
Keywords-Plus = {{ATTENTIONAL CONTROL; EEG}},
Research-Areas = {{Computer Science}},
Web-of-Science-Categories  = {{Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory \& Methods}},
Author-Email = {{aneupane@uab.edu
   mdlutforrahman@csebuet.org
   saxena@uab.edu
   lmhirshf@syr.edu}},
Number-of-Cited-References = {{31}},
Times-Cited = {{6}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Doc-Delivery-Number = {{BI5GU}},
Unique-ID = {{ISI:000412403900040}},
DA = {{2019-10-28}},
}

@article{ ISI:000345481600007,
Author = {Maguire, Mandy J. and Magnon, Grant and Fitzhugh, Anna E.},
Title = {{Improving data retention in EEG research with children using
   child-centered eye tracking}},
Journal = {{JOURNAL OF NEUROSCIENCE METHODS}},
Year = {{2014}},
Volume = {{238}},
Pages = {{78-81}},
Month = {{DEC 30}},
Abstract = {{Background: Event Related Potentials (ERPs) elicited by visual stimuli
   have increased our understanding of developmental disorders and adult
   cognitive abilities for decades; however, these studies are very
   difficult with populations who cannot sustain visual attention such as
   infants and young children. Current methods for studying such
   populations include requiring a button response, which may be impossible
   for some participants, and experimenter monitoring, which is subject to
   error, highly variable, and spatially imprecise.
   New method: We developed a child-centered methodology to integrate EEG
   data acquisition and eye-tracking technologies that uses
   ``attention-getters{''} in which stimulus display is contingent upon the
   child's gaze. The goal was to increase the number of trials retained.
   Additionally, we used the eye-tracker to categorize and analyze the EEG
   data based on gaze to specific areas of the visual display, compared to
   analyzing based on stimulus presentation.
   Results compared with existing methods: The number of trials retained
   was substantially improved using the child-centered methodology compared
   to a button-press response in 7-8 year olds. In contrast, analyzing the
   EEG based on eye gaze to specific points within the visual display as
   opposed to stimulus presentation provided too few trials for reliable
   interpretation.
   Conclusions: By using the linked EEG-eye-tracker we significantly
   increased data retention. With this method, studies can be completed
   with fewer participants and a wider range of populations. However,
   caution should be used when epoching based on participants' eye gaze
   because, in this case, this technique provided substantially fewer
   trials. (C) 2014 Elsevier B.V. All rights reserved.}},
Publisher = {{ELSEVIER SCIENCE BV}},
Address = {{PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Maguire, MJ (Reprint Author), Univ Texas Dallas, 1966 Inwood Rd, Dallas, TX 75235 USA.
   Maguire, Mandy J.; Magnon, Grant; Fitzhugh, Anna E., Univ Texas Dallas, Dallas, TX 75235 USA.}},
DOI = {{10.1016/j.jneumeth.2014.09.014}},
ISSN = {{0165-0270}},
EISSN = {{1872-678X}},
Keywords = {{EEG; Children; Eye tracking}},
Keywords-Plus = {{FACIAL EXPRESSIONS; INFANT BRAIN; LOOKING; FACES; ERP}},
Research-Areas = {{Biochemistry \& Molecular Biology; Neurosciences \& Neurology}},
Web-of-Science-Categories  = {{Biochemical Research Methods; Neurosciences}},
Author-Email = {{mandy.maguire@utdallas.edu}},
ORCID-Numbers = {{Middleton, Anna/0000-0003-3775-3316}},
Funding-Acknowledgement = {{NIH/NICHDUnited States Department of Health \& Human ServicesNational
   Institutes of Health (NIH) - USANIH Eunice Kennedy Shriver National
   Institute of Child Health \& Human Development (NICHD)
   {[}1R03HD064629-01]; University of Texas at Dallas Faculty Initiative
   Grants}},
Funding-Text = {{We would like to thank the following people for their help in this
   project: McKenna Jackson, Bambi Delarosa, Julie Schneider and Alyson
   Abel. This work was funded by NIH/NICHD grant 1R03HD064629-01 and
   University of Texas at Dallas Faculty Initiative Grants awarded to the
   first author.}},
Number-of-Cited-References = {{12}},
Times-Cited = {{2}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{28}},
Journal-ISO = {{J. Neurosci. Methods}},
Doc-Delivery-Number = {{AU2YZ}},
Unique-ID = {{ISI:000345481600007}},
OA = {{Green Accepted}},
DA = {{2019-10-28}},
}

@article{ ISI:000345503200010,
Author = {Cambra, Cristina and Penacchio, Olivier and Silvestre, Nuria and Leal,
   Aurora},
Title = {{Visual attention to subtitles when viewing a cartoon by deaf and hearing
   children: an eye-tracking pilot study}},
Journal = {{PERSPECTIVES-STUDIES IN TRANSLATION THEORY AND PRACTICE}},
Year = {{2014}},
Volume = {{22}},
Number = {{4, SI}},
Pages = {{607-617}},
Month = {{OCT 2}},
Abstract = {{Watching a subtitled programme is a complex activity, as it entails
   paying attention to various stimuli simultaneously, some of which are
   visual (images and subtitles) and others auditory (oral language and
   background sounds). The aim of this study is to analyse the ocular
   movement of a group of children including both hearing and deaf children
   when watching a television cartoon using an eye tracker. The sample
   comprises 22 children (11 of whom are deaf and 11 of whom are hearing)
   aged between seven and 11. The results show that both hearing and deaf
   children spend more time looking at the images than at the subtitles,
   with the character's lips being the facial feature to which they pay
   most attention. Participant age and reading speed are variables that
   significantly affect the degree of attention paid to subtitles: the
   youngest children with the slowest reading speed lose their attention as
   the cartoon progresses. However, participants' auditory condition (deaf
   or hearing) does not show significant differences regarding maintaining
   attention on subtitles.}},
Publisher = {{ROUTLEDGE JOURNALS, TAYLOR \& FRANCIS LTD}},
Address = {{2-4 PARK SQUARE, MILTON PARK, ABINGDON OX14 4RN, OXON, ENGLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Cambra, C (Reprint Author), Univ Autonoma Barcelona, Dept Educ Psychol, E-08193 Barcelona, Spain.
   Cambra, Cristina, Univ Autonoma Barcelona, Dept Educ Psychol, E-08193 Barcelona, Spain.
   Penacchio, Olivier, Univ Autonoma Barcelona, Comp Vis Ctr, E-08193 Barcelona, Spain.
   Silvestre, Nuria; Leal, Aurora, Univ Autonoma Barcelona, Fac Psychol, E-08193 Barcelona, Spain.}},
DOI = {{10.1080/0907676X.2014.923477}},
ISSN = {{0907-676X}},
EISSN = {{1747-6623}},
Keywords = {{eye-tracking; television; cartoon; subtitling; reading speed; deaf
   viewers}},
Keywords-Plus = {{AUDIOVISUAL REDUNDANCY; TELEVISION; COMPREHENSION; INFORMATION}},
Research-Areas = {{Linguistics}},
Web-of-Science-Categories  = {{Linguistics; Language \& Linguistics}},
Author-Email = {{cristina.cambra@uab.cat}},
Number-of-Cited-References = {{32}},
Times-Cited = {{3}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{18}},
Journal-ISO = {{Perspect.-Stud. Transl.}},
Doc-Delivery-Number = {{AU3HN}},
Unique-ID = {{ISI:000345503200010}},
DA = {{2019-10-28}},
}

@article{ ISI:000343840700084,
Author = {Varela, Paula and Antunez, Lucia and Silva Cadena, Rafael and Gimenez,
   Ana and Ares, Gaston},
Title = {{Attentional capture and importance of package attributes for consumers'
   perceived similarities and differences among products: A case study with
   breakfast cereal packages}},
Journal = {{FOOD RESEARCH INTERNATIONAL}},
Year = {{2014}},
Volume = {{64}},
Pages = {{701-710}},
Month = {{OCT}},
Abstract = {{The present work studied attentional capture and importance of package
   attributes for consumers' perception of similarities and differences
   among products through a combination of eye-tracking and projective
   mapping. As a case study, fifty consumers performed a projective mapping
   task with ten breakfast cereal packages while wearing a mobile
   eye-tracker. The combination of mobile eye-tracking and projective
   mapping enabled a more comprehensive analysis of the importance of
   package attributes for consumer perception. Eye tracking allowed the
   identification of the most relevant package features for perceived
   similarity and differences among products and spotted attributes that
   were attended to but were not relevant, as well as package features that
   were relevant for categorization but were not largely attended to.
   Results suggest that studying attentional capture could contribute to
   better understanding attribute importance for consumer perception.
   Irrespectively of the saliency, most consumers looked at the same key
   information, mainly located on the front-of-pack. Few consumers read the
   nutritional label and ingredient list (a much lower proportion than in
   previous static eye tracker studies). Results suggested that mobile
   eye-tracking has a great potential for assessing consumers' evaluation
   of packages in ecological settings. However, several disadvantages and
   limitations of the technique should be taken into account (C) 2014
   Elsevier Ltd. All rights reserved.}},
Publisher = {{ELSEVIER SCIENCE BV}},
Address = {{PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Varela, P (Reprint Author), Nofima AS, POB 210, N-1431 As, Norway.
   Varela, Paula, Nofima AS, N-1431 As, Norway.
   Antunez, Lucia; Silva Cadena, Rafael; Gimenez, Ana; Ares, Gaston, Univ Republica, Fac Quim, Dept Ciencia \& Tecnol Alimentos, Montevideo 11800, Uruguay.
   Gimenez, Ana; Ares, Gaston, Univ Republica, Fac Psicol, Ctr Invest Basica Psicol, Montevideo 11200, Uruguay.
   Varela, Paula, CSIC, Inst Agroquim \& Tecnol Alimentos, Valencia 46980, Spain.}},
DOI = {{10.1016/j.foodres.2014.08.015}},
ISSN = {{0963-9969}},
EISSN = {{1873-7145}},
Keywords = {{Eye-tracking; Packaging; Food labels; Projective mapping; Napping;
   Sensory profiling}},
Keywords-Plus = {{MULTIPLE FACTOR-ANALYSIS; EYE-TRACKING; PERCEPTION; INFORMATION;
   PREFERENCE; QUALITY; CATEGORIZATION; STRATEGIES; INTENTION; BISCUITS}},
Research-Areas = {{Food Science \& Technology}},
Web-of-Science-Categories  = {{Food Science \& Technology}},
Author-Email = {{paula.varela.tomasco@nofima.no}},
ResearcherID-Numbers = {{Varela, Paula/H-4135-2012
   }},
ORCID-Numbers = {{Antunez, Lucia/0000-0003-2497-6609}},
Funding-Acknowledgement = {{Spanish Ministry of Science and InnovationMinistry of Science and
   Innovation, Spain (MICINN)Ministry of Education and Science, Spain;
   Spanish Ministry of Education, Culture and Sports; CAPES-BrasilCAPES;
   Espacio Interdisciplinario from Universidad de la Republica (Uruguay)}},
Funding-Text = {{The authors are indebted to the Spanish Ministry of Science and
   Innovation for the contract awarded to the author Paula Varela (Juan de
   la Cierva Program) and to the Spanish Ministry of Education, Culture and
   Sports for the Jose Castillejo grant awarded to author Paula Varela. The
   authors would also like to thank CAPES-Brasil and Espacio
   Interdisciplinario from Universidad de la Republica (Uruguay) for
   financial support.}},
Number-of-Cited-References = {{48}},
Times-Cited = {{15}},
Usage-Count-Last-180-days = {{2}},
Usage-Count-Since-2013 = {{29}},
Journal-ISO = {{Food Res. Int.}},
Doc-Delivery-Number = {{AR8QX}},
Unique-ID = {{ISI:000343840700084}},
DA = {{2019-10-28}},
}

@article{ ISI:000342802800005,
Author = {Keller, Carmen and Kreuzmair, Christina and Leins-Hess, Rebecca and
   Siegrist, Michael},
Title = {{Numeric and graphic risk information processing of high and low
   numerates in the intuitive and deliberative decision modes: An
   eye-tracker study}},
Journal = {{JUDGMENT AND DECISION MAKING}},
Year = {{2014}},
Volume = {{9}},
Number = {{5}},
Pages = {{420-432}},
Month = {{SEP}},
Abstract = {{The influence of numeracy on information processing of two risk
   communication formats (percentage and pictograph) was examined using an
   eye tracker. A sample from the general population (N = 159) was used. In
   intuitive and deliberative decision conditions, the participants were
   presented with a hypothetical scenario presenting a test result. The
   participants indicated their feelings and their perceived risk, evoked
   by a 17\% risk level. In the intuitive decision condition, a significant
   correlation (r =.30) between numeracy and the order of information
   processing was found: the higher the numeracy, the earlier the
   processing of the percentage, and the lower the numeracy, the earlier
   the processing of the pictograph. This intuitive, initial focus on a
   format prevailed over the first half of the intuitive decision-making
   process. In the deliberative decision condition, the correlation between
   numeracy and order of information processing was not significant. In
   both decision conditions, high and low numerates processed pictograph
   and percentage formats with similar depths and derived similar meanings
   from them in terms of feelings and perceived risk. In both conditions
   numeracy had no effects on the degree of attention on the percentage or
   the pictograph (number of fixations on formats and transitions between
   them). The results suggest that pictographs attract low numerates'
   attention, and percentages attract high numerates' attention in the
   first, intuitive, phase of numeric information processing. Pictographs
   thus ensure low numerates' further elaboration on numeric risk
   information, which is an important precondition of risk understanding
   and decision making.}},
Publisher = {{SOC JUDGMENT \& DECISION MAKING}},
Address = {{FLORIDA STATE UNIV, TALLAHASSEE, FL 32306-1110 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Keller, C (Reprint Author), ETH, IED, Univ Str 22,CHN J75-2, CH-8092 Zurich, Switzerland.
   Keller, Carmen, ETH, IED, CH-8092 Zurich, Switzerland.
   Kreuzmair, Christina; Leins-Hess, Rebecca; Siegrist, Michael, ETH, CH-8092 Zurich, Switzerland.}},
ISSN = {{1930-2975}},
Keywords = {{numeracy; information processing; eye tracker; intuitive and
   deliberative decision making; risk communication}},
Keywords-Plus = {{SELF-EFFICACY; HEALTH-RISK; COMMUNICATION; COMPREHENSION; FORMAT;
   JUDGMENTS; BENEFITS; PARENTS; ABILITY}},
Research-Areas = {{Psychology}},
Web-of-Science-Categories  = {{Psychology, Multidisciplinary}},
Author-Email = {{ckeller@ethz.ch}},
ResearcherID-Numbers = {{Keller, Carmen/G-3546-2013}},
Number-of-Cited-References = {{40}},
Times-Cited = {{17}},
Usage-Count-Last-180-days = {{2}},
Usage-Count-Since-2013 = {{21}},
Journal-ISO = {{Judgm. Decis. Mak.}},
Doc-Delivery-Number = {{AQ4WZ}},
Unique-ID = {{ISI:000342802800005}},
OA = {{DOAJ Gold}},
DA = {{2019-10-28}},
}

@article{ ISI:000339645900008,
Author = {Magdaleno, Fernando and Martinez, Roberto},
Title = {{Evaluating the quality of riparian forest vegetation: the Riparian
   Forest Evaluation (RFV) index}},
Journal = {{FOREST SYSTEMS}},
Year = {{2014}},
Volume = {{23}},
Number = {{2}},
Pages = {{259-272}},
Month = {{AUG}},
Abstract = {{Aim of study: This paper presents a novel index, the Riparian Forest
   Evaluation (RFV) index, for assessing the ecological condition of
   riparian forests. The status of riparian ecosystems has global
   importance due to the ecological and social benefits and services they
   provide. The initiation of the European Water Framework Directive
   (2000/60/CE) requires the assessment of the hydromorphological quality
   of natural channels. The Directive describes riparian forests as one of
   the fundamental components that determine the structure of riverine
   areas. The RFV index was developed to meet the aim of the Directive and
   to complement the existing methodologies for the evaluation of riparian
   forests.
   Area of study: The RFV index was applied to a wide range of streams and
   rivers (170 water bodies) in Spain.
   Material and methods: The calculation of the RFV index is based on the
   assessment of both the spatial continuity of the forest (in its three
   core dimensions: longitudinal, transversal and vertical) and the
   regeneration capacity of the forest, in a sampling area related to the
   river hydromorphological pattern. This index enables an evaluation of
   the quality and degree of alteration of riparian forests. In addition,
   it helps to determine the scenarios that are necessary to improve the
   status of riparian forests and to develop processes for restoring their
   structure and composition.
   Main results: The results were compared with some previous tools for the
   assessment of riparian vegetation. The RFV index got the highest average
   scores in the basins of northern Spain, which suffer lower human
   influence. The forests in central and southern rivers got worse scores.
   The bigger differences with other tools were found in complex and
   partially altered streams and rivers.
   Research highlights: The study showed the index's applicability under
   diverse hydromorphological and ecological conditions and the main
   advantages of its application. The utilization of the index allows a
   better understanding of the status of riparian forests, and enhances
   improvements in the conservation and management of riparian areas.}},
Publisher = {{INST NACIONAL INVESTIGACION TECHNOLOGIA AGRARIA ALIMENTARIA}},
Address = {{CTRA CORUNA KM 7 5, MADRID, 28040, SPAIN}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Magdaleno, F (Reprint Author), CEDEX Ctr Studies \& Expt Publ Works, C Alfonso 12, Madrid 28014, Spain.
   Magdaleno, Fernando; Martinez, Roberto, CEDEX Ctr Studies \& Expt Publ Works, Madrid 28014, Spain.}},
DOI = {{10.5424/fs/2014232-04488}},
ISSN = {{2171-5068}},
EISSN = {{2171-9845}},
Keywords = {{riparian quality; Water Framework Directive; connectivity; indicator;
   hydromorphology}},
Keywords-Plus = {{RIVERS; CONNECTIVITY; INDICATORS; CORRIDORS; INTEGRITY; ECOLOGY;
   HABITAT; REGIMES; STREAMS}},
Research-Areas = {{Forestry}},
Web-of-Science-Categories  = {{Forestry}},
Author-Email = {{fernando.magdaleno@cedex.es}},
Number-of-Cited-References = {{69}},
Times-Cited = {{4}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{48}},
Journal-ISO = {{For. Syst.}},
Doc-Delivery-Number = {{AM1YT}},
Unique-ID = {{ISI:000339645900008}},
OA = {{DOAJ Gold}},
DA = {{2019-10-28}},
}

@article{ ISI:000336476900005,
Author = {Molina, Ana I. and Redondo, Miguel A. and Ortega, Manuel and Lacave,
   Carmen},
Title = {{Evaluating a graphical notation for modeling collaborative learning
   activities: A family of experiments}},
Journal = {{SCIENCE OF COMPUTER PROGRAMMING}},
Year = {{2014}},
Volume = {{88}},
Number = {{SI}},
Pages = {{54-81}},
Month = {{AUG 1}},
Abstract = {{It is increasingly common to use languages and notations, mainly of a
   graphical nature, to assist in the design and specification of learning
   systems. There are several proposals, although few of them support the
   modeling of collaborative tasks. In this paper, we identify the main
   features to be considered for modeling this kind of activities and we
   propose the use of the CIAN notation for this purpose. In this work, we
   also try to empirically analyze the quality (in particular the
   understandability) of that notation. To this end, three empirical
   studies have been conducted. In these experiments we used several
   sources of information: subjective perception of the designers, their
   profiles and their performance on a set of understandability exercises,
   as well as the physical evidence provided by an eye tracker device. The
   results obtained denote positive perceptions about the use of the CLAN
   notation for modeling collaborative learning activities. (C) 2014
   Elsevier BM. All rights reserved.}},
Publisher = {{ELSEVIER}},
Address = {{RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Molina, AI (Reprint Author), Univ Castilla La Mancha, Escuela Super Informat Ciudad Real, Dept Tecnol \& Sistemas Informac, Paseo Univ 4, Ciudad Real 13071, Spain.
   Molina, Ana I.; Redondo, Miguel A.; Ortega, Manuel; Lacave, Carmen, Univ Castilla La Mancha, Escuela Super Informat Ciudad Real, Dept Tecnol \& Sistemas Informac, Ciudad Real 13071, Spain.}},
DOI = {{10.1016/j.scico.2014.02.019}},
ISSN = {{0167-6423}},
EISSN = {{1872-7964}},
Keywords = {{Educational modeling; CSCL; Notation assessments; Empirical study; Eye
   tracking}},
Keywords-Plus = {{ENVIRONMENTS; USABILITY; PROTOCOLS; LANGUAGE}},
Research-Areas = {{Computer Science}},
Web-of-Science-Categories  = {{Computer Science, Software Engineering}},
Author-Email = {{Analsabel.Molina@uclm.es
   Miguel.Redondo@uclm.es
   Manuel.Ortega@uclm.es
   Carmen.Lacave@uclm.es}},
ResearcherID-Numbers = {{Redondo, Miguel A./F-7852-2015
   Ortega, Manuel/M-6083-2019
   Lacave Rodero, Carmen/G-4729-2015
   Bravo, Jose/F-8861-2015
   Molina, Ana I/M-1392-2014}},
ORCID-Numbers = {{Redondo, Miguel A./0000-0001-5809-3412
   Ortega, Manuel/0000-0002-0194-7744
   Lacave Rodero, Carmen/0000-0003-2770-8482
   Bravo, Jose/0000-0001-5752-2406
   Molina, Ana I/0000-0002-3449-2539}},
Funding-Acknowledgement = {{INTEGroup; GITE-LEARN and iColab projects - Regional Government of
   Castilla-La Mancha; EDUCA-Prog project - Ministry of Science and
   Innovation}},
Funding-Text = {{This work has been partially supported by the INTEGroup, GITE-LEARN and
   iColab projects, funded by the Regional Government of Castilla-La
   Mancha, and the EDUCA-Prog project funded by the Ministry of Science and
   Innovation. The authors want to thank all the participants in the
   studies performed for their collaboration.}},
Number-of-Cited-References = {{101}},
Times-Cited = {{4}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{12}},
Journal-ISO = {{Sci. Comput. Program.}},
Doc-Delivery-Number = {{AH9QM}},
Unique-ID = {{ISI:000336476900005}},
OA = {{Bronze}},
DA = {{2019-10-28}},
}

@article{ ISI:000339614800104,
Author = {Krogh-Jespersen, Sheila and Woodward, Amanda L.},
Title = {{Making Smart Social Judgments Takes Time: Infants' Recruitment of Goal
   Information When Generating Action Predictions}},
Journal = {{PLOS ONE}},
Year = {{2014}},
Volume = {{9}},
Number = {{5}},
Month = {{MAY 16}},
Abstract = {{Previous research has shown that young infants perceive others' actions
   as structured by goals. One open question is whether the recruitment of
   this understanding when predicting others' actions imposes a cognitive
   challenge for young infants. The current study explored infants' ability
   to utilize their knowledge of others' goals to rapidly predict future
   behavior in complex social environments and distinguish goal-directed
   actions from other kinds of movements. Fifteen-month-olds (N = 40)
   viewed videos of an actor engaged in either a goal-directed (grasping)
   or an ambiguous (brushing the back of her hand) action on a Tobii
   eye-tracker. At test, critical elements of the scene were changed and
   infants' predictive fixations were examined to determine whether they
   relied on goal information to anticipate the actor's future behavior.
   Results revealed that infants reliably generated goal-based visual
   predictions for the grasping action, but not for the back-of-hand
   behavior. Moreover, response latencies were longer for goal-based
   predictions than for location-based predictions, suggesting that
   goal-based predictions are cognitively taxing. Analyses of areas of
   interest indicated that heightened attention to the overall scene, as
   opposed to specific patterns of attention, was the critical indicator of
   successful judgments regarding an actor's future goal-directed behavior.
   These findings shed light on the processes that support ``smart'' social
   behavior in infants, as it may be a challenge for young infants to use
   information about others' intentions to inform rapid predictions.}},
Publisher = {{PUBLIC LIBRARY SCIENCE}},
Address = {{1160 BATTERY STREET, STE 100, SAN FRANCISCO, CA 94111 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Krogh-Jespersen, S (Reprint Author), Univ Chicago, Dept Psychol, Chicago, IL 60637 USA.
   Krogh-Jespersen, Sheila; Woodward, Amanda L., Univ Chicago, Dept Psychol, Chicago, IL 60637 USA.}},
DOI = {{10.1371/journal.pone.0098085}},
Article-Number = {{e98085}},
ISSN = {{1932-6203}},
Keywords-Plus = {{DIRECTED ACTIONS; GAZE; ATTRIBUTION; PERCEPTION; ABILITY; OTHERS; OBJECT}},
Research-Areas = {{Science \& Technology - Other Topics}},
Web-of-Science-Categories  = {{Multidisciplinary Sciences}},
Funding-Acknowledgement = {{Eunice Kennedy Shriver National Institute of Child Health \& Human
   Development of the National Institutes of Health {[}P01HD064653];
   National Science FoundationNational Science Foundation (NSF) {[}DLS
   0951489]}},
Funding-Text = {{Research reported in this publication was supported by grants to the
   second author from the Eunice Kennedy Shriver National Institute of
   Child Health \& Human Development of the National Institutes of Health
   under Award Number P01HD064653 ( http://www.nichd.nih.gov) and the
   National Science Foundation under award number DLS 0951489
   (www.nsf.gov). The funders had no role in study design, data collection
   and analysis, decision to publish, or preparation of the manuscript.}},
Number-of-Cited-References = {{23}},
Times-Cited = {{9}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{8}},
Journal-ISO = {{PLoS One}},
Doc-Delivery-Number = {{AM1NZ}},
Unique-ID = {{ISI:000339614800104}},
OA = {{DOAJ Gold, Green Published}},
DA = {{2019-10-28}},
}

@article{ ISI:000335486800012,
Author = {Forgacs, Balint and Lukacs, Agnes and Pleh, Csaba},
Title = {{Lateralized processing of novel metaphors: Disentangling figurativeness
   and novelty}},
Journal = {{NEUROPSYCHOLOGIA}},
Year = {{2014}},
Volume = {{56}},
Pages = {{101-109}},
Month = {{APR}},
Abstract = {{One of the intriguing and sometimes controversial findings in figurative
   language research is a right-hemisphere processing advantage for novel
   metaphors. The current divided visual field study introduced novel
   literal expressions as a control condition to assess processing novelty
   independent of figurativeness. Participants evaluated word pairs
   belonging to one of the five categories: (1) conventional metaphorical,
   (2) conventional literal, (3) novel metaphorical, (4) novel literal, and
   (5) unrelated expressions in a semantic decision task. We presented
   expressions without sentence context and controlled for additional
   factors including emotional valence, arousal, and imageability that
   could potentially influence hemispheric processing. We also utilized an
   eye-tracker to ensure lateralized presentation. We did not find the
   previously reported right-hemispherical processing advantage for novel
   metaphors. Processing was faster in the left hemisphere for all types of
   word pairs, and accuracy was also higher in the right visual field -
   left hemisphere. Novel metaphors were processed just as fast as novel
   literal expressions, suggesting that the primary challenge during the
   comprehension of novel expressions is not a serial processing of
   salience, but perhaps a more left hemisphere weighted semantic
   integration. Our results cast doubt on the right-hemisphere theory of
   metaphors, and raise the possibility that other uncontrolled variables
   were responsible for previous results. The lateralization of processing
   of two word expressions seems to be more contingent on the specific task
   at hand than their figurativeness or saliency. (C) 2014 Elsevier Ltd.
   All rights reserved.}},
Publisher = {{PERGAMON-ELSEVIER SCIENCE LTD}},
Address = {{THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Forgacs, B (Reprint Author), Cent European Univ, Cognit Dev Ctr, Hattyu U 14, H-1015 Budapest, Hungary.
   Forgacs, Balint; Lukacs, Agnes; Pleh, Csaba, Budapest Univ Technol \& Econ BME, Dept Cognit Sci, H-1111 Budapest, Hungary.
   Forgacs, Balint, Cent European Univ, Dept Cognit Sci, H-1023 Budapest, Hungary.
   Pleh, Csaba, Eszterhazy Karoly Coll, Doctoral Sch Educ, H-3300 Eger, Hungary.}},
DOI = {{10.1016/j.neuropsychologia.2014.01.003}},
ISSN = {{0028-3932}},
EISSN = {{1873-3514}},
Keywords = {{Metaphor; Figurative language; Right hemisphere; Salience; Coarse
   coding; Divided visual field}},
Keywords-Plus = {{RIGHT CEREBRAL HEMISPHERE; VISUAL WORD RECOGNITION; BRAIN-DAMAGED
   PATIENTS; NEURAL MECHANISMS; COMPREHENSION; APPRECIATION; MEANINGS;
   LANGUAGE; ASYMMETRIES; SENTENCE}},
Research-Areas = {{Behavioral Sciences; Neurosciences \& Neurology; Psychology}},
Web-of-Science-Categories  = {{Behavioral Sciences; Neurosciences; Psychology, Experimental}},
Author-Email = {{forgacsb@ceu.hu
   alukacs@cogsci.bme.hu
   pleh.csaba@ektf.hu}},
ResearcherID-Numbers = {{Forgacs, Balint/G-2538-2016}},
ORCID-Numbers = {{Forgacs, Balint/0000-0002-2191-5459}},
Number-of-Cited-References = {{86}},
Times-Cited = {{15}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{20}},
Journal-ISO = {{Neuropsychologia}},
Doc-Delivery-Number = {{AG5VM}},
Unique-ID = {{ISI:000335486800012}},
DA = {{2019-10-28}},
}

@article{ ISI:000334682200001,
Author = {Hung, Yueh-Nu},
Title = {{``WHAT ARE YOU LOOKING AT?{''} AN EYE MOVEMENT EXPLORATION IN SCIENCE
   TEXT READING}},
Journal = {{INTERNATIONAL JOURNAL OF SCIENCE AND MATHEMATICS EDUCATION}},
Year = {{2014}},
Volume = {{12}},
Number = {{2}},
Pages = {{241-260}},
Month = {{APR}},
Abstract = {{The main purpose of this research was to investigate how Taiwanese grade
   6 readers selected and used information from different print (main text,
   headings, captions) and visual elements (decorational, representational,
   interpretational) to comprehend a science text through tracking their
   eye movement behaviors. Six grade 6 students read a double page of
   science text written in Chinese during which their eye movements were
   documented and analyzed using an EyeLink 1000 eye tracker. The results
   suggest that illustrations received less attention than print; however,
   readers who had more fixations on illustrations had better
   comprehension. While both headings and captions were in the print
   category, the headings received much less attention than did the
   captions. The article concludes with implications for teaching science
   reading and suggestions for future research beyond this exploratory case
   study.}},
Publisher = {{SPRINGER}},
Address = {{VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Hung, YN (Reprint Author), Natl Taichung Univ Educ, Taichung, Taiwan.
   Natl Taichung Univ Educ, Taichung, Taiwan.}},
DOI = {{10.1007/s10763-013-9406-z}},
ISSN = {{1571-0068}},
EISSN = {{1573-1774}},
Keywords = {{eye movement; multiple representations; reading comprehension; science
   text}},
Keywords-Plus = {{LITERACY; ILLUSTRATIONS; LEARN}},
Research-Areas = {{Education \& Educational Research}},
Web-of-Science-Categories  = {{Education \& Educational Research}},
Author-Email = {{yuehnu@ms3.ntcu.edu.tw}},
Number-of-Cited-References = {{38}},
Times-Cited = {{8}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{37}},
Journal-ISO = {{Int. J. Sci. Math. Educ.}},
Doc-Delivery-Number = {{AF4KZ}},
Unique-ID = {{ISI:000334682200001}},
DA = {{2019-10-28}},
}

@article{ ISI:000332433000015,
Author = {Chang, Yongmin and Choi, Sungmook},
Title = {{Effects of seductive details evidenced by gaze duration}},
Journal = {{NEUROBIOLOGY OF LEARNING AND MEMORY}},
Year = {{2014}},
Volume = {{109}},
Pages = {{131-138}},
Month = {{MAR}},
Abstract = {{According to a meta-analysis of empirical studies, seductive details
   such as emotionally interesting text segments and attention-grabbing
   pictures have significant negative effects on the reader's recall,
   reading comprehension, and learning of important textual information.
   This study investigates the negative effects of seductive details on
   recall of main ideas and reading comprehension by using an eye-tracking
   technique. In the experiment, a total of 56 undergraduate students read
   a block of expository text with seductive details, and the spatial and
   temporal distribution of attention was measured by gaze duration and
   recorded by an eye tracker. Then recall and reading comprehension tests
   were employed. Two multiple regression analyses were conducted to
   investigate the relationship between attention allocation and reading
   performance. The results indicate that increased attention to seductive
   sentences, not to seductive pictures, was a major determinant of poor
   performance in terms of both recall and reading comprehension,
   suggesting that increased attentional allocation to seductive sentences
   may hinder information retrieval and produce a less coherent mental
   representation of given text. (C) 2014 The Authors. Published by
   Elsevier Inc. All rights reserved.}},
Publisher = {{ACADEMIC PRESS INC ELSEVIER SCIENCE}},
Address = {{525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Choi, S (Reprint Author), Kyungpook Natl Univ, Dept English Educ, 80 Daehakro, Taegu 702701, South Korea.
   Chang, Yongmin, Kyungpook Natl Univ, Dept Mol Med, Coll Med, Taegu 702701, South Korea.
   Chang, Yongmin, Kyungpook Natl Univ Hosp, Dept Radiol, Taegu, South Korea.
   Choi, Sungmook, Kyungpook Natl Univ, Dept English Educ, Teachers Coll, Taegu 702701, South Korea.}},
DOI = {{10.1016/j.nlm.2014.01.005}},
ISSN = {{1074-7427}},
EISSN = {{1095-9564}},
Keywords = {{Seductive details; Eye tracking; Gaze duration; Recall; Reading
   comprehension}},
Keywords-Plus = {{WORKING-MEMORY SPAN; EYE-MOVEMENTS; ATTENTION; TEXT; COMPREHENSION;
   KNOWLEDGE; SELECTION; CONTEXT; RECALL; ADULTS}},
Research-Areas = {{Behavioral Sciences; Neurosciences \& Neurology; Psychology}},
Web-of-Science-Categories  = {{Behavioral Sciences; Neurosciences; Psychology; Psychology,
   Multidisciplinary}},
Author-Email = {{sungmookchoi@knu.ac.kr}},
Funding-Acknowledgement = {{Kyungpook National University A.S Research Fund}},
Funding-Text = {{This study was supported by the Kyungpook National University A.S
   Research Fund, 2012.}},
Number-of-Cited-References = {{54}},
Times-Cited = {{17}},
Usage-Count-Last-180-days = {{2}},
Usage-Count-Since-2013 = {{32}},
Journal-ISO = {{Neurobiol. Learn. Mem.}},
Doc-Delivery-Number = {{AC3PF}},
Unique-ID = {{ISI:000332433000015}},
OA = {{Other Gold}},
DA = {{2019-10-28}},
}

@article{ ISI:000334523400024,
Author = {Gliga, Teodora and Senju, Atsushi and Pettinato, Michele and Charman,
   Tony and Johnson, Mark H. and Basis Team},
Title = {{Spontaneous Belief Attribution in Younger Siblings of Children on the
   Autism Spectrum}},
Journal = {{DEVELOPMENTAL PSYCHOLOGY}},
Year = {{2014}},
Volume = {{50}},
Number = {{3}},
Pages = {{903-913}},
Month = {{MAR}},
Abstract = {{The recent development in the measurements of spontaneous mental state
   understanding, employing eye-movements instead of verbal responses, has
   opened new opportunities for understanding the developmental origin of
   ``mind-reading{''} impairments frequently described in autism spectrum
   disorders (ASDs). Our main aim was to characterize the relationship
   between mental state understanding and the broader autism phenotype,
   early in childhood. An eye-tracker was used to capture anticipatory
   looking as a measure of false beliefs attribution in 3-year-old children
   with a family history of autism (at-risk participants, n = 47) and
   controls (control participants, n = 39). Unlike controls, the at-risk
   group, independent of their clinical outcome (ASD, broader autism
   phenotype or typically developing), performed at chance. Performance was
   not related to children's verbal or general IQ, nor was it explained by
   children ``missing out{''} on crucial information, as shown by an
   analysis of visual scanning during the task. We conclude that
   difficulties with using mental state understanding for action prediction
   may be an endophenotype of autism spectrum disorders.}},
Publisher = {{AMER PSYCHOLOGICAL ASSOC}},
Address = {{750 FIRST ST NE, WASHINGTON, DC 20002-4242 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Gliga, T (Reprint Author), Univ London Birkbeck Coll, Ctr Brain \& Cognit Dev, Malet St, London WC1E 7HX, England.
   Gliga, Teodora; Senju, Atsushi; Pettinato, Michele; Johnson, Mark H., Univ London Birkbeck Coll, Ctr Brain \& Cognit Dev, London WC1E 7HX, England.
   Pettinato, Michele, UCL, Dept Speech Hearing \& Phonet Sci, London, England.
   Charman, Tony, Kings Coll London, Inst Psychiat, London WC2R 2LS, England.}},
DOI = {{10.1037/a0034146}},
ISSN = {{0012-1649}},
EISSN = {{1939-0599}},
Keywords = {{autism; family risk; false belief; eye-tracking}},
Keywords-Plus = {{FALSE-BELIEF; MIND ABILITIES; ACTION ANTICIPATION; JOINT ATTENTION; AGE;
   DISENGAGEMENT; INDIVIDUALS; PHENOTYPE; LANGUAGE; ABSENCE}},
Research-Areas = {{Psychology}},
Web-of-Science-Categories  = {{Psychology, Developmental}},
Author-Email = {{t.gliga@bbk.ac.uk}},
ResearcherID-Numbers = {{Senju, Atsushi/C-4097-2008
   Charman, Tony/A-2085-2014
   }},
ORCID-Numbers = {{Senju, Atsushi/0000-0002-8081-7170
   Charman, Tony/0000-0003-1993-6549
   Gliga, Teodora/0000-0001-8053-7286
   Pettinato, Michele/0000-0002-4457-5565
   Green, Jonathan/0000-0002-0143-181X
   Johnson, Mark/0000-0003-4229-2585}},
Funding-Acknowledgement = {{AutisticaAutistica {[}7221]; Medical Research CouncilMedical Research
   Council UK (MRC) {[}G1100252, G0701484, MR/K021389/1]}},
Number-of-Cited-References = {{49}},
Times-Cited = {{8}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{33}},
Journal-ISO = {{Dev. Psychol.}},
Doc-Delivery-Number = {{AF2DS}},
Unique-ID = {{ISI:000334523400024}},
OA = {{Green Published, Other Gold, Green Accepted}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000360096500010,
Author = {Khan, Rizwan Ahmed and Meyer, Alexandre and Konik, Hubert and Bouakaz,
   Saida},
Editor = {{Shaikh, FK and Chowdhry, BS and Zeadally, S and Hussain, DMA and Memon, AA and Uqaili, MA}},
Title = {{Automatic Analysis of Affective States: Visual Attention Based Approach}},
Booktitle = {{COMMUNICATION TECHNOLOGIES, INFORMATION SECURITY AND SUSTAINABLE
   DEVELOPMENT}},
Series = {{Communications in Computer and Information Science}},
Year = {{2014}},
Volume = {{414}},
Pages = {{98-108}},
Note = {{3rd International Multi-Topic Conference on Communication Technologies,
   Information Security and Sustainable Development (IMTIC), Jamshoro,
   PAKISTAN, DEC 18-20, 2013}},
Organization = {{Higher Educ Commiss; Natl ICT R\&D Fund; Mehran Univ Engn \& Technol}},
Abstract = {{Computing environment is moving from computer centered designs to
   human-centered designs. Human's tend to communicate wealth of
   information through affective states or expressions. Thus automatic
   analysis of user affective states have become inevitable for computer
   vision community. In this paper first we focus on understanding human
   visual system (HVS) when it decodes or recognizes facial expressions. To
   understand HVS, we have conducted psycho-visual experimental study with
   an eye-tracker, to find which facial region is perceptually more
   attractive or salient for a particular expression. Secondly, based on
   results obtained from psycho-visual experimental study we have proposed
   a novel framework for automatic analysis of affective states. Framework
   creates discriminative feature space by processing only salient facial
   regions to extract Pyramid Histogram of Orientation Gradients (PHOG)
   features. The proposed framework achieved automatic expression
   recognition accuracy of 95.3\% on extended Cohn-Kanade (CK+) facial
   expression database for six universal facial expressions. We have also
   discussed generalization capabilites of proposed framework on unseen
   data. In the last paper discusses effectiveness of proposed framework
   against low resolution image sequences.}},
Publisher = {{SPRINGER-VERLAG BERLIN}},
Address = {{HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Khan, RA (Reprint Author), Univ Lyon, CNRS, Lyon, France.
   Khan, Rizwan Ahmed; Meyer, Alexandre; Konik, Hubert; Bouakaz, Saida, Univ Lyon, CNRS, Lyon, France.
   Khan, Rizwan Ahmed; Meyer, Alexandre; Bouakaz, Saida, Univ Lyon 1, UMR5205, LIRIS, F-69622 Villeurbanne, France.
   Konik, Hubert, Univ St Etienne, UMR5516, Lab Hubert Curien, F-42000 St Etienne, France.}},
DOI = {{10.1007/978-3-319-10987-9\_10}},
ISSN = {{1865-0929}},
EISSN = {{1865-0937}},
ISBN = {{978-3-319-10986-2}},
Keywords = {{Facial expression recognition; Human vision; Eye-tracker; Pyramid
   histogram of oriented gradients; Classification}},
Keywords-Plus = {{FACIAL EXPRESSION; RECOGNITION; TEXTURE}},
Research-Areas = {{Computer Science}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Computer Science, Hardware \&
   Architecture; Computer Science, Information Systems}},
Author-Email = {{Rizwan-Ahmed.Khan@liris.cnrs.fr
   Alexandre.Meyer@liris.cnrs.fr
   Saida.Bouakaz@liris.cnrs.fr}},
ResearcherID-Numbers = {{Khan, Rizwan/N-7134-2018}},
ORCID-Numbers = {{Khan, Rizwan/0000-0003-0819-800X}},
Number-of-Cited-References = {{21}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Doc-Delivery-Number = {{BD3RD}},
Unique-ID = {{ISI:000360096500010}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000342766800044,
Author = {Uzunosmanoglu, Selin Deniz and Cakir, Murat Perit},
Editor = {{Zaphiris, P and Ioannou, A}},
Title = {{Examining an Online Collaboration Learning Environment with the Dual
   Eye-Tracking Paradigm: The Case of Virtual Math Teams}},
Booktitle = {{LEARNING AND COLLABORATION TECHNOLOGIES: DESIGNING AND DEVELOPING NOVEL
   LEARNING EXPERIENCES, PT I}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2014}},
Volume = {{8523}},
Pages = {{462-472}},
Note = {{1st International Conference on Learning and Collaboration Technologies
   (LCT), Heraklion, GREECE, JUN 22-27, 2014}},
Abstract = {{The aim of this study is to investigate the computer supported
   collaborative problem solving processes using the dual eye-tracking
   method. 18 university students participated in this study, and 9 pairs
   tried to solve 10 geometry problems using Virtual Math Team (VMT) online
   environment. Which situations the participants' eye movements, and eye
   gazes overlap, and how usability of VMT environment affect the problem
   solving processes are tried to identify. After experiments with two
   eye-trackers, a questionnaire including System Usability Scale and
   open-ended questions was filled by participants. Eye-tracker data were
   analyzed both quantitatively using cross-recurrence analysis, and
   qualitatively using interaction analysis. Analysis of eye-tracker data
   and open-ended questions are consistent, and support to each other.
   Results show that pairs collaborating with higher level have more gazes
   overlapping, more shared understanding, and anticipatory gazes than
   pairs having with low level. Also, usability of the system and awareness
   tools affect the collaboration processes.}},
Publisher = {{SPRINGER-VERLAG BERLIN}},
Address = {{HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Uzunosmanoglu, SD (Reprint Author), METU, Comp Educ \& Instruct Technol Dept, Ankara, Turkey.
   Uzunosmanoglu, Selin Deniz, METU, Comp Educ \& Instruct Technol Dept, Ankara, Turkey.
   Cakir, Murat Perit, METU, Dept Cognit Sci, Ankara, Turkey.}},
ISSN = {{0302-9743}},
ISBN = {{978-3-319-07482-5; 978-3-319-07481-8}},
Keywords = {{computer supported collaborative learning; collaborative problem
   solving; joint attention; gaze overlap; dual eye tracking}},
Research-Areas = {{Computer Science}},
Web-of-Science-Categories  = {{Computer Science, Information Systems; Computer Science, Theory \&
   Methods}},
Author-Email = {{sdeniz@metu.edu.tr
   perit@metu.edu.tr}},
Number-of-Cited-References = {{19}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{8}},
Doc-Delivery-Number = {{BB3GQ}},
Unique-ID = {{ISI:000342766800044}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000383780005032,
Author = {Bi, Youyi and Reid, Tahira N.},
Book-Group-Author = {{ASEE}},
Title = {{Understanding Students' Process for Solving Engineering Problems Using
   Eye Gaze Data}},
Booktitle = {{2014 ASEE ANNUAL CONFERENCE}},
Series = {{ASEE Annual Conference \& Exposition}},
Year = {{2014}},
Note = {{ASEE Annual Conference, Indianapolis, IN, JUN 15-18, 2014}},
Organization = {{ASEE}},
Abstract = {{It is well known that engineering is considered one of the more
   demanding fields of study to embark on. In mechanical engineering,
   courses such as thermodynamics, statics, mechanics of materials and
   others are perceived as challenging by students. Several factors impact
   students' ability to solve problems presented in these courses,
   including the ability to visualize the abstract concepts presented to
   them. Exams and homework assignments are among the standard tools used
   to assess students' performance and comprehension of course material.
   Student ability is determined by the quality of the written answers and
   by how well they document the process used to solve a problem. However,
   they provide only limited opportunities to reveal the viewing strategies
   used that may give additional insight into how students initially
   approach the given problem.
   In the present study, we use a within-subject experimental design to
   investigate the relationship between spatial visualization abilities of
   students and how students solve specific problems in the area of
   mechanics of materials. We employ a non-invasive eye-tracker (Tobii
   X-60) to record participants' eye movements during each problem solving
   task. According to the eye-mind hypothesis, people look at what they are
   thinking about. Participants were asked to solve several problems in the
   field of mechanics of materials, and the diagram of each problem was
   shown on a computer display. The data collected included: participants'
   fixation time, fixation counts, and scan paths of the critical areas of
   each diagram. The data were correlated with students' performance on
   Purdue Spatial Visualization Test and solid mechanics problems.
   The preliminary results show differences in the eye gaze data of high
   and low performance participants and provide insight into students'
   problem-solving strategies and difficulties, offering instructors new
   facts to adopt appropriate teaching methods for different students.}},
Publisher = {{AMER SOC ENGINEERING EDUCATION}},
Address = {{1818 N STREET, NW SUITE 600, WASHINGTON, DC 20036 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Bi, YY (Reprint Author), Purdue Univ, Sch Mech Engn, W Lafayette, IN 47907 USA.
   Bi, Youyi; Reid, Tahira N., Purdue Univ, Sch Mech Engn, W Lafayette, IN 47907 USA.}},
ISSN = {{2153-5965}},
Keywords = {{engineering problem-solving; eye gaze data; visual attention}},
Keywords-Plus = {{MOVEMENTS; ATTENTION; DESIGN; INFORMATION; PERCEPTION; SEQUENCES;
   FIXATIONS; TASK}},
Research-Areas = {{Education \& Educational Research; Engineering}},
Web-of-Science-Categories  = {{Education \& Educational Research; Education, Scientific Disciplines;
   Engineering, Multidisciplinary}},
Number-of-Cited-References = {{60}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{1}},
Doc-Delivery-Number = {{BF6XW}},
Unique-ID = {{ISI:000383780005032}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000387829200036,
Author = {Fritz, Thomas and Begel, Andrew and Mueller, Sebastian C. and
   Yigit-Elliott, Serap and Zuger, Manuela},
Editor = {{Jalote, P and Briand, L and VanderHoek, A}},
Title = {{Using Psycho-Physiological Measures to Assess Task Difficulty in
   Software Development}},
Booktitle = {{36TH INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING (ICSE 2014)}},
Year = {{2014}},
Pages = {{402-413}},
Note = {{36th International Conference on Software Engineering (ICSE), Hyderabad,
   INDIA, MAY 31-JUN 07, 2014-2015}},
Organization = {{Accenture; HCL; SAP; Google; IBM; Fonds Natl Rech Luxembourg; Bosch;
   Progress; Talent Sprint; CA Technologies; S\&P Capital IQ, McGraw Hill
   Financial; Broadridge; Assoc Comp Machinery; SIGSOFT; iSoft; IEEE Comp
   Soc; Tech Council Software Engn; SIGSE; IIID}},
Abstract = {{Software developers make programming mistakes that cause serious bugs
   for their customers. Existing work to detect problematic software
   focuses mainly on post hoc identification of correlations between bug
   fixes and code. We propose a new approach to address this problem detect
   when software developers are experiencing difficulty while they work on
   their programming tasks, and stop them before they can introduce bugs
   into the code.
   In this paper, we investigate a novel approach to classify the
   difficulty of code comprehension tasks using data from
   psycho-physiological sensors. We present the results of a study we
   conducted with 15 professional programmers to see how well an
   eye-tracker, an electrodermal activity sensor, and an
   electroencephalography sensor could be used to predict whether
   developers would find a task to be difficult. We can predict nominal
   task difficulty (easy/difficult) for a new developer with 64.99\%
   precision and 64.58\% recall, and for a new task with 84.38\% precision
   and 69.79\% recall. We can improve the Naive Bayes classifier's
   performance if we trained it on just the eye-tracking data over the
   entire dataset, or by using a sliding window data collection schema with
   a 55 second time window. Our work brings the community closer to a
   viable and reliable measure of task difficulty that could power the next
   generation of programming support tools.}},
Publisher = {{ASSOC COMPUTING MACHINERY}},
Address = {{1515 BROADWAY, NEW YORK, NY 10036-9998 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Fritz, T (Reprint Author), Univ Zurich, Zurich, Switzerland.
   Fritz, Thomas; Mueller, Sebastian C.; Zuger, Manuela, Univ Zurich, Zurich, Switzerland.
   Begel, Andrew, Microsoft Res, Redmond, WA USA.
   Yigit-Elliott, Serap, Exponent, Bellevue, WA USA.}},
DOI = {{10.1145/2568225.2568266}},
ISBN = {{978-1-4503-2756-5}},
Keywords = {{psycho-physiological; task difficulty; study}},
Keywords-Plus = {{MENTAL WORKLOAD; EYE-MOVEMENTS; RESPONSES; LOAD; BLINKING; EYEBLINK;
   FLIGHT; STATE}},
Research-Areas = {{Computer Science}},
Web-of-Science-Categories  = {{Computer Science, Software Engineering}},
Number-of-Cited-References = {{72}},
Times-Cited = {{55}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{3}},
Doc-Delivery-Number = {{BG3AP}},
Unique-ID = {{ISI:000387829200036}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000396448000031,
Author = {Batemanazan, Vanisri and Jaafar, Azizah and Salehuddin, Khazriyati},
Editor = {{Vengadasamy, R and Amir, Z and Noor, NM}},
Title = {{A comparative study on the eye movement patterns in Malay-English
   bilingual readers}},
Booktitle = {{INTERNATIONAL CONFERENCE ON KNOWLEDGE-INNOVATION-EXCELLENCE: SYNERGY IN
   LANGUAGE RESEARCH AND PRACTICE (2013)}},
Series = {{Procedia Social and Behavioral Sciences}},
Year = {{2014}},
Volume = {{118}},
Pages = {{229-234}},
Note = {{International Conference on Knowledge-Innovation-Excellence - Synergy in
   Language Research and Practice (SoLLs.INTEC), Cyberjaya, MALAYSIA, JUL
   02-03, 2013}},
Organization = {{Natl Univ Malaysia, Fac Social Sci \& Humanities, Sch Language Studies
   \& Linguist}},
Abstract = {{During the process of reading, the eyes do not move continuously but
   jump from one word to another, and they also stop at particular points.
   In eye-tracking studies, these processes are called `saccade' and
   `fixation'. This paper presents an analysis of the reading patterns of
   readers reading Malay and English sentences and questions using the eye
   tracker. Malay-English bilingual readers read a set of statements in
   Malay and English and their reading comprehension was tested using
   questions with different levels of difficulty. Their correct responses
   and reactions to the comprehension questions are also presented and
   discussed in this paper. (C) 2013 The Authors. Published by Elsevier
   Ltd.}},
Publisher = {{ELSEVIER SCIENCE BV}},
Address = {{SARA BURGERHARTSTRAAT 25, PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Salehuddin, K (Reprint Author), Univ Kebangsaan Malaysia, Fac Humanities \& Social Sci, Ukm Bangi 43600, Selangor, Malaysia.
   Batemanazan, Vanisri, Univ Kebangsaan Malaysia, Fac Informat Sci \& Techonol, Ukm Bangi 43600, Selangor, Malaysia.
   Jaafar, Azizah; Salehuddin, Khazriyati, Univ Kebangsaan Malaysia, Fac Humanities \& Social Sci, Ukm Bangi 43600, Selangor, Malaysia.}},
DOI = {{10.1016/j.sbspro.2014.02.031}},
ISSN = {{1877-0428}},
Keywords = {{eye movement; bilingual readers; saccade; fixation; eye tracker}},
Keywords-Plus = {{PARAFOVEAL PREVIEW BENEFIT; COMPREHENSION; HEBREW; WORDS; TEXT}},
Research-Areas = {{Education \& Educational Research; Linguistics}},
Web-of-Science-Categories  = {{Education \& Educational Research; Language \& Linguistics}},
Author-Email = {{khazdin@ukm.my}},
Funding-Acknowledgement = {{Ministry of Education of Malaysia {[}ERGS/1/2011/SSI/UKM/02/5]}},
Funding-Text = {{This research was funded by the Ministry of Education of Malaysia, under
   the ERGS/1/2011/SSI/UKM/02/5. Research grant. Special gratitude is given
   to Albercht Inhoff and Weger for sharing their 2005 experimental
   stimulus as a guide. The authors would like to thank all participants
   who participated in the Experiment.}},
Number-of-Cited-References = {{15}},
Times-Cited = {{2}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Doc-Delivery-Number = {{BH0TJ}},
Unique-ID = {{ISI:000396448000031}},
OA = {{Bronze}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000432659500002,
Author = {Park, Ukeob and Mallipeddi, Rammohan and Lee, Minho},
Editor = {{Loo, CK and Yap, KS and Wong, KW and Teoh, A and Huang, K}},
Title = {{Human Implicit Intent Discrimination Using EEG and Eye Movement}},
Booktitle = {{NEURAL INFORMATION PROCESSING (ICONIP 2014), PT I}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2014}},
Volume = {{8834}},
Pages = {{11-18}},
Note = {{21st International Conference on Neural Information Processing (ICONIP),
   Kuching, MALAYSIA, NOV 03-06, 2014}},
Abstract = {{In this paper, we propose a new human implicit intent understanding
   model based on multi-modal information, which is a combination of eye
   movement data and brain wave signal obtained from eye-tracker and
   Electroencephalography (EEG) sensors respectively. From the eye movement
   data, we extract human implicit intention related to features such as
   fixation count and fixation duration corresponding to the areas of
   interest (AOI). Also, we analyze the EEG signals based on phase
   synchrony method. Combining the eye movement and EEG information, we
   train several classifiers such as support vector machine classifier,
   Gaussian Mixture Model and Naive Bayesian, which can successfully
   identify the human's implicit intention into two defined categories,
   i.e. navigational and informational intentions. Experimental results
   show that the human implicit intention can be better understood using
   multimodal information.}},
Publisher = {{SPRINGER INTERNATIONAL PUBLISHING AG}},
Address = {{GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Lee, M (Reprint Author), Kyungpook Natl Univ, Sch Elect Engn, 1370 Sankyuk Dong, Taegu 702701, South Korea.
   Park, Ukeob, Kyungpook Natl Univ, Dept Robot Engn, Taegu 702701, South Korea.
   Mallipeddi, Rammohan; Lee, Minho, Kyungpook Natl Univ, Sch Elect Engn, 1370 Sankyuk Dong, Taegu 702701, South Korea.}},
ISSN = {{0302-9743}},
EISSN = {{1611-3349}},
ISBN = {{978-3-319-12637-1}},
Keywords = {{brain-computer interface (BCI); electroencephalographic (EEG); eye
   movement; phase synchrony; intent recognition; multi-modality}},
Keywords-Plus = {{SIGNALS}},
Research-Areas = {{Computer Science}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Computer Science, Theory \&
   Methods}},
Author-Email = {{uepark@ee.knu.ac.kr
   mallipeddi.ram@gmail.com
   mholee@gmail.com}},
Funding-Acknowledgement = {{Original Technology Research Program for Brain Science through the
   National Research Foundation of Korea (NRF) - Ministry of Education,
   Science and Technology {[}2014-028219]; Basic Science Research Program
   through the National Research Foundation of Korea (NRF) - Ministry of
   Science, ICT and future Planning {[}2013R1A2A2A01068687]}},
Funding-Text = {{This research was supported by the Original Technology Research Program
   for Brain Science through the National Research Foundation of Korea
   (NRF) funded by the Ministry of Education, Science and Technology
   (2014-028219) (50\%) and also Basic Science Research Program through the
   National Research Foundation of Korea (NRF) funded by the Ministry of
   Science, ICT and future Planning (2013R1A2A2A01068687) (50\%).}},
Number-of-Cited-References = {{13}},
Times-Cited = {{5}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Doc-Delivery-Number = {{BK2GB}},
Unique-ID = {{ISI:000432659500002}},
DA = {{2019-10-28}},
}

@article{ ISI:000328730300110,
Author = {Bulf, Hermann and Valenza, Eloisa and Turati, Chiara},
Title = {{How a Hat May Affect 3-Month-Olds' Recognition of a Face: An
   Eye-Tracking Study}},
Journal = {{PLOS ONE}},
Year = {{2013}},
Volume = {{8}},
Number = {{12}},
Month = {{DEC 11}},
Abstract = {{Recent studies have shown that infants' face recognition rests on a
   robust face representation that is resilient to a variety of facial
   transformations such as rotations in depth, motion, occlusion or
   deprivation of inner/outer features. Here, we investigated whether
   3-month-old infants' ability to represent the invariant aspects of a
   face is affected by the presence of an external add-on element, i.e. a
   hat. Using a visual habituation task, three experiments were carried out
   in which face recognition was investigated by manipulating the
   presence/absence of a hat during face encoding (i.e. habituation phase)
   and face recognition (i.e. test phase). An eye-tracker system was used
   to record the time infants spent looking at face-relevant information
   compared to the hat. The results showed that infants' face recognition
   was not affected by the presence of the external element when the type
   of the hat did not vary between the habituation and test phases, and
   when both the novel and the familiar face wore the same hat during the
   test phase (Experiment 1). Infants' ability to recognize the invariant
   aspects of a face was preserved also when the hat was absent in the
   habituation phase and the same hat was shown only during the test phase
   (Experiment 2). Conversely, when the novel face identity competed with a
   novel hat, the hat triggered the infants' attention, interfering with
   the recognition process and preventing the infants' preference for the
   novel face during the test phase (Experiment 3). Findings from the
   current study shed light on how faces and objects are processed when
   they are simultaneously presented in the same visual scene, contributing
   to an understanding of how infants respond to the multiple and composite
   information available in their surrounding environment.}},
Publisher = {{PUBLIC LIBRARY SCIENCE}},
Address = {{1160 BATTERY STREET, STE 100, SAN FRANCISCO, CA 94111 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Bulf, H (Reprint Author), Univ Milano Bicocca, Dept Psychol, Milan, Italy.
   Bulf, Hermann; Turati, Chiara, Univ Milano Bicocca, Dept Psychol, Milan, Italy.
   Valenza, Eloisa, Univ Padua, Dept Dev \& Socializat Psychol, Padua, Italy.
   Valenza, Eloisa, Univ Padua, Interdept Ctr Cognit Sci, Padua, Italy.}},
DOI = {{10.1371/journal.pone.0082839}},
Article-Number = {{e82839}},
ISSN = {{1932-6203}},
Keywords-Plus = {{MOTHERS FACE; NEWBORNS; PREFERENCE; INFANTS; REPRESENTATION; MEMORY}},
Research-Areas = {{Science \& Technology - Other Topics}},
Web-of-Science-Categories  = {{Multidisciplinary Sciences}},
Author-Email = {{hermann.bulf@unimib.it}},
Funding-Acknowledgement = {{Universita degli Studi di Padova, Progetti di Ricerca di Ateneo
   {[}CPDA113109/11]; European Research Council Starting GrantEuropean
   Research Council (ERC) {[}241176]}},
Funding-Text = {{This research project was supported by a grant from Universita degli
   Studi di Padova, Progetti di Ricerca di Ateneo (code CPDA113109/11), and
   by a European Research Council Starting Grant to Dr Chiara Turati -
   ODMIR No. 241176. The funders had no role in study design, data
   collection and analysis, decision to publish, or preparation of the
   manuscript.}},
Number-of-Cited-References = {{25}},
Times-Cited = {{1}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{14}},
Journal-ISO = {{PLoS One}},
Doc-Delivery-Number = {{276EG}},
Unique-ID = {{ISI:000328730300110}},
OA = {{DOAJ Gold, Green Published}},
DA = {{2019-10-28}},
}

@article{ ISI:000324114500006,
Author = {Kim, Eun H. and Alvarez, Tara L.},
Title = {{The horizontal dark oculomotor rest position}},
Journal = {{GRAEFES ARCHIVE FOR CLINICAL AND EXPERIMENTAL OPHTHALMOLOGY}},
Year = {{2013}},
Volume = {{251}},
Number = {{9}},
Pages = {{2119-2130}},
Month = {{SEP}},
Abstract = {{This study sought to investigate whether eye dominance and age are
   related to the stimulus-free oculomotor resting state described via the
   dark disconjugate position (near or far), the dark conjugate position
   (left to right), and the near dissociated phoria.
   Nineteen non-presbyopes and 25 presbyopes with normal binocular vision
   participated in two identical sessions. The left-eye and the right-eye
   positions were recorded using a video-based infrared eye tracker while
   the subjects were in total darkness. Dark disconjugate responses and
   dark conjugate responses were calculated by computing the difference and
   the average of the left-eye and the right-eye response, respectively.
   The right-eye decaying to the phoria level was recorded for 15 s.
   A one-way ANOVA assessed statistical differences in dark conjugate and
   dark disconjugate positions, comparing 1) the right-eye and the left-eye
   sensory and/or motor dominant groups and 2) the non-presbyope and
   presbyope groups. The test-retests of the dark disconjugate position,
   the dark conjugate position and the near dissociated heterophoria were
   high between sessions (r > 0.85; p < 0.00001). For non-presbyopes the
   right-eye (left-eye) motor and sensory dominant subjects showed a
   rightward (leftward) dark conjugate position (p < 0.01). The dark
   disconjugate position was receded in presbyopes compared to
   non-presbyopes (p < 0.0001).
   The data support that the left-eye, or the right-eye, motor and sensory
   dominance predicts the direction of the dark conjugate position. Future
   studies could investigate the underlying neural substrates that may, in
   part, contribute to the resting state of the oculomotor system in a
   stimulus-free environment. Knowledge of the brain-behavior governing
   visual-field preference has implications for understanding the natural
   aging process of the visual system.}},
Publisher = {{SPRINGER}},
Address = {{233 SPRING ST, NEW YORK, NY 10013 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Alvarez, TL (Reprint Author), New Jersey Inst Technol, Dept Biomed Engn, Newark, NJ 07102 USA.
   Kim, Eun H.; Alvarez, Tara L., New Jersey Inst Technol, Dept Biomed Engn, Newark, NJ 07102 USA.}},
DOI = {{10.1007/s00417-013-2379-3}},
ISSN = {{0721-832X}},
EISSN = {{1435-702X}},
Keywords = {{Motor eye dominance; Sensory eye dominance; Dark adaptation; Oculomotor
   rest position; Dark conjugate position; Dark disconjugate position}},
Keywords-Plus = {{VISUAL EVOKED-POTENTIALS; OCULAR DOMINANCE; EYE-MOVEMENTS;
   SPHERICAL/ASTIGMATIC ANISOMETROPIA; HEMISPHERIC-ASYMMETRY; PEAK
   VELOCITY; SEX ANALYSIS; VERGENCE; CONVERGENCE; AGE}},
Research-Areas = {{Ophthalmology}},
Web-of-Science-Categories  = {{Ophthalmology}},
Author-Email = {{Alvarez@adm.njit.edu}},
ORCID-Numbers = {{Alvarez, Tara/0000-0002-8797-1613}},
Funding-Acknowledgement = {{NSF MRINational Science Foundation (NSF)NSF - Office of the Director
   (OD) {[}CBET1228254]}},
Funding-Text = {{Grants This research was supported in part by NSF MRI CBET1228254 to
   TLA.}},
Number-of-Cited-References = {{74}},
Times-Cited = {{2}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{9}},
Journal-ISO = {{Graefes Arch. Clin. Exp. Ophthalmol.}},
Doc-Delivery-Number = {{214ES}},
Unique-ID = {{ISI:000324114500006}},
DA = {{2019-10-28}},
}

@article{ ISI:000322351100016,
Author = {Cagiltay, Nergiz Ercil and Tokdemir, Gul and Kilic, Ozkan and Topalli,
   Damla},
Title = {{Performing and analyzing non-formal inspections of entity relationship
   diagram (ERD)}},
Journal = {{JOURNAL OF SYSTEMS AND SOFTWARE}},
Year = {{2013}},
Volume = {{86}},
Number = {{8}},
Pages = {{2184-2195}},
Month = {{AUG}},
Abstract = {{Designing and understanding of diagrammatic representations is a
   critical issue for the success of software projects because diagrams in
   this field provide a collection of related information with various
   perceptual signs and they help software engineers to understand
   operational systems at different levels of information system
   development process. Entity relationship diagram (ERD) is one of the
   main diagrammatic representations of a conceptual data model that
   reflects users' data requirements in a database system. In today's
   business environment, the business model is in a constant change which
   creates highly dynamic data requirements which also requires additional
   processes like modifications of ERD. However, in the literature there
   are not many measures to better understand the behaviors of software
   engineers during designing and understanding these representations.
   Hence, the main motivation of this study is to develop measures to
   better understand performance of software engineers during their
   understanding process of ERD. Accordingly, this study proposes two
   measures for ERD defect detection process. The defect detection
   difficulty level (DF) measures how difficult a defect to be detected
   according to the other defects for a group of software engineers. Defect
   detection performance (PP) measure is also proposed to understand the
   performance of a software engineer during the defect detection process.
   The results of this study are validated through the eye tracker data
   collected during the defect detection process of participants.
   Additionally, a relationship between the defect detection performance
   (PP) of a software engineer and his/her search patterns within an ERD is
   analyzed. Second experiment with five participants is also conducted to
   show the correlation between the proposed metric results and eye tracker
   data. The results of experiment-2 also found to be similar for DF and PP
   values. The results of this study are expected to provide insights to
   the researchers, software companies, and to the educators to improve ERD
   reasoning process. Through these measures several design guidelines can
   be developed for better graphical representations and modeling of the
   information which would improve quality of these diagrams. Moreover,
   some reviewing instructions can be developed for the software engineers
   to improve their reviewing process in ERD. These guidelines in turn will
   provide some tools for the educators to improve design and review skills
   of future software engineers. (c) 2013 Elsevier Inc. All rights
   reserved.}},
Publisher = {{ELSEVIER SCIENCE INC}},
Address = {{360 PARK AVE SOUTH, NEW YORK, NY 10010-1710 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Kilic, O (Reprint Author), Middle E Tech Univ, Inst Informat, TR-06531 Ankara, Turkey.
   Cagiltay, Nergiz Ercil, Atilim Univ, Software Engn Dept, Ankara, Turkey.
   Tokdemir, Gul, Cankaya Univ, Dept Comp Engn, Ankara, Turkey.
   Kilic, Ozkan, Middle E Tech Univ, Inst Informat, TR-06531 Ankara, Turkey.
   Topalli, Damla, Atilim Univ, Dept Comp Engn, Ankara, Turkey.}},
DOI = {{10.1016/j.jss.2013.03.106}},
ISSN = {{0164-1212}},
EISSN = {{1873-1228}},
Keywords = {{Defect detection; ERD; Eye tracking}},
Keywords-Plus = {{READING TECHNIQUES; EYE-MOVEMENTS; INFORMATION; QUALITY}},
Research-Areas = {{Computer Science}},
Web-of-Science-Categories  = {{Computer Science, Software Engineering; Computer Science, Theory \&
   Methods}},
Author-Email = {{nergiz@atilim.edu.tr
   gtokdemir@cankaya.edu.tr
   okilic@ii.metu.edu.tr
   dtopalli@atilim.edu.tr}},
ResearcherID-Numbers = {{Cagiltay, Nergiz/O-3082-2019}},
Number-of-Cited-References = {{44}},
Times-Cited = {{3}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{15}},
Journal-ISO = {{J. Syst. Softw.}},
Doc-Delivery-Number = {{190OW}},
Unique-ID = {{ISI:000322351100016}},
DA = {{2019-10-28}},
}

@article{ ISI:000321588700001,
Author = {Antunez, Lucia and Vidal, Leticia and Sapolinski, Alejandra and Gimenez,
   Ana and Maiche, Alejandro and Ares, Gaston},
Title = {{How do design features influence consumer attention when looking for
   nutritional information on food labels? Results from an eye-tracking
   study on pan bread labels}},
Journal = {{INTERNATIONAL JOURNAL OF FOOD SCIENCES AND NUTRITION}},
Year = {{2013}},
Volume = {{64}},
Number = {{5}},
Pages = {{515-527}},
Month = {{AUG}},
Abstract = {{The aim of this work was to evaluate consumer visual processing of food
   labels when evaluating the salt content of pan bread labels and to study
   the influence of label design and nutritional labelling format on
   consumer attention. A total of 16 pan bread labels, designed according
   to a full factorial design, were presented to 52 participants, who were
   asked to decide whether the sodium content of each label was medium or
   low, while their eye movements were recorded using an eye tracker.
   Results showed that most participants looked at nutrition labels and the
   traffic light system to conclude on the salt content of the labels.
   However, the average percentage of participants who looked at the actual
   sodium content was much lower. Nutrition information format affected
   participants' processing of nutrition information. Among other effects,
   the inclusion of the traffic light system increased participants'
   attention towards some kind of nutrition information and facilitated its
   processing, but not its understanding.}},
Publisher = {{TAYLOR \& FRANCIS LTD}},
Address = {{2-4 PARK SQUARE, MILTON PARK, ABINGDON OR14 4RN, OXON, ENGLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Ares, G (Reprint Author), Univ Republica, Fac Quim, Dept Ciencia \& Tecnol Alimentos, Gen Flores 2124, Montevideo 11800, Uruguay.
   Antunez, Lucia; Vidal, Leticia; Sapolinski, Alejandra; Gimenez, Ana; Ares, Gaston, Univ Republica, Fac Quim, Dept Ciencia \& Tecnol Alimentos, Montevideo 11800, Uruguay.
   Antunez, Lucia; Gimenez, Ana; Maiche, Alejandro; Ares, Gaston, Univ Republica, Fac Psicol, Ctr Invest Basica Psicol CIBPsi, Montevideo 11800, Uruguay.}},
DOI = {{10.3109/09637486.2012.759187}},
ISSN = {{0963-7486}},
EISSN = {{1465-3478}},
Keywords = {{nutritional information; food labels; front-of-pack; eye tracking;
   attention}},
Keywords-Plus = {{FRONT-OF-PACK; VISUAL-SEARCH; PERCEPTION; CAPTURE; CHOICES}},
Research-Areas = {{Food Science \& Technology; Nutrition \& Dietetics}},
Web-of-Science-Categories  = {{Food Science \& Technology; Nutrition \& Dietetics}},
Author-Email = {{gares@fq.edu.uy}},
ORCID-Numbers = {{Antunez, Lucia/0000-0003-2497-6609
   Vidal, Leticia/0000-0001-6781-9852}},
Funding-Acknowledgement = {{Agencia Nacional de Investigacion e Innovacion (ANII, Uruguay); Espacio
   Interdisciplinario (Universidad de la Republica, Uruguay)}},
Funding-Text = {{The authors are indebted to Agencia Nacional de Investigacion e
   Innovacion (ANII, Uruguay) for the scholarship granted to Lucia Antunez
   and to Espacio Interdisciplinario (Universidad de la Republica, Uruguay)
   for financial support. The authors report no conflicts of interest. The
   authors alone are responsible for the content and writing of the paper.}},
Number-of-Cited-References = {{34}},
Times-Cited = {{29}},
Usage-Count-Last-180-days = {{2}},
Usage-Count-Since-2013 = {{67}},
Journal-ISO = {{Int. J. Food Sci. Nutr.}},
Doc-Delivery-Number = {{180IZ}},
Unique-ID = {{ISI:000321588700001}},
DA = {{2019-10-28}},
}

@article{ ISI:000209525900019,
Author = {Sivarajah, Yathunanthan and Holden, Eun-Jung and Togneri, Roberto and
   Dentith, Michael},
Title = {{Identifying effective interpretation methods for magnetic data by
   profiling and analyzing human data interactions}},
Journal = {{INTERPRETATION-A JOURNAL OF SUBSURFACE CHARACTERIZATION}},
Year = {{2013}},
Volume = {{1}},
Number = {{1}},
Pages = {{T45-T55}},
Month = {{AUG}},
Abstract = {{Geoscientific data interpretation is a highly subjective and complex
   task because human intuition and biases play a significant role. Based
   on these interpretations, however, the mining and petroleum industries
   make decisions with paramount financial and environmental implications.
   To improve the accuracy and efficacy of these interpretations, it is
   important to better understand the interpretation process and the impact
   of different interpretation techniques, including data processing and
   display methods. As a first step toward this goal, we aim to
   quantitatively analyze the variability in geophysical data
   interpretation between and within individuals. We carried out an
   experiment to analyze how individuals interact with magnetic data during
   the process of identifying prescribed targets. Participants performed
   two target spotting exercises where the same magnetic image was
   presented at different orientations. The task was to identify the
   magnetic response from porphyry-style intrusive systems. The experiment
   involved analyzing the data observation pattern during the
   interpretation process using an eye tracker system that captures the
   interpreter's eye gaze motion and the target-spotting performance. The
   time at which targets were identified was also recorded. Fourteen
   participants with varying degrees of experience and expertise
   participated in this study. The results show inconsistencies within and
   between the interpreters in target-spotting performance. The results
   show a correlation between a systematic data observation pattern and
   target-spotting performance. Improved target-spotting performance was
   obtained when the magnetic image was observed from multiple
   orientations. These findings will help to identify and quantify the
   effective interpretation practices, which can provide a roadmap for the
   training of geoscientific data interpreters and contribute toward the
   understanding of the uncertainties in the data interpretation process.}},
Publisher = {{SOC EXPLORATION GEOPHYSICISTS}},
Address = {{8801 S YALE ST, TULSA, OK 74137 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Sivarajah, Y (Reprint Author), Univ Western Australia, Ctr Explorat Targeting, Perth, WA 6009, Australia.
   Sivarajah, Yathunanthan; Holden, Eun-Jung; Dentith, Michael, Univ Western Australia, Ctr Explorat Targeting, Perth, WA 6009, Australia.
   Togneri, Roberto, Univ Western Australia, Elect Elect \& Comp Engn, Perth, WA 6009, Australia.}},
DOI = {{10.1190/INT-2013-0002.1}},
ISSN = {{2324-8858}},
EISSN = {{2324-8866}},
Research-Areas = {{Geochemistry \& Geophysics}},
Web-of-Science-Categories  = {{Geochemistry \& Geophysics}},
Author-Email = {{sivary01@student.uwa.edu.au
   eun-jung.holden@uwa.edu.au
   roberto.togneri@uwa.edu.au
   michael.dentith@uwa.edu.au}},
ORCID-Numbers = {{Holden, Eun-Jung/0000-0002-8752-1639
   Togneri, Roberto/0000-0002-3778-4633}},
Funding-Acknowledgement = {{University of Western Australia}},
Funding-Text = {{We would like to acknowledge IVEC for the loan of the eye tracker. We
   would also like to thank the volunteer geoscientists who participated in
   the experiments; and Barrick Gold of Australia Ltd. for the permission
   to use their data in this research. We also acknowledge The University
   of Western Australia for providing a Scholarship for International
   Research Fees (SIRF) for this study.}},
Number-of-Cited-References = {{15}},
Times-Cited = {{4}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{2}},
Journal-ISO = {{Interpretation}},
Doc-Delivery-Number = {{V41DC}},
Unique-ID = {{ISI:000209525900019}},
DA = {{2019-10-28}},
}

@article{ ISI:000317883300005,
Author = {Schmitow, Clara and Kochukhova, Olga},
Title = {{How infants look at others' manual interactions: The role of experience}},
Journal = {{INFANT BEHAVIOR \& DEVELOPMENT}},
Year = {{2013}},
Volume = {{36}},
Number = {{2}},
Pages = {{223-227}},
Month = {{APR}},
Abstract = {{Human actions are often embedded in contexts of social interactions.
   However, just a few studies that have explored the development of
   infants' understanding of other people's manual actions do take this
   variable into account. In this study, 10- and 18-month-old infants were
   shown three interactive manual actions which the infants could or could
   not perform themselves. The infants' gaze shifts to the action target
   were recorded with an eye tracker. The results indicated that
   18-month-old infants look faster to the target than their younger
   counterparts when they observe actions that they can perform themselves.
   The results suggest that the infants' own capacity to perform an action
   facilitates understanding of the goal of the action in a social
   interaction. (C) 2013 Elsevier Inc. All rights reserved.}},
Publisher = {{ELSEVIER SCIENCE INC}},
Address = {{360 PARK AVE SOUTH, NEW YORK, NY 10010-1710 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Kochukhova, O (Reprint Author), Uppsala Univ, Dept Psychol, Box 1225, SE-75142 Uppsala, Sweden.
   Schmitow, Clara; Kochukhova, Olga, Uppsala Univ, Dept Psychol, SE-75142 Uppsala, Sweden.}},
DOI = {{10.1016/j.infbeh.2013.01.002}},
ISSN = {{0163-6383}},
Keywords = {{Goal understanding; Infants; Experience; Social interactions; Eye
   tracking}},
Keywords-Plus = {{EYE-MOVEMENTS; OBJECT MANIPULATION; GAZE SHIFTS; PERCEPTION; CHILDREN;
   ADULTS; BALL}},
Research-Areas = {{Psychology}},
Web-of-Science-Categories  = {{Psychology, Developmental}},
Author-Email = {{Olga.Kochukhova@psyk.uu.se}},
Number-of-Cited-References = {{27}},
Times-Cited = {{1}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{13}},
Journal-ISO = {{Infant Behav. Dev.}},
Doc-Delivery-Number = {{129ZU}},
Unique-ID = {{ISI:000317883300005}},
DA = {{2019-10-28}},
}

@article{ ISI:000326655900001,
Author = {van Rooy, Bertus and Pretorius, Elizabeth J.},
Title = {{Is reading in an agglutinating language different from an analytic
   language? An analysis of isiZulu and English reading based on eye
   movements}},
Journal = {{SOUTHERN AFRICAN LINGUISTICS AND APPLIED LANGUAGE STUDIES}},
Year = {{2013}},
Volume = {{31}},
Number = {{3}},
Pages = {{281-297}},
Abstract = {{In this article we examine the reading ability of Grade 4 learners in
   their home language, isiZulu and in English (first additional language),
   using both eye-tracking activities and traditional pen-and-paper reading
   comprehension assessment. The aim of the study was twofold: firstly, to
   compare bilingual reading performance in an agglutinating language
   (isiZulu) and an analytic language (English). The learners' eye-tracking
   profiles were obtained in both languages to see how they differed across
   the two languages and their eye-tracking profiles were also analysed
   according to their comprehension performance in both languages.
   Secondly, the eye-tracking profiles in both languages were also analysed
   in terms of reading ability, to determine how eye-tracking profiles
   differed among strong, average and weak readers in the two languages. In
   general, pen-and-paper tests showed that the entire group read with
   relatively low comprehension. The main findings from the eye tracker
   showed significant differences when the learners read in the two
   languages, on nearly all the selected eye-tracking variables. The eye
   movement profiles in isiZulu may be attributable to the longer word
   units in the conjunctive orthography of isiZulu. However, although there
   were several significant differences in eye movements between the
   reading ability groups in English, differences in eye movements due to
   differential reading skill did not emerge strongly in isiZulu. This may
   be due to floor effects in novice isiZulu readers or to a longer
   developmental trajectory in the early stages of reading in isiZulu. Some
   implications for early reading instruction in isiZulu are briefly
   considered.}},
Publisher = {{NATL INQUIRY SERVICES CENTRE PTY LTD}},
Address = {{19 WORCESTER STREET, PO BOX 377, GRAHAMSTOWN 6140, SOUTH AFRICA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Pretorius, EJ (Reprint Author), Univ S Africa, Dept Linguist, POB 392, ZA-0003 Pretoria, South Africa.
   van Rooy, Bertus, North West Univ, Sch Languages, Vanderbijlpark, South Africa.
   Pretorius, Elizabeth J., Univ S Africa, Dept Linguist, ZA-0003 Pretoria, South Africa.}},
DOI = {{10.2989/16073614.2013.837603}},
ISSN = {{1607-3614}},
EISSN = {{1727-9461}},
Keywords-Plus = {{ACQUISITION; ATTENTION; SKILLS}},
Research-Areas = {{Linguistics}},
Web-of-Science-Categories  = {{Linguistics; Language \& Linguistics}},
Author-Email = {{pretoej@unisa.ac.za}},
ResearcherID-Numbers = {{Pretorius, Elizabeth/E-9459-2019}},
ORCID-Numbers = {{Pretorius, Elizabeth/0000-0003-2137-1604}},
Number-of-Cited-References = {{30}},
Times-Cited = {{5}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{8}},
Journal-ISO = {{South. Afr. Linguist. Appl. Lang.}},
Doc-Delivery-Number = {{247XB}},
Unique-ID = {{ISI:000326655900001}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000321825100033,
Author = {Quan Huynh-Thu and Vienne, Cyril and Blonde, Laurent},
Editor = {{Rogowitz, BE and Pappas, TN and DeRidder, H}},
Title = {{Visual storytelling in 2D and stereoscopic 3D video: effect of blur on
   visual attention}},
Booktitle = {{HUMAN VISION AND ELECTRONIC IMAGING XVIII}},
Series = {{Proceedings of SPIE}},
Year = {{2013}},
Volume = {{8651}},
Note = {{Conference on Human Vision and Electronic Imaging XVIII, Burlingame, CA,
   FEB 04-07, 2013}},
Organization = {{Soc Imaging Sci \& Technol (IS\&T); SPIE; Qualcomm Inc; Datacolor; Dolby
   Labs, Inc}},
Abstract = {{Visual attention is an inherent mechanism that plays an important role
   in the human visual perception. As our visual system has limited
   capacity and cannot efficiently process the information from the entire
   visual field, we focus our attention on specific areas of interest in
   the image for detailed analysis of these areas. In the context of media
   entertainment, the viewers' visual attention deployment is also
   influenced by the art of visual storytelling. To this date, visual
   editing and composition of scenes in stereoscopic 3D content creation
   still mostly follows those used in 2D. In particular, out-of-focus blur
   is often used in 2D motion pictures and photography to drive the
   viewer's attention towards a sharp area of the image. In this paper, we
   study specifically the impact of defocused foreground objects on visual
   attention deployment in stereoscopic 3D content. For that purpose, we
   conducted a subjective experiment using an eye-tracker. Our results
   bring more insights on the deployment of visual attention in
   stereoscopic 3D content viewing, and provide further understanding on
   visual attention behavior differences between 2D and 3D. Our results
   show that a traditional 2D scene compositing approach such as the use of
   foreground blur does not necessarily produce the same effect on visual
   attention deployment in 2D and 3D. Implications for stereoscopic content
   creation and visual fatigue are discussed.}},
Publisher = {{SPIE-INT SOC OPTICAL ENGINEERING}},
Address = {{1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Quan, HT (Reprint Author), Technicolor, 975 Ave Champs Blancs CS 17616, F-35576 Cesson Sevigne, France.
   Quan Huynh-Thu; Vienne, Cyril; Blonde, Laurent, Technicolor, F-35576 Cesson Sevigne, France.}},
DOI = {{10.1117/12.2006844}},
Article-Number = {{865112}},
ISSN = {{0277-786X}},
EISSN = {{1996-756X}},
ISBN = {{978-0-8194-9424-5}},
Keywords = {{visual attention; stereoscopic 3D; blur}},
Research-Areas = {{Optics; Imaging Science \& Photographic Technology}},
Web-of-Science-Categories  = {{Optics; Imaging Science \& Photographic Technology}},
ResearcherID-Numbers = {{Blonde, Laurent/G-6233-2010}},
ORCID-Numbers = {{Blonde, Laurent/0000-0003-3112-7701}},
Number-of-Cited-References = {{15}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{7}},
Doc-Delivery-Number = {{BFX86}},
Unique-ID = {{ISI:000321825100033}},
DA = {{2019-10-28}},
}

@article{ ISI:000312884400025,
Author = {Konijnenberg, Carolien and Melinder, Annika},
Title = {{Neurodevelopmental investigation of the mirror neurone system in
   children of women receiving opioid maintenance therapy during pregnancy}},
Journal = {{ADDICTION}},
Year = {{2013}},
Volume = {{108}},
Number = {{1}},
Pages = {{154-160}},
Month = {{JAN}},
Abstract = {{Aims Opioid maintenance therapy (OMT) is generally recommended for
   pregnant opioid-dependent women. Previous studies investigating the
   long-term effects of OMT on children's cognitive development found that
   children of women in OMT have an increased risk of developing deficits
   in motor and visual perceptual skills, which are important aspects of
   the mirror neurone system (MNS), a complex neural circuit involved in
   learning and social interactions. The aim of the current study was to
   investigate aspects of the MNS in children of women in OMT. Design A 2
   (control group versus OMT group)?x?2 (human versus mechanic) mixed
   factorial design. Setting The Cognitive Developmental Research Unit at
   the University of Oslo, Norway. Participants Fifteen children of women
   in OMT and 15 non-exposed children participated. Measurements
   Goal-directed eye movements were recorded using a Tobii 1750 eye
   tracker. Neurocognitive tests were employed to map children's cognitive
   development. Findings The OMT group made fewer proactive goal-directed
   eye movements {[}mean?=?-37.73, standard deviation (SD)?=?208.56]
   compared to the control group (mean?=?181.47, SD?=?228.65),
   F(1,28)?=?7.53, P?=?0.01, ?2?=?0.21. No differences were found on tests
   of visual perception or goal understanding. Conclusions Use of opioid
   maintenance therapy during pregnancy appears to be associated with
   impaired goal-directed eye movements in the 4-year-old infant which may
   affect later social adjustment adversely.}},
Publisher = {{WILEY}},
Address = {{111 RIVER ST, HOBOKEN 07030-5774, NJ USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Melinder, A (Reprint Author), Univ Oslo, Dept Psychol, Cognit Dev Res Unit, N-0370 Oslo, Norway.
   Konijnenberg, Carolien; Melinder, Annika, Univ Oslo, Dept Psychol, Cognit Dev Res Unit, N-0370 Oslo, Norway.
   Konijnenberg, Carolien, Univ Oslo, Norwegian Ctr Addict Res, N-0370 Oslo, Norway.}},
DOI = {{10.1111/j.1360-0443.2012.04006.x}},
ISSN = {{0965-2140}},
EISSN = {{1360-0443}},
Keywords = {{Buprenorphine; cognitive development; methadone; mirror neurone system;
   prenatal exposure; opioid maintenance therapy}},
Keywords-Plus = {{NEONATAL ABSTINENCE SYNDROME; COGNITIVE-DEVELOPMENT; BIRTH-WEIGHT;
   WEANLING RAT; METHADONE; BUPRENORPHINE; EXPOSURE; INFANTS; OUTCOMES;
   MOTHERS}},
Research-Areas = {{Substance Abuse; Psychiatry}},
Web-of-Science-Categories  = {{Substance Abuse; Psychiatry}},
Author-Email = {{a.m.d.melinder@psykologi.uio.no}},
ResearcherID-Numbers = {{Melinder, Annika/L-6782-2016
   }},
ORCID-Numbers = {{Melinder, Annika/0000-0001-7773-0579
   Konijnenberg, Carolien/0000-0001-5732-5488}},
Funding-Acknowledgement = {{Norwegian Directorate for Children, Youth and Family Affairs
   {[}06/34707]; Norwegian Research CouncilResearch Council of Norway}},
Funding-Text = {{The study was supported by grants from the Norwegian Directorate for
   Children, Youth and Family Affairs (06/34707) for the authors, and by a
   grant from the Norwegian Research Council to the first author.}},
Number-of-Cited-References = {{34}},
Times-Cited = {{18}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{23}},
Journal-ISO = {{Addiction}},
Doc-Delivery-Number = {{061XU}},
Unique-ID = {{ISI:000312884400025}},
DA = {{2019-10-28}},
}

@article{ ISI:000307796300010,
Author = {Khushaba, Rami N. and Greenacre, Luke and Kodagoda, Sarath and Louviere,
   Jordan and Burke, Sandra and Dissanayake, Gamini},
Title = {{Choice modeling and the brain: A study on the Electroencephalogram (EEG)
   of preferences}},
Journal = {{EXPERT SYSTEMS WITH APPLICATIONS}},
Year = {{2012}},
Volume = {{39}},
Number = {{16}},
Pages = {{12378-12388}},
Month = {{NOV 15}},
Abstract = {{Choice conjures the idea of a directed selection of a desirable action
   or object, motivated by internal likes and dislikes, or other such
   preferences. However, such internal processes are simply the domain of
   our human physiology. Understanding the physiological processes of
   decision making across a variety of contexts is a central aim in
   decision science as it has a great potential to further progress
   decision research. As a pilot study in this field, this paper explores
   the nature of decision making by examining the associated brain
   activity, Electroencephalogram (EEG), of people to understand how the
   brain responds while undertaking choices designed to elicit the
   subjects' preferences. To facilitate such a study, the Tobii-Studio eye
   tracker system was utilized to capture the participants' choice based
   preferences when they were observing seventy-two sets of objects. These
   choice sets were composed of three images offering potential personal
   computer backgrounds. Choice based preferences were identified by having
   the respondent click on their preferred one. In addition, a brain
   computer interface (BCI) represented by the commercial Emotiv EPOC
   wireless EEG headset with 14 channels was utilized to capture the
   associated brain activity during the period of the experiments.
   Principal Component Analysis (PCA) was utilized to preprocess the EEG
   data before analyzing it with the Fast Fourier Transform (FFT) to
   observe the changes in the main principal frequency bands, delta (0.5-4
   Hz), theta (4-7 Hz), alpha (8-12 Hz), beta (13-30 Hz), and gamma (30-40
   Hz). A mutual information (MI) measure was then used to study
   left-to-right hemisphere differences as well as front-to-back
   difference. Eighteen participants were recruited to perform the
   experiments with the average results showing clear and significant
   change in the spectral activity in the frontal (F3 and F4), parietal (P7
   and P8) and occipital (O1 and O2) areas while the participants were
   indicating their preferences. The results show that, when considering
   the amount of information exchange between the left and right
   hemispheres, theta bands exhibited minimal redundancy and maximum
   relevance to the task at hand when extracted from symmetric frontal,
   parietal, and occipital regions while alpha dominated in the frontal and
   parietal regions and beta dominating mainly in the occipital and
   temporal regions. (C) 2012 Elsevier Ltd. All rights reserved.}},
Publisher = {{PERGAMON-ELSEVIER SCIENCE LTD}},
Address = {{THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Khushaba, RN (Reprint Author), Univ Technol, CIMS, Fac Engn \& Informat Technol, Sydney, Uts, Australia.
   Khushaba, Rami N.; Kodagoda, Sarath; Dissanayake, Gamini, Univ Technol, CIMS, Fac Engn \& Informat Technol, Sydney, Uts, Australia.
   Greenacre, Luke; Louviere, Jordan; Burke, Sandra, Univ Technol, Ctr Study Choice CenSoc, Sydney, Uts, Australia.}},
DOI = {{10.1016/j.eswa.2012.04.084}},
ISSN = {{0957-4174}},
EISSN = {{1873-6793}},
Keywords = {{Decision making; Electroencephalogram (EEG); User preferences}},
Keywords-Plus = {{OSCILLATIONS}},
Research-Areas = {{Computer Science; Engineering; Operations Research \& Management Science}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Engineering, Electrical \&
   Electronic; Operations Research \& Management Science}},
Author-Email = {{Rami.khushaba@uts.edu.au
   Luke.Greenacre@uts.edu.au
   sarath.kodagoda@uts.edu.au
   Jordan.Louviere@uts.edu.au
   Sandra.Burke@uts.edu.au
   Gamini.dissanayake@uts.edu.au}},
ResearcherID-Numbers = {{Greenacre, Luke/E-7156-2015
   Khushaba, Rami/O-1038-2015
   Dissanayake, Gamini/F-7361-2017
   }},
ORCID-Numbers = {{Greenacre, Luke/0000-0002-6029-6523
   Khushaba, Rami/0000-0001-8528-8979
   Dissanayake, Gamini/0000-0002-7992-0680
   Kodagoda, Sarath/0000-0001-5175-9138
   Louviere, Jordan/0000-0001-7688-164X}},
Number-of-Cited-References = {{34}},
Times-Cited = {{34}},
Usage-Count-Last-180-days = {{6}},
Usage-Count-Since-2013 = {{72}},
Journal-ISO = {{Expert Syst. Appl.}},
Doc-Delivery-Number = {{992RB}},
Unique-ID = {{ISI:000307796300010}},
DA = {{2019-10-28}},
}

@article{ ISI:000309753200006,
Author = {Rocha, Tania and Bessa, Maximino and Goncalves, Martinho and Cabral,
   Luciana and Godinho, Francisco and Peres, Emanuel and Reis, Manuel C.
   and Magalhaes, Luis and Chalmers, Alan},
Title = {{The Recognition of Web Pages' Hyperlinks by People with Intellectual
   Disabilities: An Evaluation Study}},
Journal = {{JOURNAL OF APPLIED RESEARCH IN INTELLECTUAL DISABILITIES}},
Year = {{2012}},
Volume = {{25}},
Number = {{6}},
Pages = {{542-552}},
Month = {{NOV}},
Abstract = {{Background One of the most mentioned problems of web accessibility, as
   recognized in several different studies, is related to the difficulty
   regarding the perception of what is or is not clickable in a web page.
   In particular, a key problem is the recognition of hyperlinks by a
   specific group of people, namely those with intellectual disabilities.
   Materials and Methods This experiment investigated a methodology based
   on the direct observation, video recording, interview and data obtained
   by an eye tracker device. Ten participants took part in this study. They
   were divided into two groups and asked to perform two tasks: Sing a song
   and Listen to a story in two websites. These websites were developed to
   include specific details. The first website presented an image
   navigation menu (INM), whereas the other one showed a text navigation
   menu (TNM). Results There was a general improvement regarding the
   participants' performance when using INMs. Conclusion The referred
   analysis indeed shows that not only did these specific participants gain
   a better understanding of the demanding task, but also they showed an
   improved perception concerning the content of the navigation menu that
   included hyperlinks with images.}},
Publisher = {{WILEY-BLACKWELL}},
Address = {{111 RIVER ST, HOBOKEN 07030-5774, NJ USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Rocha, T (Reprint Author), Univ Tras Os Montes \& Alto Douro, Escola Ciencias \& Tecnol, Dept Engn, Apartado 1013, P-5001801 Vila Real, Portugal.
   Rocha, Tania; Bessa, Maximino; Goncalves, Martinho; Cabral, Luciana; Godinho, Francisco; Peres, Emanuel; Reis, Manuel C.; Magalhaes, Luis, Univ Tras Os Montes \& Alto Douro, Escola Ciencias \& Tecnol, Dept Engn, P-5001801 Vila Real, Portugal.
   Bessa, Maximino; Goncalves, Martinho; Peres, Emanuel; Magalhaes, Luis, INESC TEC Oporto, Oporto, Portugal.
   Reis, Manuel C., IEETA, Aveiro, Portugal.
   Chalmers, Alan, Univ Warwick, Warwick, England.}},
DOI = {{10.1111/j.1468-3148.2012.00700.x}},
ISSN = {{1360-2322}},
EISSN = {{1468-3148}},
Keywords = {{accessibility; intellectual disabilities; internet; usability}},
Research-Areas = {{Psychology; Rehabilitation}},
Web-of-Science-Categories  = {{Psychology, Educational; Rehabilitation}},
Author-Email = {{trocha@utad.pt}},
ResearcherID-Numbers = {{Peres, Emanuel/E-8392-2011
   Magalhaes, Luis/B-4300-2012
   Reis, Manuel J.C.S./G-2410-2012
   Bessa, Maximino/B-4729-2012
   }},
ORCID-Numbers = {{Peres, Emanuel/0000-0001-5669-7976
   Magalhaes, Luis/0000-0002-4426-0002
   Reis, Manuel J.C.S./0000-0002-8872-5721
   Bessa, Maximino/0000-0002-3002-704X
   Bessa, Luciana/0000-0001-9602-9729
   Rocha, Tania/0000-0002-2605-9284}},
Number-of-Cited-References = {{24}},
Times-Cited = {{15}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{28}},
Journal-ISO = {{J. Appl. Res. Intellect. Disabil.}},
Doc-Delivery-Number = {{019QA}},
Unique-ID = {{ISI:000309753200006}},
DA = {{2019-10-28}},
}

@article{ ISI:000309582800001,
Author = {Ploechl, Michael and Ossandon, Jose P. and Koenig, Peter},
Title = {{Combining EEG and eye tracking: identification, characterization, and
   correction of eye movement artifacts in electroencephalographic data}},
Journal = {{FRONTIERS IN HUMAN NEUROSCIENCE}},
Year = {{2012}},
Volume = {{6}},
Month = {{OCT 9}},
Abstract = {{Eye movements introduce large artifacts to electroencephalographic
   recordings (EEG) and thus render data analysis difficult or even
   impossible. Trials contaminated by eye movement and blink artifacts have
   to be discarded, hence in standard EEG-paradigms subjects are required
   to fixate on the screen. To overcome this restriction, several
   correction methods including regression and blind source separation have
   been proposed. Yet, there is no automated standard procedure
   established. By simultaneously recording eye movements and
   64-channel-EEG during a guided eye movement paradigm, we investigate and
   review the properties of eye movement artifacts, including
   corneo-retinal dipole changes, saccadic spike potentials and eyelid
   artifacts, and study their interrelations during different types of eye
   and eyelid movements. In concordance with earlier studies our results
   confirm that these artifacts arise from different independent sources
   and that depending on electrode site, gaze direction, and choice of
   reference these sources contribute differently to the measured signal.
   We assess the respective implications for artifact correction methods
   and therefore compare the performance of two prominent approaches,
   namely linear regression and independent component analysis (ICA). We
   show and discuss that due to the independence of eye artifact sources,
   regression-based correction methods inevitably over- or under-correct
   individual artifact components, while ICA is in principle suited to
   address such mixtures of different types of artifacts. Finally, we
   propose an algorithm, which uses eye tracker information to objectively
   identify eye-artifact related ICA-components (ICs) in an automated
   manner. In the data presented here, the algorithm performed very similar
   to human experts when those were given both, the topographies of the ICs
   and their respective activations in a large amount of trials. Moreover
   it performed more reliable and almost twice as effective than human
   experts when those had to base their decision on IC topographies only.
   Furthermore, a receiver operating characteristic (ROC) analysis
   demonstrated an optimal balance of false positive and false negative at
   an area under curve (AUC) of more than 0.99. Removing the automatically
   detected ICs from the data resulted in removal or substantial
   suppression of ocular artifacts including microsaccadic spike
   potentials, while the relevant neural signal remained unaffected. In
   conclusion the present work aims at a better understanding of individual
   eye movement artifacts, their interrelations and the respective
   implications for eye artifact correction. Additionally, the proposed
   ICA-procedure provides a tool for optimized detection and correction of
   eye movement related artifact components.}},
Publisher = {{FRONTIERS MEDIA SA}},
Address = {{AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Plochl, M (Reprint Author), Univ Osnabruck, Inst Cognit Sci, Albrechtstr 28, D-49069 Osnabruck, Germany.
   Ploechl, Michael; Ossandon, Jose P.; Koenig, Peter, Univ Osnabruck, Inst Cognit Sci, D-49069 Osnabruck, Germany.
   Koenig, Peter, Univ Med Ctr Hamburg Eppendorf, Dept Neurophysiol \& Pathophysiol, Hamburg, Germany.}},
DOI = {{10.3389/fnhum.2012.00278}},
Article-Number = {{278}},
ISSN = {{1662-5161}},
Keywords = {{eyetracking; EEG; independent component analysis (ICA); regression;
   artifact correction; eye movements}},
Keywords-Plus = {{INDEPENDENT COMPONENT ANALYSIS; EOG CORRECTION; AUTOMATIC REMOVAL;
   OCULAR ARTIFACTS; BLINK ARTIFACTS; MICROSACCADES; POTENTIALS; BRAIN;
   FMRI; VOLUNTARY}},
Research-Areas = {{Neurosciences \& Neurology; Psychology}},
Web-of-Science-Categories  = {{Neurosciences; Psychology}},
Author-Email = {{mploechl@uni-osnabrueck.de}},
ORCID-Numbers = {{Ossandon, Jose Pablo/0000-0002-2539-390X
   Konig, Peter/0000-0003-3654-5267}},
Funding-Acknowledgement = {{Cognition and Neuroergonomics/Collaborative Technology Alliance
   {[}911NF-10-2-0022]; Extending Sensorimotor Contingencies to Cognition
   (eSMCs) {[}FP7-ICT-270212]}},
Funding-Text = {{Michael Plochl's and Peter Konig's work was supported by the Cognition
   and Neuroergonomics/Collaborative Technology Alliance grant
   \#911NF-10-2-0022. Jose P. Ossandon's work was supported by grant
   FP7-ICT-270212 Extending Sensorimotor Contingencies to Cognition
   (eSMCs). We thank our tagging experts Danja Porada and Rene Scheeringa,
   as well as Tim Kietzmann for his very helpful suggestions.}},
Number-of-Cited-References = {{91}},
Times-Cited = {{106}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{78}},
Journal-ISO = {{Front. Hum. Neurosci.}},
Doc-Delivery-Number = {{017IG}},
Unique-ID = {{ISI:000309582800001}},
OA = {{DOAJ Gold, Green Published}},
DA = {{2019-10-28}},
}

@article{ ISI:000306310200012,
Author = {Chawarska, Katarzyna and Macari, Suzanne and Shic, Frederick},
Title = {{Context modulates attention to social scenes in toddlers with autism}},
Journal = {{JOURNAL OF CHILD PSYCHOLOGY AND PSYCHIATRY}},
Year = {{2012}},
Volume = {{53}},
Number = {{8}},
Pages = {{903-913}},
Month = {{AUG}},
Abstract = {{Background: In typical development, the unfolding of social and
   communicative skills hinges upon the ability to allocate and sustain
   attention toward people, a skill present moments after birth. Deficits
   in social attention have been well documented in autism, though the
   underlying mechanisms are poorly understood. Methods: In order to parse
   the factors that are responsible for limited social attention in
   toddlers with autism, we manipulated the context in which a person
   appeared in their visual field with regard to the presence of salient
   social (child-directed speech and eye contact) and nonsocial (distractor
   toys) cues for attention. Participants included 13- to 25-month-old
   toddlers with autism (autism; n = 54), developmental delay (DD; n = 22),
   and typical development (TD; n = 48). Their visual responses were
   recorded with an eye-tracker. Results: In conditions devoid of eye
   contact and speech, the distribution of attention between key features
   of the social scene in toddlers with autism was comparable to that in DD
   and TD controls. However, when explicit dyadic cues were introduced,
   toddlers with autism showed decreased attention to the entire scene and,
   when they looked at the scene, they spent less time looking at the
   speakers face and monitoring her lip movements than the control groups.
   In toddlers with autism, decreased time spent exploring the entire scene
   was associated with increased symptom severity and lower nonverbal
   functioning; atypical language profiles were associated with decreased
   monitoring of the speakers face and her mouth. Conclusions: While in
   certain contexts toddlers with autism attend to people and objects in a
   typical manner, they show decreased attentional response to dyadic cues
   for attention. Given that mechanisms supporting responsivity to dyadic
   cues are present shortly after birth and are highly consequential for
   development of social cognition and communication, these findings have
   important implications for the understanding of the underlying
   mechanisms of limited social monitoring and identifying pivotal targets
   for treatment.}},
Publisher = {{WILEY}},
Address = {{111 RIVER ST, HOBOKEN 07030-5774, NJ USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Chawarska, K (Reprint Author), Yale Child Study Ctr, 40 Temple St,Suite 7D, New Haven, CT 06510 USA.
   Chawarska, Katarzyna; Macari, Suzanne; Shic, Frederick, Yale Univ, Sch Med, Ctr Child Study, New Haven, CT 06510 USA.}},
DOI = {{10.1111/j.1469-7610.2012.02538.x}},
ISSN = {{0021-9630}},
EISSN = {{1469-7610}},
Keywords = {{Autism; infants; toddlers; eye-tracking; visual attention; dyadic
   attention; scanning; naturalistic; dynamic stimuli}},
Keywords-Plus = {{SPECTRUM DISORDERS; LANGUAGE-DEVELOPMENT; 2-YEAR-OLD TODDLERS; FACIAL
   EXPRESSIONS; PRESCHOOL-CHILDREN; BIOLOGICAL MOTION; FACE RECOGNITION;
   EYE-MOVEMENT; INFANTS; PERCEPTION}},
Research-Areas = {{Psychology; Psychiatry}},
Web-of-Science-Categories  = {{Psychology, Developmental; Psychiatry; Psychology}},
Author-Email = {{katarzyna.chawarska@yale.edu}},
ORCID-Numbers = {{Shic, Frederick/0000-0002-9040-1259}},
Funding-Acknowledgement = {{National Alliance for Autism Research Foundation; Autism Speaks
   Foundation; NIMH ACE {[}P50 MH081756-01]; NSF CDI-Type I grant
   {[}0835767]; NIMHUnited States Department of Health \& Human
   ServicesNational Institutes of Health (NIH) - USANIH National Institute
   of Mental Health (NIMH) {[}1R03MH086732, R03 MH092618-01A1]}},
Funding-Text = {{The study was supported by the National Alliance for Autism Research
   Foundation, Autism Speaks Foundation, NIMH ACE grant \#P50 MH081756-01
   Project 2 (PI: K. Chawarska), NSF CDI-Type I grant \#0835767 (PIs: K.
   Chawarska and B. Scassellati), NIMH grants \#1R03MH086732 (PI: S.
   Macari), and R03 MH092618-01A1 (PI: F. Shic). The content is solely the
   responsibility of the authors and does not necessarily represent the
   official views of the National Science Foundation, National Institute of
   Mental Health, National Institute of Child Health and Development, or
   the National Institutes of Health. We would like to thank Dr. Daniel
   Campbell for his advice on statistical analysis. We would also like to
   thank Drs. Celine Saulnier, Amanda Steiner, Rhea Paul, and Elizabeth
   Simmons for their contribution to sample characterization. We wish to
   express our deepest appreciation to the families and their children for
   their time and participation.}},
Number-of-Cited-References = {{70}},
Times-Cited = {{101}},
Usage-Count-Last-180-days = {{3}},
Usage-Count-Since-2013 = {{77}},
Journal-ISO = {{J. Child Psychol. Psychiatry}},
Doc-Delivery-Number = {{972VW}},
Unique-ID = {{ISI:000306310200012}},
OA = {{Green Accepted}},
DA = {{2019-10-28}},
}

@article{ ISI:000307913700002,
Author = {Rodriguez, Christina M. and Cook, Anne E. and Jedrziewski, Chezlie T.},
Title = {{Reading between the lines: Implicit assessment of the association of
   parental attributions and empathy with abuse risk}},
Journal = {{CHILD ABUSE \& NEGLECT}},
Year = {{2012}},
Volume = {{36}},
Number = {{7-8}},
Pages = {{564-571}},
Month = {{JUL-AUG}},
Abstract = {{Objective: Researchers in the child maltreatment field have
   traditionally relied on explicit self-reports to study factors that may
   exacerbate physical child abuse risk. The current investigation
   evaluated an implicit analog task utilizing eye tracking technology to
   assess both parental attributions of child misbehavior and empathy.
   Method: Based on the observation that readers experience comprehension
   difficulty when encountering passages inconsistent with their beliefs,
   an eye tracker gauged the extent of difficulty parents experienced
   reading vignettes that inappropriately characterized a child as culpable
   for misbehavior and that presented a non-empathic child interaction.
   Results: Results suggest self-reports of attributions and empathy are
   related to both child abuse potential and discipline intentions;
   however, the eye tracking analog for empathy correlated with abuse
   potential but not punishment decisions whereas the analog for
   attributions correlated with punishment decisions but not abuse
   potential.
   Conclusions: Such contrasts between self-report and analog assessment
   underscore the need for continued research studying theorized abuse risk
   constructs using alternative approaches to better identify the important
   risk markers associated with elevated child abuse risk and to minimize
   methodological overlap. (C) 2012 Elsevier Ltd. All rights reserved.}},
Publisher = {{PERGAMON-ELSEVIER SCIENCE LTD}},
Address = {{THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Rodriguez, CM (Reprint Author), Univ N Carolina, Dept Psychol, POB 26170, Greensboro, NC 27402 USA.
   Rodriguez, Christina M., Univ N Carolina, Dept Psychol, Greensboro, NC 27402 USA.
   Cook, Anne E.; Jedrziewski, Chezlie T., Univ Utah, Salt Lake City, UT USA.}},
DOI = {{10.1016/j.chiabu.2012.05.004}},
ISSN = {{0145-2134}},
Keywords = {{Child maltreatment; Physical child abuse potential; Attributions of
   child misbehavior; Empathy; Analog tasks}},
Keywords-Plus = {{CHILD PHYSICAL ABUSE; EYE-MOVEMENTS; MALTREATMENT; INFORMATION; STRESS;
   COMPREHENSION; INVENTORY; MOTHERS; IMPACT}},
Research-Areas = {{Family Studies; Psychology; Social Work}},
Web-of-Science-Categories  = {{Family Studies; Psychology, Social; Social Work}},
ORCID-Numbers = {{Rodriguez, Christina/0000-0002-5090-0707}},
Number-of-Cited-References = {{34}},
Times-Cited = {{13}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{19}},
Journal-ISO = {{Child Abuse Negl.}},
Doc-Delivery-Number = {{994EM}},
Unique-ID = {{ISI:000307913700002}},
DA = {{2019-10-28}},
}

@article{ ISI:000300070200018,
Author = {Teglas, Erno and Gergely, Anna and Kupan, Krisztina and Miklosi, Adam
   and Topal, Jozsef},
Title = {{Dogs' Gaze Following Is Tuned to Human Communicative Signals}},
Journal = {{CURRENT BIOLOGY}},
Year = {{2012}},
Volume = {{22}},
Number = {{3}},
Pages = {{209-212}},
Month = {{FEB 7}},
Abstract = {{Recent evidence suggests that preverbal infants' gaze following can be
   triggered only if an actor's head turn is preceded by the expression of
   communicative intent {[}1]. Such connectedness between ostensive and
   referential signals may be uniquely human, enabling infants to
   effectively respond to referential communication directed to them. In
   the light of increasing evidence of dogs' social communicative skills
   {[}2], an intriguing question is whether dogs' responsiveness to human
   directional gestures {[}3] is associated with the situational context in
   an infant-like manner. Borrowing a method used in infant studies {[}1],
   dogs watched video presentations of a human actor turning toward one of
   two objects, and their eye-gaze patterns were recorded with an eye
   tracker. Results show a higher tendency of gaze following in dogs when
   the human's head turning was preceded by the expression of communicative
   intent (direct gaze, addressing). This is the first evidence to show
   that (1) eye-tracking techniques can be used for studying dogs' social
   skills and (2) the exploitation of human gaze cues depends on the
   communicatively relevant pattern of ostensive and referential signals in
   dogs. Our findings give further support to the existence of a
   functionally infant-analog social competence in this species.}},
Publisher = {{CELL PRESS}},
Address = {{600 TECHNOLOGY SQUARE, 5TH FLOOR, CAMBRIDGE, MA 02139 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Topal, J (Reprint Author), Hungarian Acad Sci, Res Inst Psychol, Comparat Behav Res Grp, Victor Hugo St 18-22, H-1132 Budapest, Hungary.
   Teglas, Erno; Kupan, Krisztina; Topal, Jozsef, Hungarian Acad Sci, Res Inst Psychol, Comparat Behav Res Grp, H-1132 Budapest, Hungary.
   Teglas, Erno, Cent European Univ, Cognit Dev Ctr, H-1051 Budapest, Hungary.
   Gergely, Anna; Miklosi, Adam, Eotvos Lorand Univ, Dept Ethol, H-1117 Budapest, Hungary.}},
DOI = {{10.1016/j.cub.2011.12.018}},
ISSN = {{0960-9822}},
Keywords-Plus = {{CANIS-FAMILIARIS; HUMAN INFANTS; COMPREHENSION}},
Research-Areas = {{Biochemistry \& Molecular Biology; Cell Biology}},
Web-of-Science-Categories  = {{Biochemistry \& Molecular Biology; Cell Biology}},
Author-Email = {{topaljozsef@gmail.com}},
ResearcherID-Numbers = {{Miklosi, Adam/N-5147-2017}},
ORCID-Numbers = {{Miklosi, Adam/0000-0003-4831-8985}},
Funding-Acknowledgement = {{Hungarian Science Foundation (OTKA)Orszagos Tudomanyos Kutatasi
   Alapprogramok (OTKA) {[}K76043, NK83997]; European Union (EU)European
   Union (EU) {[}FP7 ICT-215554 LIREC]; Marie Curie Research Training
   NetworkEuropean Union (EU)}},
Funding-Text = {{This work was funded by the Hungarian Science Foundation (OTKA K76043 \&
   NK83997), the European Union (EU FP7 ICT-215554 LIREC), and the Marie
   Curie Research Training Network.}},
Number-of-Cited-References = {{12}},
Times-Cited = {{100}},
Usage-Count-Last-180-days = {{2}},
Usage-Count-Since-2013 = {{43}},
Journal-ISO = {{Curr. Biol.}},
Doc-Delivery-Number = {{889JO}},
Unique-ID = {{ISI:000300070200018}},
OA = {{Bronze}},
DA = {{2019-10-28}},
}

@article{ ISI:000298521200004,
Author = {Bednarik, Roman},
Title = {{Expertise-dependent visual attention strategies develop over time during
   debugging with multiple code representations}},
Journal = {{INTERNATIONAL JOURNAL OF HUMAN-COMPUTER STUDIES}},
Year = {{2012}},
Volume = {{70}},
Number = {{2}},
Pages = {{143-155}},
Month = {{FEB}},
Abstract = {{In modern multi-representational environments, software developers need
   to coordinate various information sources to effectively perform
   maintenance tasks. Although visual attention is an important skill in
   software development, our current understanding of the role of visual
   attention in the coordination of representations during maintenance
   tasks is minimal. Therefore, we applied eye-tracking to capture visual
   attention strategies and construct a detailed account of visual
   attention during debugging. Two groups of programmers with two distinct
   levels of experience debugged a program with the help of multiple
   representations. The proportion of time spent looking at each
   representation, the frequency of switching attention between
   visualrepresentations and the type of switch were investigated during
   consecutive phases of debugging. We found repetitive patterns in visual
   strategies that were associated with less expertise and lower
   performance. Novice developers made use of both the code and graphical
   representations while frequently switching between them. More
   experienced participants expended more efforts integrating the available
   information and primarily concentrated on systematically relating the
   code to the output. Our results informed us about the differences in
   program debugging strategies from a fine-grain, temporal perspective and
   have implications for the design of future development environments. (C)
   2011 Elsevier Ltd. All rights reserved.}},
Publisher = {{ACADEMIC PRESS LTD- ELSEVIER SCIENCE LTD}},
Address = {{24-28 OVAL RD, LONDON NW1 7DX, ENGLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Bednarik, R (Reprint Author), Univ Eastern Finland, Sch Comp, FI-80101 Joensuu, Finland.
   Univ Eastern Finland, Sch Comp, FI-80101 Joensuu, Finland.}},
DOI = {{10.1016/j.ijhcs.2011.09.003}},
ISSN = {{1071-5819}},
Keywords = {{Expertise in programming; Visual attention; Program debugging}},
Keywords-Plus = {{RESTRICTED FOCUS VIEWER; PROGRAM COMPREHENSION; COMPUTER-PROGRAMS;
   SOFTWARE; TRACKING; MAINTENANCE}},
Research-Areas = {{Computer Science; Engineering; Psychology}},
Web-of-Science-Categories  = {{Computer Science, Cybernetics; Ergonomics; Psychology, Multidisciplinary}},
Author-Email = {{roman.bednarik@uef.fi}},
ORCID-Numbers = {{Bednarik, Roman/0000-0003-1726-3520}},
Number-of-Cited-References = {{39}},
Times-Cited = {{10}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{8}},
Journal-ISO = {{Int. J. Hum.-Comput. Stud.}},
Doc-Delivery-Number = {{868KC}},
Unique-ID = {{ISI:000298521200004}},
DA = {{2019-10-28}},
}

@article{ ISI:000312597900001,
Author = {Munk, Carmen and Rey, Guenter Daniel and Diergarten, Anna Katharina and
   Nieding, Gerhild and Schneider, Wolfgang and Ohler, Peter},
Title = {{Cognitive Processing of Film Cuts Among 4-to 8-Year-Old Children An Eye
   Tracker Experiment}},
Journal = {{EUROPEAN PSYCHOLOGIST}},
Year = {{2012}},
Volume = {{17}},
Number = {{4}},
Pages = {{257-265}},
Abstract = {{An eye tracker experiment investigated 4-, 6-, and 8-year old children's
   cognitive processing of film cuts. Nine short film sequences with or
   without editing errors were presented to 79 children. Eye movements up
   to 400 ms after the targeted film cuts were measured and analyzed using
   a new calculation formula based on Manhattan Metrics. No age effects
   were found for jump cuts (i.e., small movement discontinuities in a
   film). However, disturbances resulting from reversed-angle shots (i.e.,
   a switch of the left-right position of actors in successive shots) led
   to increased reaction times between 6- and 8-year old children, whereas
   children of all age groups had difficulties coping with narrative
   discontinuity (i.e., the canonical chronological sequence of film
   actions is disrupted). Furthermore, 4- year old children showed a
   greater number of overall eye movements than 6- and 8-year old children.
   This indicates that some viewing skills are developed between 4 and 6
   years of age. The results of the study provide evidence of a crucial
   time span of knowledge acquisition for television-based media literacy
   between 4 and 8 years.}},
Publisher = {{HOGREFE \& HUBER PUBLISHERS}},
Address = {{PO BOX 2487, KIRKLAND, WA 98083-2487 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Munk, C (Reprint Author), Univ Wurzburg, Inst Psychol, Dept Psychol 4, Rontgenring 10, D-97070 Wurzburg, Germany.
   Munk, Carmen, Univ Wurzburg, Inst Psychol, Dept Psychol 4, D-97070 Wurzburg, Germany.
   Ohler, Peter, Univ Technol, Chemnitz, Germany.}},
DOI = {{10.1027/1016-9040/a000098}},
ISSN = {{1016-9040}},
EISSN = {{1878-531X}},
Keywords = {{media literacy; eye movements; film cuts; editing rules; editing errors}},
Keywords-Plus = {{COMPREHENSION}},
Research-Areas = {{Psychology}},
Web-of-Science-Categories  = {{Psychology, Multidisciplinary}},
Author-Email = {{carmen.munk@psychologie.uni-wuerzburg.de}},
Number-of-Cited-References = {{21}},
Times-Cited = {{11}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{20}},
Journal-ISO = {{Eur. Psychol.}},
Doc-Delivery-Number = {{057XO}},
Unique-ID = {{ISI:000312597900001}},
DA = {{2019-10-28}},
}

@article{ ISI:000302410300005,
Author = {Vilaro, Anna and Duchowski, Andrew T. and Orero, Pilar and Grindinger,
   Tom and Tetreault, Stephen and di Giovanni, Elena},
Title = {{How sound is the Pear Tree Story? Testing the effect of varying audio
   stimuli on visual attention distribution}},
Journal = {{PERSPECTIVES-STUDIES IN TRANSLATION THEORY AND PRACTICE}},
Year = {{2012}},
Volume = {{20}},
Number = {{1, SI}},
Pages = {{55-65}},
Abstract = {{Looking at the literature on the subject of audio description, there is
   a neglected area which is often mentioned but never studied: soundtrack.
   This article describes an empirical test with the aim of investigating
   one of the many thorny issues in audio description: how the soundtrack
   of audiovisual texts influences perception and comprehension of the
   scene.
   Viewers' perception of dynamic media can vary depending on the stimuli
   introduced, and likely also correlates with the viewer's cognitive
   intent. This piece presents the results of a test which overlaid various
   background sounds to a selected clip from the film The Pear Tree Story.
   Eye movements were recorded with an eye tracker while the clip was being
   screened. Further comprehension questions were then asked.
   The approach studied here is the classification of disparate viewing
   patterns through scanpath comparison, and the possible variance in
   visual attention that may occur due to the introduction of different
   auditory stimuli over the same dynamic media.}},
Publisher = {{ROUTLEDGE JOURNALS, TAYLOR \& FRANCIS LTD}},
Address = {{2-4 PARK SQUARE, MILTON PARK, ABINGDON OX14 4RN, OXON, ENGLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Orero, P (Reprint Author), Univ Autonoma Barcelona, CAIAC, E-08193 Barcelona, Spain.
   Vilaro, Anna; Orero, Pilar, Univ Autonoma Barcelona, CAIAC, E-08193 Barcelona, Spain.
   Duchowski, Andrew T.; Grindinger, Tom, Clemson Univ, Sch Comp, Clemson, SC USA.
   Tetreault, Stephen, Rhode Isl Coll, Dept Comp Sci \& Math, Providence, RI 02908 USA.
   di Giovanni, Elena, Univ Macerata, Dept Linguist Literary \& Philol Res, Macerata, Italy.
   Orero, Pilar, Univ Autonoma Barcelona, CAIAC Res Ctr, E-08193 Barcelona, Spain.
   di Giovanni, Elena, Univ Macerata, Language Ctr, Macerata, Italy.}},
DOI = {{10.1080/0907676X.2011.632682}},
ISSN = {{0907-676X}},
EISSN = {{1747-6623}},
Keywords = {{audio description; sound; media accessibility; perception; eye-tracking}},
Research-Areas = {{Linguistics}},
Web-of-Science-Categories  = {{Linguistics; Language \& Linguistics}},
Author-Email = {{Pilar.orero@uab.cat}},
ResearcherID-Numbers = {{Orero, Pilar/I-3703-2013}},
ORCID-Numbers = {{DI GIOVANNI, Elena/0000-0002-5823-0082
   Orero, Pilar/0000-0003-0269-1936}},
Number-of-Cited-References = {{14}},
Times-Cited = {{8}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{9}},
Journal-ISO = {{Perspect.-Stud. Transl.}},
Doc-Delivery-Number = {{920NA}},
Unique-ID = {{ISI:000302410300005}},
DA = {{2019-10-28}},
}

@article{ ISI:000298524300034,
Author = {Tsai, Meng-Jung and Hou, Huei-Tse and Lai, Meng-Lung and Liu, Wan-Yi and
   Yang, Fang-Ying},
Title = {{Visual attention for solving multiple-choice science problem: An
   eye-tracking analysis}},
Journal = {{COMPUTERS \& EDUCATION}},
Year = {{2012}},
Volume = {{58}},
Number = {{1}},
Pages = {{375-385}},
Month = {{JAN}},
Abstract = {{This study employed an eye-tracking technique to examine students'
   visual attention when solving a multiple-choice science problem. Six
   university students participated in a problem-solving task to predict
   occurrences of landslide hazards from four images representing four
   combinations of four factors. Participants' responses and visual
   attention were recorded by an eye tracker. Participants were asked to
   think aloud during the entire task. A 4 (options) x 4 (factors) repeated
   measures design, two paired t-tests and effect sizes analyses were
   conducted to compare the fixation duration between chosen and rejected
   options and between relevant and irrelevant factors. Content analyses
   were performed to analyze participants' responses and think aloud
   protocols and to examine individual's Hot Zone image. Finally,
   sequential analysis on fixated LookZones was further utilized to compare
   the scan patterns between successful and unsuccessful problem solvers.
   The results showed that, while solving an image-based multiple-choice
   science problem, students, in general, paid more attention to chosen
   options than rejected alternatives, and spent more time inspecting
   relevant factors than irrelevant ones. Additionally, successful problem
   solvers focused more on relevant factors, while unsuccessful problem
   solvers experienced difficulties in decoding the problem, in recognizing
   the relevant factors, and in self-regulating of concentration. Future
   study can be done to examine the reliability and the usability of
   providing adaptive instructional scaffoldings for problem solving
   according to students' visual attention allocations and transformations
   in a larger scale. Eye-tracking techniques are suggested to be used to
   deeply explore the cognitive process during e-learning and be applied to
   future online assessment systems. (C) 2011 Elsevier Ltd. All rights
   reserved.}},
Publisher = {{PERGAMON-ELSEVIER SCIENCE LTD}},
Address = {{THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Tsai, MJ (Reprint Author), Natl Taiwan Univ Sci \& Technol, Grad Inst Digital Learning \& Educ, 93,Sec 4,Keelung Rd, Taipei 106, Taiwan.
   Tsai, Meng-Jung; Liu, Wan-Yi, Natl Taiwan Univ Sci \& Technol, Grad Inst Digital Learning \& Educ, Taipei 106, Taiwan.
   Hou, Huei-Tse, Natl Taiwan Univ Sci \& Technol, Grad Inst Appl Sci \& Technol, Taipei 106, Taiwan.
   Yang, Fang-Ying, Natl Taiwan Normal Univ, Grad Inst Sci Educ, Taipei, Taiwan.}},
DOI = {{10.1016/j.compedu.2011.07.012}},
ISSN = {{0360-1315}},
EISSN = {{1873-782X}},
Keywords = {{Teaching/learning strategies; Applications in subject areas; Media in
   education; Interdisciplinary projects; Evaluation methodology}},
Keywords-Plus = {{ARITHMETIC WORD-PROBLEMS; BEHAVIORAL-PATTERNS; COMPREHENSION; KNOWLEDGE;
   ONLINE; MOVEMENTS; STRATEGIES; FIXATIONS; TEACHERS; CHILDREN}},
Research-Areas = {{Computer Science; Education \& Educational Research}},
Web-of-Science-Categories  = {{Computer Science, Interdisciplinary Applications; Education \&
   Educational Research}},
Author-Email = {{mjtsai99@mail.ntust.edu.tw}},
ORCID-Numbers = {{Tsai, Meng-Jung/0000-0002-8994-861X}},
Funding-Acknowledgement = {{National Science Council of TaiwanNational Science Council of Taiwan
   {[}NSC 98-2511-S-011-004-MY2, 98-2511-S-003-047-MY2,
   99-2511-S-011-006-MY3, 100-2918-I-011-001]}},
Funding-Text = {{This study is supported by National Science Council of Taiwan under the
   following grants: NSC 98-2511-S-011-004-MY2, 98-2511-S-003-047-MY2,
   99-2511-S-011-006-MY3 and 100-2918-I-011-001. The authors appreciate Dr.
   Hung-Ta Pal's help in MATLAB programming, Dr. Bradley Love's comments
   and the reviewers' suggestions for revisions.}},
Number-of-Cited-References = {{43}},
Times-Cited = {{88}},
Usage-Count-Last-180-days = {{4}},
Usage-Count-Since-2013 = {{149}},
Journal-ISO = {{Comput. Educ.}},
Doc-Delivery-Number = {{868LH}},
Unique-ID = {{ISI:000298524300034}},
DA = {{2019-10-28}},
}

@article{ ISI:000295435700003,
Author = {Alamargot, Denis and Caporossi, Gilles and Chesnet, David and Ros,
   Christine},
Title = {{What makes a skilled writer? Working memory and audience awareness
   during text composition}},
Journal = {{LEARNING AND INDIVIDUAL DIFFERENCES}},
Year = {{2011}},
Volume = {{21}},
Number = {{5}},
Pages = {{505-516}},
Month = {{OCT}},
Abstract = {{This study investigated the role of working memory capacity as a factor
   for individual differences in the ability to compose a text with
   communicative efficiency based on audience awareness. We analyzed its
   differential effects on the dynamics of the writing processes, as well
   as on the content of the finished product. Twenty-five graduate students
   composed a procedural text explaining how to assemble a model turbine.
   They were free to consult a documentary source, featuring captioned
   pictures of turbine parts and assembly steps, at any time. Graphomotor
   and eye movements were recorded using `Eye and Pen' software with an
   eye-tracker and digitizing tablet. Results showed that high WM capacity
   writers used a different strategy to explore the visual source, making
   longer writing pauses and producing more detailed procedures, and
   achieved the communicative goal more efficiently, by introducing more
   reader supports. In conclusion, we discuss the feasibility of audience
   awareness training. (C) 2011 Elsevier Inc. All rights reserved.}},
Publisher = {{ELSEVIER SCIENCE BV}},
Address = {{PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Alamargot, D (Reprint Author), Univ Poitiers, Lab CeRCA CNRS 6234, MSHS, Batiment A5,5 Rue Theodore Lefebvre, F-86000 Poitiers, France.
   Alamargot, Denis, Univ Poitiers, Lab CeRCA CNRS 6234, MSHS, F-86000 Poitiers, France.
   Alamargot, Denis; Ros, Christine, Univ Poitiers, CeRCA CNRS Lab, F-86000 Poitiers, France.
   Caporossi, Gilles, GERAD HEC Montreal, Quebec City, PQ, Canada.}},
DOI = {{10.1016/j.lindif.2011.06.001}},
ISSN = {{1041-6080}},
Keywords = {{Written production; Procedural text; Audience awareness; Expertise;
   Individual differences; Online measures; Eye movements}},
Keywords-Plus = {{IMPROVING WRITTEN COMMUNICATION; INDIVIDUAL-DIFFERENCES; CAPACITY
   THEORY; KNOWLEDGE; SPAN; COMPREHENSION; STUDENTS; EYE; READERS; DEVICE}},
Research-Areas = {{Psychology}},
Web-of-Science-Categories  = {{Psychology, Educational}},
Author-Email = {{Denis.Alamargot@univ-poitiers.fr}},
ResearcherID-Numbers = {{Caporossi, Gilles/C-9987-2018
   }},
ORCID-Numbers = {{Chesnet, David/0000-0003-1322-839X}},
Number-of-Cited-References = {{67}},
Times-Cited = {{16}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{22}},
Journal-ISO = {{Learn. Individ. Differ.}},
Doc-Delivery-Number = {{827NR}},
Unique-ID = {{ISI:000295435700003}},
DA = {{2019-10-28}},
}

@article{ ISI:000309245900003,
Author = {Pibernik, Jesenka and Dolic, Jurica and Dilberovic, Ivan},
Title = {{T-shirt design by digital direct printing technique}},
Journal = {{TEKSTIL}},
Year = {{2011}},
Volume = {{60}},
Number = {{10}},
Pages = {{504-511}},
Month = {{OCT}},
Abstract = {{The digital design and print on textile potentials are available on-line
   to every user. The new technology possibilities allow users to combine
   words, fonts, illustrations and photographs through on-line web
   application and create new prints. One of the main characteristics of
   those web sites is the limitation they put on designers. The functional,
   tactile and kinetic evaluation of clothing is not possible. The research
   goal is the evaluation of the visual perception and semantic
   interpretation of student population in on-line T-shirt design process.
   The aim of the paper is to help designers and broader their
   understanding of the possibilities and limitations of digital direct
   printing technique on textile.}},
Publisher = {{ASSOC TEXTILE ENGINEERS TECHNICIANS CROATIA}},
Address = {{NOVAKOVA 8-11, POB 829, HR-41001 ZAGREB, CROATIA}},
Type = {{Article}},
Language = {{Croatian}},
Affiliation = {{Pibernik, J (Reprint Author), Sveucilista Zagrebu, Graficki Fak, Zagreb, Croatia.
   Pibernik, Jesenka; Dolic, Jurica; Dilberovic, Ivan, Sveucilista Zagrebu, Graficki Fak, Zagreb, Croatia.}},
ISSN = {{0492-5882}},
Keywords = {{design; meaning; t shirt; digital print; Eye Tracker}},
Keywords-Plus = {{APPROPRIATENESS}},
Research-Areas = {{Materials Science}},
Web-of-Science-Categories  = {{Materials Science, Textiles}},
Author-Email = {{jpiberni@grf.hr}},
Number-of-Cited-References = {{17}},
Times-Cited = {{1}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{3}},
Journal-ISO = {{Tekstil}},
Doc-Delivery-Number = {{012OH}},
Unique-ID = {{ISI:000309245900003}},
DA = {{2019-10-28}},
}

@article{ ISI:000292908700012,
Author = {Sodergren, Mikael H. and Orihuela-Espina, Felipe and Mountney, Peter and
   Clark, James and Teare, Julian and Darzi, Ara and Yang, Guang-Zhong},
Title = {{Orientation Strategies in Natural Orifice Translumenal Endoscopic
   Surgery}},
Journal = {{ANNALS OF SURGERY}},
Year = {{2011}},
Volume = {{254}},
Number = {{2}},
Pages = {{257-266}},
Month = {{AUG}},
Abstract = {{Objective: The aims of this study were to (1) describe the visual
   attention strategies employed by surgeons that are associated with high
   performance in reorientation and (2) identify key structures guiding
   attention deployment of the surgeon in the process of self-orientation
   in common clinical natural orifice translumenal endoscopic surgery
   (NOTES) scenarios.
   Background: Disorientation has been identified as one of the major
   barriers to be overcome before widespread clinical NOTES uptake.
   Understanding disorientation requires description of key
   perceptual-motor factors leading to disorientation, assessment of their
   relative impact, and quantification of navigation performance.
   Methods: Twenty-one surgeons were shown a series of 8 images acquired
   during human NOTES operations from the flexible endoscope from different
   perspectives to induce disorientation. Gaze behavior was recorded using
   an eye tracker as the subjects were asked to establish the image
   orientation. Main outcome measures were times taken to establish
   orientation, eye-tracking parameters, and fixation sequences on organs
   and structures/regions of interest (ROI).
   Results: High-performance subjects had a lower number of fixations and
   normalized dwell time per ROI compared with others, suggesting a more
   structured and focused approach to orientation. Orientation strategies
   associated with high performance were described using a validated
   algorithm for comparing visual reorientation behavior and amount of
   visual attention on individual ROIs in each scenario were quantified.
   Key areas of organs and structures during reorientation were illustrated
   using dwell time normalized visual maps.
   Conclusions: Targeted orientation strategies revealed in this study are
   expected to aid in decreasing the learning curve associated with NOTES
   and increase performance even for experienced surgeons and
   gastroenterologists. Crucially, these data can provide guidance for
   designing orientation friendly NOTES platforms.}},
Publisher = {{LIPPINCOTT WILLIAMS \& WILKINS}},
Address = {{TWO COMMERCE SQ, 2001 MARKET ST, PHILADELPHIA, PA 19103 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Sodergren, MH (Reprint Author), Univ London Imperial Coll Sci Technol \& Med, Acad Surg Unit, St Marys Hosp, Dept Biosurg \& Surg Technol, 10th Floor QEQM,S Wharf Rd, London W2 1NY, England.
   Sodergren, Mikael H.; Orihuela-Espina, Felipe; Clark, James, Univ London Imperial Coll Sci Technol \& Med, Dept Biosurg \& Surg Technol, London W2 1NY, England.
   Orihuela-Espina, Felipe; Mountney, Peter, Univ London Imperial Coll Sci Technol \& Med, Inst Biomed Engn, London W2 1NY, England.
   Teare, Julian, Univ London Imperial Coll Sci Technol \& Med, Dept Intervent Endoscopy, London W2 1NY, England.
   Darzi, Ara; Yang, Guang-Zhong, Univ London Imperial Coll Sci Technol \& Med, Inst Global Hlth Innovat, London W2 1NY, England.}},
DOI = {{10.1097/SLA.0b013e31822513c6}},
ISSN = {{0003-4932}},
EISSN = {{1528-1140}},
Keywords-Plus = {{MINIMALLY INVASIVE SURGERY; VISUAL-ATTENTION; REORIENTATION; TRACKING;
   MODEL}},
Research-Areas = {{Surgery}},
Web-of-Science-Categories  = {{Surgery}},
Author-Email = {{m.sodergren@imperial.ac.uk}},
ResearcherID-Numbers = {{Orihuela-Espina, Felipe/J-9845-2012
   }},
ORCID-Numbers = {{Orihuela-Espina, Felipe/0000-0001-8963-7283
   teare, Julian/0000-0003-3551-9139}},
Number-of-Cited-References = {{20}},
Times-Cited = {{9}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{9}},
Journal-ISO = {{Ann. Surg.}},
Doc-Delivery-Number = {{794OC}},
Unique-ID = {{ISI:000292908700012}},
DA = {{2019-10-28}},
}

@article{ ISI:000292110400003,
Author = {Keller, Carmen},
Title = {{Using a Familiar Risk Comparison Within a Risk Ladder to Improve Risk
   Understanding by Low Numerates: A Study of Visual Attention}},
Journal = {{RISK ANALYSIS}},
Year = {{2011}},
Volume = {{31}},
Number = {{7}},
Pages = {{1043-1054}},
Month = {{JUL}},
Abstract = {{Previous experimental research provides evidence that a familiar risk
   comparison within a risk ladder is understood by low-and high-numerate
   individuals. It especially helps low numerates to better evaluate risk.
   In the present study, an eye tracker was used to capture individuals'
   visual attention to a familiar risk comparison, such as the risk
   associated with smoking. Two parameters of information
   processing-efficiency and level-were derived from visual attention. A
   random sample of participants from the general population (N = 68)
   interpreted a given risk level with the help of the risk ladder.
   Numeracy was negatively correlated with overall visual attention on the
   risk ladder (r(s) = -0.28, p = 0.01), indicating that the lower the
   numeracy, the more the time spent looking at the whole risk ladder.
   Numeracy was positively correlated with the efficiency of processing
   relevant frequency (rs = 0.34, p < 0.001) and relevant textual
   information (r(s) = 0.34, p < 0.001), but not with the efficiency of
   processing relevant comparative information and numerical information.
   There was a significant negative correlation between numeracy and the
   level of processing of relevant comparative risk information (r(s) =
   -0.21, p < 0.01), indicating that low numerates processed the
   comparative risk information more deeply than the high numerates. There
   was no correlation between numeracy and perceived risk. These results
   add to previous experimental research, indicating that the smoking risk
   comparison was crucial for low numerates to evaluate and understand
   risk. Furthermore, the eye-tracker method is promising for studying
   information processing and improving risk communication formats.}},
Publisher = {{WILEY-BLACKWELL}},
Address = {{COMMERCE PLACE, 350 MAIN ST, MALDEN 02148, MA USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Keller, C (Reprint Author), ETH, IED, Univ Str 22,CHN J75-2, CH-8092 Zurich, Switzerland.
   ETH, IED, CH-8092 Zurich, Switzerland.}},
DOI = {{10.1111/j.1539-6924.2010.01577.x}},
ISSN = {{0272-4332}},
Keywords = {{Eye tracker; familiar risk comparison; information processing; numeracy;
   risk communication; visual attention}},
Keywords-Plus = {{BREAST-CANCER RISK; SUBJECTIVE NUMERACY; EYE-MOVEMENTS; HEALTH-RISK;
   PERCEPTION; COMMUNICATION; INFORMATION; SCALE; MOTIVATION; CONSUMERS}},
Research-Areas = {{Public, Environmental \& Occupational Health; Mathematics; Mathematical
   Methods In Social Sciences}},
Web-of-Science-Categories  = {{Public, Environmental \& Occupational Health; Mathematics,
   Interdisciplinary Applications; Social Sciences, Mathematical Methods}},
Author-Email = {{ckeller@ethz.ch}},
ResearcherID-Numbers = {{Keller, Carmen/G-3546-2013}},
Number-of-Cited-References = {{34}},
Times-Cited = {{14}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{13}},
Journal-ISO = {{Risk Anal.}},
Doc-Delivery-Number = {{783UV}},
Unique-ID = {{ISI:000292110400003}},
DA = {{2019-10-28}},
}

@article{ ISI:000288020200016,
Author = {Vidal-Abarca, Eduardo and Martinez, Tomas and Salmeron, Ladislao and
   Cerdan, Raquel and Gilabert, Ramiro and Gil, Laura and Mana, Amelia and
   Llorens, Ana C. and Ferris, Ricardo},
Title = {{Recording online processes in task-oriented reading with Read\&Answer}},
Journal = {{BEHAVIOR RESEARCH METHODS}},
Year = {{2011}},
Volume = {{43}},
Number = {{1}},
Pages = {{179-192}},
Month = {{MAR}},
Abstract = {{We present an application to study task-oriented reading processes
   called Read\&Answer. The application mimics paper-and-pencil situations
   in which a reader interacts with one or more documents to perform a
   specific task, such as answering questions, writing an essay, or similar
   activities. Read\&Answer presents documents and questions with a mask.
   The reader unmasks documents and questions so that only a piece of
   information is available at a time. This way the entire interaction
   between the reader and the documents on the task is recorded and can be
   analyzed. We describe Read\&Answer and present its applications for
   research and assessment. Finally, we explain two studies that compare
   readers' performance on Read\&Answer with students' reading times and
   comprehension levels on a paper-and-pencil task, and on a computer task
   recorded with eye-tracking. The use of Read\&Answer produced similar
   comprehension scores, although it changed the pattern of reading times.}},
Publisher = {{SPRINGER}},
Address = {{233 SPRING ST, NEW YORK, NY 10013 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Vidal-Abarca, E (Reprint Author), Univ Valencia, Avda Blasco Ibanez 21, Valencia 46010, Spain.
   Vidal-Abarca, Eduardo; Martinez, Tomas; Salmeron, Ladislao; Cerdan, Raquel; Gilabert, Ramiro; Gil, Laura; Mana, Amelia; Llorens, Ana C.; Ferris, Ricardo, Univ Valencia, Valencia 46010, Spain.}},
DOI = {{10.3758/s13428-010-0032-1}},
ISSN = {{1554-351X}},
EISSN = {{1554-3528}},
Keywords = {{Reading comprehension; Reading strategies; Task-oriented reading;
   Reading processes; Reading assessment}},
Keywords-Plus = {{EXPOSITORY TEXT COMPREHENSION; RESTRICTED FOCUS VIEWER; MULTIPLE
   DOCUMENTS; EYE-MOVEMENTS; INFORMATION; TRACKING; FIXATIONS; KNOWLEDGE;
   RELEVANCE}},
Research-Areas = {{Psychology}},
Web-of-Science-Categories  = {{Psychology, Mathematical; Psychology, Experimental}},
Author-Email = {{vidala@uv.es}},
ResearcherID-Numbers = {{Salmeron, Ladislao/C-7562-2011
   Vidal-Abarca, Eduardo/L-6255-2014
   Mana Lloria, Amelia/L-6936-2014
   Cerdan, Raquel/L-4542-2014
   Gil, Laura/C-9911-2011}},
ORCID-Numbers = {{Salmeron, Ladislao/0000-0002-4283-4279
   Vidal-Abarca, Eduardo/0000-0002-5332-6106
   Mana Lloria, Amelia/0000-0002-5304-5611
   Cerdan, Raquel/0000-0002-8283-8995
   Gil, Laura/0000-0002-0937-6245}},
Number-of-Cited-References = {{36}},
Times-Cited = {{38}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{12}},
Journal-ISO = {{Behav. Res. Methods}},
Doc-Delivery-Number = {{730FW}},
Unique-ID = {{ISI:000288020200016}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000300056700277,
Author = {Selpi and Wilhelm, Torsten and Jansson, Marcus and Hagstrom, Li and
   Brandin, Niklas and Andersson, Magnus and Gronvall, John-Fredrik},
Book-Group-Author = {{IEEE}},
Title = {{Automatic real-time FACS-coder to anonymise drivers in eye tracker
   videos}},
Booktitle = {{2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV
   WORKSHOPS)}},
Year = {{2011}},
Note = {{IEEE International Conference on Computer Vision (ICCV), Barcelona,
   SPAIN, NOV 06-13, 2011}},
Organization = {{IEEE; Toyota; Google; Microsoft Res; Siemens; Technicolor; Adobe;
   Alcatel Lucent; Gentex Corp; Kooaba Image Recognit; Mitsubishi Elect;
   Mobileye; Object Video (OV); Toshiba; Xerox; Zeiss; 2d3; SATURNUS}},
Abstract = {{Driver's face is a rich source of information for understanding driver
   behaviour. From the driver's face, one could get an idea of the driver's
   emotional state and where s/he looks at. In recent years, naturalistic
   driving studies and field operational tests have been conducted to
   collect driver behavioural data, which often includes video of the
   driver, from many drivers driving for an extended period of time. Due to
   the Data Privacy Act, it is desirable to make the driver video
   anonymous, while preserving the original facial expressions. This paper
   describes our attempt to make a system that could do so. The system is a
   combination of an automatic Facial Action Coding System (FACS) coder
   based on Active Appearance Models (AAMs), a classifier that analyses
   local deformations in the AAM shape mesh and a 3D visualisation. The
   image acquisition hardware is based on a SmartEye eye tracker installed
   in a vehicle. The eye tracker we used provides a constant image quality
   independent of external illumination, which is a precondition for
   deploying the system in a vehicle environment. While the system uses
   Action Unit (AU) activations internally, the evaluation was done using
   the six basic emotions.}},
Publisher = {{IEEE}},
Address = {{345 E 47TH ST, NEW YORK, NY 10017 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Selpi (Reprint Author), Chalmers Univ Technol, Dept Appl Mech, SE-41296 Gothenburg, Sweden.
   Selpi; Jansson, Marcus; Hagstrom, Li, Chalmers Univ Technol, Dept Appl Mech, SE-41296 Gothenburg, Sweden.
   Wilhelm, Torsten, Smart Eye AB, Gothenburg, Sweden.
   Brandin, Niklas; Andersson, Magnus, Raven AB, Gothenburg, Sweden.
   Gronvall, John-Fredrik, Volvo Car Corp, Gothenburg, Sweden.}},
ISBN = {{978-1-4673-0063-6}},
Keywords-Plus = {{FACIAL EXPRESSION RECOGNITION}},
Research-Areas = {{Computer Science; Engineering}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Engineering, Electrical \&
   Electronic}},
Author-Email = {{selpi@chalmers.se
   torsten@smarteye.se
   marcus.ne.jansson@gmail.com
   li.hagstrom@chalmers.se
   niklas@ravenproduktion.se
   magnus@ravenproduktion.se
   jgronval@volvocars.com}},
Number-of-Cited-References = {{16}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{2}},
Doc-Delivery-Number = {{BYS96}},
Unique-ID = {{ISI:000300056700277}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000393147800512,
Author = {Takahashi, Shigeo},
Book-Group-Author = {{ITE/SID}},
Title = {{Directing Visual Attention Through Visualization Techniques}},
Booktitle = {{IDW'11: PROCEEDINGS OF THE 18TH INTERNATIONAL DISPLAY WORKSHOPS, VOLS
   1-3}},
Series = {{IDW-International Display Workshops}},
Year = {{2011}},
Pages = {{1979-1982}},
Note = {{18th International Display Workshops (IDW `11), Nagoya Congress Ctr,
   Nagoya, JAPAN, DEC 07-09, 2011}},
Organization = {{Inst Image Informat \& Televis Engineers; Soc Informat Display}},
Abstract = {{A crucial ingredient of computer visualization is to seek visually
   plausible representation of given datasets for better understanding of
   given datasets. Nonetheless, the rapid increase of the amount of data
   makes the associated visual representation further complicated, which
   prevents us from quickly interpreting the datasets we have to
   understand. We present techniques for intentionally directing the
   observers' visual attention to specific features of the visual
   representation. This is accomplished by modulating the bottom-up and
   top-down saliency of the visual representation in our approach. The
   bottom-up saliency has been controlled applying semantic depth of fields
   according to the importance of the features contained in the visual
   representation. We also introduce distortion to the compositional
   arrangement of pictorial depth cues so that we can intentionally enhance
   the top-down saliency of the target visual representation. Several
   experiments have been conducted together with an eye tracker in order to
   validate our approach for guiding the spatial distribution of visual
   attention on the visualization images.}},
Publisher = {{INST IMAGE INFORMATION \& TELEVISION ENGINEERS}},
Address = {{KIKAI-SHINKO-KAIKAN, 3-5-8 SHIBA-KOEN, MINATO-KU, TOKYO 105-0011, JAPAN}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Takahashi, S (Reprint Author), Univ Tokyo, Grad Sch Frontier Sci, 5-1-5 Kashiwanoha, Kashiwa, Chiba 2778561, Japan.
   Takahashi, Shigeo, Univ Tokyo, Grad Sch Frontier Sci, 5-1-5 Kashiwanoha, Kashiwa, Chiba 2778561, Japan.}},
ISSN = {{1883-2490}},
Research-Areas = {{Engineering; Optics}},
Web-of-Science-Categories  = {{Engineering, Electrical \& Electronic; Optics}},
Number-of-Cited-References = {{5}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Doc-Delivery-Number = {{BG9EI}},
Unique-ID = {{ISI:000393147800512}},
DA = {{2019-10-28}},
}

@article{ ISI:000285476500002,
Author = {Bailey, R. and Bau, D.},
Title = {{Ensemble smoother assimilation of hydraulic head and return flow data to
   estimate hydraulic conductivity distribution}},
Journal = {{WATER RESOURCES RESEARCH}},
Year = {{2010}},
Volume = {{46}},
Month = {{DEC 16}},
Abstract = {{Numerical groundwater models, frequently used to enhance understanding
   of the hydrologic and chemical processes in local or regional aquifers,
   are often hindered by an incomplete representation of the parameters
   which characterize these processes. In this study, we present the use of
   a data assimilation algorithm that incorporates all past model results
   and data measurements, an ensemble smoother (ES) to provide enhanced
   estimates of aquifer hydraulic conductivity (K) through assimilation of
   hydraulic head (H) and groundwater return flow volume (RFV) measurements
   into groundwater model simulation results. On the basis of the Kalman
   filter methodology, residuals between forecasted model results and
   measurements, together with covariances between model results at
   measurement locations and nonmeasurement locations, are used to correct
   model results. Parameter estimation is achieved by incorporating model
   parameters into the algorithm, thus allowing the correlation between H,
   RFV, and K to correct the K fields. The applicability of the ES is
   demonstrated using a synthetic two-dimensional transient groundwater
   modeling simulation. Sensitivity analyses are carried out to show the
   performance of the ES in regard to measurement error, number of
   measurements, number of assimilation times, correlation length of the K
   fields, and the number of stream gage locations. Results show that the
   departure of the K fields from a reference K field is greatly reduced
   through data assimilation and demonstrate that the ES scheme is a
   promising alternative to other inverse modeling techniques because of
   low computational burden and the ability to run the algorithm entirely
   independent of the groundwater model simulation.}},
Publisher = {{AMER GEOPHYSICAL UNION}},
Address = {{2000 FLORIDA AVE NW, WASHINGTON, DC 20009 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Bailey, R (Reprint Author), Colorado State Univ, Dept Civil \& Environm Engn, 1372 Campus Delivery, Ft Collins, CO 80523 USA.
   Bailey, R.; Bau, D., Colorado State Univ, Dept Civil \& Environm Engn, Ft Collins, CO 80523 USA.}},
DOI = {{10.1029/2010WR009147}},
Article-Number = {{W12543}},
ISSN = {{0043-1397}},
EISSN = {{1944-7973}},
Keywords-Plus = {{HYDROLOGIC DATA ASSIMILATION; GROUNDWATER-FLOW; PARAMETER-ESTIMATION;
   KALMAN SMOOTHER; INVERSE PROBLEM; MASS-TRANSPORT; STEADY-STATE;
   TRANSMISSIVITY; FILTER; UNCERTAINTY}},
Research-Areas = {{Environmental Sciences \& Ecology; Marine \& Freshwater Biology; Water
   Resources}},
Web-of-Science-Categories  = {{Environmental Sciences; Limnology; Water Resources}},
Author-Email = {{rtbailey@engr.colostate.edu
   domenico.bau@colostate.edu}},
ResearcherID-Numbers = {{Bau, Domenico/D-7023-2011
   }},
ORCID-Numbers = {{Bailey, Ryan/0000-0002-6539-1474}},
Funding-Acknowledgement = {{Colorado Agricultural Experiment Station (CAES) {[}COL00690]}},
Funding-Text = {{The majority of this work has been made possible by a Colorado
   Agricultural Experiment Station (CAES) grant (project COL00690). We
   would also like to acknowledge Harrie-Jan Hendricks Franssen as well as
   two anonymous reviewers for their helpful comments and suggestions in
   improving the content of this paper.}},
Number-of-Cited-References = {{46}},
Times-Cited = {{33}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{9}},
Journal-ISO = {{Water Resour. Res.}},
Doc-Delivery-Number = {{696YE}},
Unique-ID = {{ISI:000285476500002}},
DA = {{2019-10-28}},
}

@article{ ISI:000284231800030,
Author = {Klucken, Tim and Brouwer, Anne-Marie and Chatziastros, Astros and
   Kagerer, Sabine and Netter, Petra and Hennig, Juergen},
Title = {{The Impact of Coping Style on Gaze Duration}},
Journal = {{PLOS ONE}},
Year = {{2010}},
Volume = {{5}},
Number = {{11}},
Month = {{NOV 15}},
Abstract = {{The understanding of individual differences in response to threat (e.g.,
   attentional bias) is important to better understand the development of
   anxiety disorders. Previous studies revealed only a small attentional
   bias in high-anxious (HA) subjects. One explanation for this finding may
   be the assumption that all HA-subjects show a constant attentional bias.
   Current models distinguish HA-subjects depending on their level of
   tolerance for uncertainty and for arousal. These models assume that only
   HA-subjects with intolerance for uncertainty but tolerance for arousal
   ({''}sensitizers{''}) show an attentional bias, compared to HA-subjects
   with intolerance for uncertainty and intolerance for arousal
   ({''}fluctuating subjects{''}). Further, it is assumed that repressors
   (defined as intolerance for arousal but tolerance for uncertainty) would
   react with avoidance behavior when confronted with threatening stimuli.
   The present study investigated the influence of coping styles on
   attentional bias. After an extensive recruiting phase, 36 subjects were
   classified into three groups (sensitizers, fluctuating, and repressors).
   All subjects were exposed to presentations of happy and threatening
   faces, while recording gaze durations with an eye-tracker. The results
   showed that only sensitizer showed an attentional bias: they gazed
   longer at the threatening face rather than at the happy face during the
   first 500 ms. The results support the findings of the relationship
   between anxiety and attention and extend these by showing variations
   according to coping styles. The differentiation of subjects according to
   a multifaceted coping style allows a better prediction of the
   attentional bias and contributes to an insight into the complex
   interplay of personality, coping, and behavior.}},
Publisher = {{PUBLIC LIBRARY SCIENCE}},
Address = {{1160 BATTERY STREET, STE 100, SAN FRANCISCO, CA 94111 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Klucken, T (Reprint Author), Univ Giessen, Bender Inst Neuroimaging, Giessen, Germany.
   Klucken, Tim; Kagerer, Sabine, Univ Giessen, Bender Inst Neuroimaging, Giessen, Germany.
   Brouwer, Anne-Marie, TNO Human Factors, Soesterberg, Netherlands.
   Netter, Petra; Hennig, Juergen, Univ Giessen, Dept Psychol, Giessen, Germany.
   Chatziastros, Astros, Max Planck Inst Biol Cybernet, Tubingen, Germany.}},
DOI = {{10.1371/journal.pone.0015395}},
Article-Number = {{e15395}},
ISSN = {{1932-6203}},
Keywords-Plus = {{ATTENTIONAL BIAS; TRAIT-ANXIETY; EMOTIONAL FACES; TIME-COURSE; THREAT;
   DISORDERS; INFORMATION; REPRESSORS; AVOIDANCE; VIGILANCE}},
Research-Areas = {{Science \& Technology - Other Topics}},
Web-of-Science-Categories  = {{Multidisciplinary Sciences}},
Author-Email = {{Tim.Klucken@psychol.uni-giessen.de}},
ResearcherID-Numbers = {{Klucken, Tim/F-7669-2013
   }},
ORCID-Numbers = {{Klucken, Tim/0000-0003-2433-6652
   Lauwereyns, Jan/0000-0003-0551-2550}},
Funding-Acknowledgement = {{University of Giessen; TNO Human factors}},
Funding-Text = {{This work was supported by the postdoc stipend ``Junior Science and
   Teaching Units{''} from the University of Giessen to the first author.
   The second author is supported by TNO Human factors. All funders had no
   role in study design, data collection and analysis, decision to publish,
   or preparation of the manuscript.}},
Number-of-Cited-References = {{41}},
Times-Cited = {{4}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{18}},
Journal-ISO = {{PLoS One}},
Doc-Delivery-Number = {{680KW}},
Unique-ID = {{ISI:000284231800030}},
OA = {{DOAJ Gold, Green Published}},
DA = {{2019-10-28}},
}

@article{ ISI:000275777300001,
Author = {Shiogai, Y. and Stefanovska, A. and McClintock, P. V. E.},
Title = {{Nonlinear dynamics of cardiovascular ageing}},
Journal = {{PHYSICS REPORTS-REVIEW SECTION OF PHYSICS LETTERS}},
Year = {{2010}},
Volume = {{488}},
Number = {{2-3}},
Pages = {{51-110}},
Month = {{MAR}},
Abstract = {{The application of methods drawn from nonlinear and stochastic dynamics
   to the analysis of cardiovascular time series is reviewed, with
   particular reference to the identification of changes associated with
   ageing. The natural variability of the heart rate (HRV) is considered in
   detail, including the respiratory sinus arrhythmia (RSA) corresponding
   to modulation of the instantaneous cardiac frequency by the rhythm of
   respiration. HRV has been intensively studied using traditional spectral
   analyses, e.g. by Fourier transform or autoregressive methods, and,
   because of its complexity, has been used as a paradigm for testing
   several proposed new methods of complexity analysis. These methods are
   reviewed. The application of time-frequency methods to HRV is
   considered, including in particular the wavelet transform which can
   resolve the time-dependent spectral content of HRV. Attention is focused
   on the cardio-respiratory interaction by introduction of the respiratory
   frequency variability signal (RFV), which can be acquired simultaneously
   with HRV by use of a respiratory effort transducer. Current methods for
   the analysis of interacting oscillators are reviewed and applied to
   cardio-respiratory data, including those for the quantification of
   synchronization and direction of coupling. These reveal the effect of
   ageing on the cardio-respiratory interaction through changes in the
   mutual modulation of the instantaneous cardiac and respiratory
   frequencies. Analyses of blood flow signals recorded with laser Doppler
   flowmetry are reviewed and related to the current understanding of how
   endothelial-dependent oscillations evolve with age: the inner lining of
   the vessels (the endothelium) is shown to be of crucial importance to
   the emerging picture. It is concluded that analyses of the complex and
   nonlinear dynamics of the cardiovascular system can illuminate the
   mechanisms of blood circulation, and that the heart, the lungs and the
   vascular system function as a single entity in dynamical terms. Clear
   evidence is found for dynamical ageing. (C) 2009 Elsevier B.V. All
   rights reserved.}},
Publisher = {{ELSEVIER SCIENCE BV}},
Address = {{PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS}},
Type = {{Review}},
Language = {{English}},
Affiliation = {{McClintock, PVE (Reprint Author), Univ Lancaster, Dept Phys, Lancaster LA1 4YB, England.
   Shiogai, Y.; Stefanovska, A.; McClintock, P. V. E., Univ Lancaster, Dept Phys, Lancaster LA1 4YB, England.
   Stefanovska, A., Univ Ljubljana, Fac Elect Engn, Ljubljana, Slovenia.}},
DOI = {{10.1016/j.physrep.2009.12.003}},
ISSN = {{0370-1573}},
EISSN = {{1873-6270}},
Keywords = {{Coupled oscillators; Wavelet transform; Synchronization; Ageing;
   Complexity; Phase dynamics; Heart rate variability; Iontophoresis;
   Endothelial function; Blood flow}},
Keywords-Plus = {{HEART-RATE-VARIABILITY; FRACTIONAL GAUSSIAN NOISES; LASER-DOPPLER
   FLOWMETRY; SKIN BLOOD-FLOW; ENDOTHELIUM-DEPENDENT VASODILATION; HUMAN
   CARDIORESPIRATORY SYSTEM; RESPIRATORY SINUS ARRHYTHMIA; LOW-FREQUENCY
   OSCILLATIONS; TIME-SERIES; WAVELET ANALYSIS}},
Research-Areas = {{Physics}},
Web-of-Science-Categories  = {{Physics, Multidisciplinary}},
Author-Email = {{p.v.e.mcclintock@lancaster.ac.uk}},
ORCID-Numbers = {{Stefanovska, Aneta/0000-0001-6952-8370}},
Funding-Acknowledgement = {{Wellcome Trust (UK)Wellcome Trust; Slovenian Research AgencySlovenian
   Research Agency - Slovenia; Engineering and Physical Sciences Research
   Council (UK)Engineering \& Physical Sciences Research Council (EPSRC);
   Economic and Social Research Council (UK)Economic \& Social Research
   Council (ESRC); ECEuropean Commission Joint Research CentreEuropean
   Community (EC) {[}517133 NEST]; Engineering and Physical Sciences
   Research CouncilEngineering \& Physical Sciences Research Council
   (EPSRC) {[}EP/D000610/1]}},
Funding-Text = {{We are very grateful for valuable discussions with Alireza Bahraminasab,
   Andriy Bandrivskyy, Alan Bernjak, Peter Clarkson, Philip Clemson, Andrea
   Duggento, Alison Hale, David Kenwright, Bojan Musizza, Jane Owen-Lynch,
   Milan Palus and Janko Petrovcic. One of us (YS) would like to
   acknowledge the hospitality of the Faculty of Electrical Engineering in
   the University of Ljubljana (Slovenia) where she learned techniques for
   physiological data acquisition and analysis, in particular from Alan
   Bernjak and Bojan Musizza. We are deeply indebted to all the volunteers
   that participated in the study. The research was supported by the
   Wellcome Trust (UK) and in part by the Slovenian Research Agency, the
   Engineering and Physical Sciences Research Council (UK), by the New
   Dynamics of Ageing programme of the Economic and Social Research Council
   (UK), and by the EC FP6 project BRACCIA (Contract No. 517133 NEST).}},
Number-of-Cited-References = {{206}},
Times-Cited = {{170}},
Usage-Count-Last-180-days = {{3}},
Usage-Count-Since-2013 = {{67}},
Journal-ISO = {{Phys. Rep.-Rev. Sec. Phys. Lett.}},
Doc-Delivery-Number = {{571SY}},
Unique-ID = {{ISI:000275777300001}},
OA = {{Green Published, Other Gold}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000287356400029,
Author = {Sharif, Bonita and Maletic, Jonathan I.},
Book-Group-Author = {{IEEE}},
Title = {{An Eye Tracking Study on the Effects of Layout in Understanding the Role
   of Design Patterns}},
Booktitle = {{2010 IEEE INTERNATIONAL CONFERENCE ON SOFTWARE MAINTENANCE}},
Series = {{Proceedings-IEEE International Conference on Software Maintenance}},
Year = {{2010}},
Note = {{International Conference on Software Maintenance, Timisoara, ROMANIA,
   SEP 12-18, 2010}},
Abstract = {{The effect of layout in the comprehension of design pattern roles in UML
   class diagrams is assessed. This work replicates and extends a previous
   study using questionnaires but uses an eye tracker to gather additional
   data. The purpose of the replication is to gather more insight into the
   eye gaze behavior not evident from questionnaire-based methods.
   Similarities and differences between the studies are presented. Four
   design patterns are examined in two layout schemes in the context of
   three open source systems. Fifteen participants answered a series of
   eight design pattern role detection questions. Results show a
   significant improvement in role detection accuracy and visual effort
   with a certain layout for the Strategy and Observer patterns and a
   significant improvement in role detection time for all four patterns.
   Eye gaze data indicates classes participating in a design pattern act
   like visual beacons when they are in close physical proximity and follow
   the canonical layout, even though they violate some general graph
   aesthetics.}},
Publisher = {{IEEE}},
Address = {{345 E 47TH ST, NEW YORK, NY 10017 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Sharif, B (Reprint Author), Kent State Univ, Dept Comp Sci, Kent, OH 44242 USA.
   Sharif, Bonita; Maletic, Jonathan I., Kent State Univ, Dept Comp Sci, Kent, OH 44242 USA.}},
ISSN = {{1063-6773}},
ISBN = {{978-1-4244-8629-8}},
Keywords = {{eye-tracking study; UML class diagram layout; design pattern roles}},
Keywords-Plus = {{UML CLASS DIAGRAMS; COMPREHENSION}},
Research-Areas = {{Computer Science}},
Web-of-Science-Categories  = {{Computer Science, Software Engineering}},
Author-Email = {{bsimoes@cs.kent.edu
   jmaletic@cs.kent.edu}},
Number-of-Cited-References = {{22}},
Times-Cited = {{2}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Doc-Delivery-Number = {{BTM80}},
Unique-ID = {{ISI:000287356400029}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000286578100003,
Author = {Ramanathan, Subramanian and Katti, Harish and Sebe, Nicu and
   Kankanhalli, Mohan and Chua, Tat-Seng},
Editor = {{Daniilidis, K and Maragos, P and Paragios, N}},
Title = {{An Eye Fixation Database for Saliency Detection in Images}},
Booktitle = {{COMPUTER VISION-ECCV 2010, PT IV}},
Series = {{Lecture Notes in Computer Science}},
Year = {{2010}},
Volume = {{6314}},
Number = {{IV}},
Pages = {{30+}},
Note = {{11th European Conference on Computer Vision, Heraklion, GREECE, SEP
   05-11, 2010}},
Organization = {{Inst Natl Rech Informat \& Automat; Google; Microsoft Res; Technicolor;
   Adobe; DynaVox Mayer-Johnson; Eur Res Consortium Informat Math; Gen
   Elect; IBM; Johnson Controls; Point Grey; Univ Houston; Siemens}},
Abstract = {{To learn the preferential visual attention given by humans to specific
   image content, we present NUSEF- an eye fixation database compiled from
   a pool of 758 images and 75 subjects. Eye fixations are an excellent
   modality to learn semantics-driven human understanding of images, which
   is vastly different from feature-driven approaches employed by saliency
   computation algorithms. The database comprises fixation patterns
   acquired using an eye-tracker, as subjects free-viewed images
   corresponding to many semantic categories such as faces (human and
   mammal), nudes and actions (look, read and shoot). The consistent
   presence of fixation clusters around specific image regions confirms
   that visual attention is riot subjective, but is directed towards
   salient objects and object-interactions.
   We then show how the fixation clusters can be exploited for enhancing
   image understanding, by using our eye fixation database in an active
   image segmentation application. Apart from proposing a mechanism to
   automatically determine characteristic fixation seeds for segmentation,
   we show that the use of fixation seeds generated from multiple fixation
   clusters on the salient object can lead to a 10\% improvement in
   segmentation performance over the state-of-the-art.}},
Publisher = {{SPRINGER-VERLAG BERLIN}},
Address = {{HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Ramanathan, S (Reprint Author), Univ Trento, Dept Informat Engn \& Comp Sci, Trento, Italy.
   Ramanathan, Subramanian; Sebe, Nicu, Univ Trento, Dept Informat Engn \& Comp Sci, Trento, Italy.
   Katti, Harish; Kankanhalli, Mohan; Chua, Tat-Seng, Natl Univ Singapore, Schl Comp, Singapore 117548, Singapore.}},
ISSN = {{0302-9743}},
EISSN = {{1611-3349}},
ISBN = {{978-3-642-15560-4}},
Keywords-Plus = {{SCENE}},
Research-Areas = {{Computer Science}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Computer Science, Theory \&
   Methods}},
Author-Email = {{subramanian@disi.unitn.it}},
ResearcherID-Numbers = {{Kankanhalli, Mohan/Q-9284-2019
   }},
ORCID-Numbers = {{Kankanhalli, Mohan/0000-0002-4846-2015
   Sebe, Niculae/0000-0002-6597-7248}},
Funding-Acknowledgement = {{FP7 IP European project GLOCAL}},
Funding-Text = {{This research has been partially supported by the FP7 IP European
   project GLOCAL.}},
Number-of-Cited-References = {{21}},
Times-Cited = {{93}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{5}},
Doc-Delivery-Number = {{BTD60}},
Unique-ID = {{ISI:000286578100003}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000274866100007,
Author = {Jeanmart, Sebastien and Gueheneuc, Yann-Gael and Sahraoui, Houari and
   Habra, Naji},
Book-Group-Author = {{IEEE}},
Title = {{Impact of the Visitor Pattern on Program Comprehension and Maintenance}},
Booktitle = {{ESEM: 2009 3RD INTERNATIONAL SYMPOSIUM ON EMPIRICAL SOFTWARE ENGINEERING
   AND MEASUREMENT}},
Series = {{International Symposium on Empirical Software Engineering and
   Measurement}},
Year = {{2009}},
Pages = {{69+}},
Note = {{3rd International Symposium on Empirical Software Engineering and
   Measurement, Lake Buena Vista, FL, OCT 15-16, 2009}},
Organization = {{Univ SE Calif, Ctr Syst \& Software Engn; ABB; Microsoft Res; IEEE; ACM
   SIGSOFT; N Carolina State Univ Comp Sci}},
Abstract = {{In the software engineering literature, many works claim that the use of
   design patterns improves the comprehensibility of programs and, more
   generally, their maintainability. Yet, little work attempted to study
   the impact of design patterns on the developers' tasks of program
   comprehension and modification. We design and perform an experiment to
   collect data on the impact of the Visitor pattern on comprehension and
   modification tasks with class diagrams. We use an eye-tracker to
   register saccades and fixations, the latter representing the focus of
   the developers' attention. Collected data show that the Visitor pattern
   plays a role in maintenance tasks: class diagrams with its canonical
   representation requires less efforts from developers.}},
Publisher = {{IEEE}},
Address = {{345 E 47TH ST, NEW YORK, NY 10017 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Jeanmart, S (Reprint Author), Univ Namur, PReCISE Res Ctr, Namur, Belgium.
   Jeanmart, Sebastien; Habra, Naji, Univ Namur, PReCISE Res Ctr, Namur, Belgium.
   Gueheneuc, Yann-Gael, Ecole Polytech Montreal, DGIGL, Ptidej Team, Quebec City, PQ, Canada.
   Sahraoui, Houari, Univ Montreal, DIRO, GEODES, Quebec City, PQ, Canada.}},
DOI = {{10.1109/ESEM.2009.5316015}},
ISSN = {{1938-6451}},
ISBN = {{978-1-4244-4842-5}},
Keywords-Plus = {{DESIGN PATTERNS; EYE-MOVEMENTS; DOCUMENTATION; UML}},
Research-Areas = {{Computer Science}},
Web-of-Science-Categories  = {{Computer Science, Software Engineering; Computer Science, Theory \&
   Methods}},
Author-Email = {{sjeanmar@student.fundp.ac.be
   yann-gael.gueheneuc@polymtl.ca
   sahraouh@iro.umontreal.ca
   nha@info.fundp.ac.be}},
ResearcherID-Numbers = {{Gueheneuc, Yann-Gael/K-9371-2019}},
Number-of-Cited-References = {{28}},
Times-Cited = {{17}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{1}},
Doc-Delivery-Number = {{BNL70}},
Unique-ID = {{ISI:000274866100007}},
DA = {{2019-10-28}},
}

@article{ ISI:000247196400009,
Author = {Lee, Yuan-Chieh},
Title = {{Active eye-tracking improves LASIK results}},
Journal = {{JOURNAL OF REFRACTIVE SURGERY}},
Year = {{2007}},
Volume = {{23}},
Number = {{6}},
Pages = {{581-585}},
Month = {{JUN}},
Note = {{Meeting of the International-Society-of-Refractive-Surgery of the
   American-Academy-of-Ophthalmology, Hong Kong, PEOPLES R CHINA, MAY
   14-16, 2005}},
Organization = {{Int Soc Refract Surg; Amer Acad Ophthalmol}},
Abstract = {{Purpose: To study the advantage of active eye-tracking for
   photorefractive surgery.
   Methods: In a prospective, double-masked study, LASIK for myopia and
   myopic astigmatism was performed in 50 patients using the ALLEGRETTO
   WAVE version 1007. All patients received LASIK with full comprehension
   of the importance of fixation during the procedure. All surgical
   procedures were performed by a single surgeon. The eye-tracker was
   turned off in one group (n=25) and kept on: in another group (n=25).
   Preoperatively and 3 months postoperatively, patients underwent a
   standard ophthalmic examination, which included corneal topography.
   Results: In the patients treated with the eye-tracker off, all had
   uncorrected visual acuity (UCVA) of >= 20/40 and 64\% had >= 20/20.
   Compared with the patients treated with the eye-tracker on, they had
   higher residual cylindrical astigmatism (P<.05). Those treated with the
   eye-tracker on achieved better UCVA and best spectacle-corrected visual
   acuity (P<.05). Spherical error and potential visual acuity (TMS-II)
   were not significantly different between the groups.
   Conclusions: The flying-spot system can achieve a fair result without
   active eye-tracking, but active eye-tracking helps improve the visual
   outcome and reduces postoperative cylindrical astigmatism.}},
Publisher = {{SLACK INC}},
Address = {{6900 GROVE RD, THOROFARE, NJ 08086 USA}},
Type = {{Article; Proceedings Paper}},
Language = {{English}},
Affiliation = {{Lee, YC (Reprint Author), Buddhist Tzu Chi Gen Hosp, Dept Ophthalmol, 707 Sect 3,Chung Yang Rd, Hualien 970, Taiwan.
   Buddhist Tzu Chi Gen Hosp, Dept Ophthalmol, Hualien 970, Taiwan.
   Natl Taiwan Univ Hosp, Taipei, Taiwan.
   Buddhist Tzu Chi Gen Hosp, Dept Med, Hualien 970, Taiwan.
   Buddhist Tzu Chi Gen Hosp, Grad Inst Med Sci, Hualien 970, Taiwan.}},
DOI = {{10.3928/1081-597X-20070601-08}},
ISSN = {{1081-597X}},
EISSN = {{1938-2391}},
Keywords-Plus = {{IN-SITU KERATOMILEUSIS; LASER INSITU KERATOMILEUSIS; VISX S3 ACTIVETRAK;
   ABLATION CENTRATION; REFRACTIVE SURGERY; PUPIL; DECENTRATION;
   ABERRATIONS; S2}},
Research-Areas = {{Ophthalmology; Surgery}},
Web-of-Science-Categories  = {{Ophthalmology; Surgery}},
Author-Email = {{derrick@url.com.tw}},
Number-of-Cited-References = {{18}},
Times-Cited = {{10}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{5}},
Journal-ISO = {{J. Refractive Surg.}},
Doc-Delivery-Number = {{178BO}},
Unique-ID = {{ISI:000247196400009}},
DA = {{2019-10-28}},
}

@article{ ISI:000248501300015,
Author = {Bednarik, Roman and Tukiainen, Nlkrkku},
Title = {{Validating the restricted focus viewer: A study using eye-movement
   tracking}},
Journal = {{BEHAVIOR RESEARCH METHODS}},
Year = {{2007}},
Volume = {{39}},
Number = {{2}},
Pages = {{274-282}},
Month = {{MAY}},
Abstract = {{Investigation of cognitive processes and visual attention during
   problem-solving tasks is an important part of understanding human
   reasoning. Eyetracking technology has proven to have many benefits in
   revealing visual attention patterns. However, the high price of accurate
   eyetrackers and the difficulties associated with using them represent
   major obstacles to their wider application. Therefore, previous studies
   have sought to find alternatives to eyetracking. The Restricted Focus
   Viewer (RFV) brings a small part of an otherwise blurred display to the
   focus of visual attention: A user controls what part of the screen is in
   focus by using a computer mouse and explicitly selecting the area to be
   shown in focus. Recently, some studies have employed the RFV to
   investigate cognitive behavior of users, and some researchers have even
   enhanced the tool to study usability. We replicated a previous RFV-based
   study while also recording gaze data. We compared the attention
   allocation in time and space as reported by the RFV and an eyetracker.
   Further, we investigated the effects of RFVs display blurring on the
   visual attention allocation of 18 novice and expert programmers. Our
   results indicate that the data obtained from the two tools differ. Also,
   the RFV-blurring interferes with the strategies utilized by experts, and
   has an effect on fixation duration. However, task performance was
   preserved.}},
Publisher = {{SPRINGER}},
Address = {{233 SPRING ST, NEW YORK, NY 10013 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Bednarik, R (Reprint Author), Univ Joensuu, Dept Comp Sci, POB 111, FIN-80101 Joensuu, Finland.
   Univ Joensuu, Dept Comp Sci, FIN-80101 Joensuu, Finland.}},
DOI = {{10.3758/BF03193158}},
ISSN = {{1554-351X}},
EISSN = {{1554-3528}},
Keywords-Plus = {{VISUAL-ATTENTION}},
Research-Areas = {{Psychology}},
Web-of-Science-Categories  = {{Psychology, Mathematical; Psychology, Experimental}},
Author-Email = {{bednarik@cs.joensuu.fi}},
ORCID-Numbers = {{Bednarik, Roman/0000-0003-1726-3520}},
Number-of-Cited-References = {{21}},
Times-Cited = {{11}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{8}},
Journal-ISO = {{Behav. Res. Methods}},
Doc-Delivery-Number = {{196SC}},
Unique-ID = {{ISI:000248501300015}},
DA = {{2019-10-28}},
}

@article{ ISI:000257031000006,
Author = {Chesneau, Sophie and Jbabdi, Saad and Champagne-Lavau, Maud and Giroux,
   Francine and Ska, Bernadette},
Title = {{Text comprehension, cognitive resources and aging}},
Journal = {{PSYCHOLOGIE \& NEUROPSYCHIATRIE DU VIEILLISSEMENT}},
Year = {{2007}},
Volume = {{5}},
Number = {{1}},
Pages = {{47-64}},
Month = {{MAR}},
Abstract = {{Aging brings cognitive changes. Language is not immune to these changes.
   The use of compensation strategies may permit older adults to achieve a
   performance level identical to the one obtained by younger adults. This
   research aims to study text comprehension in aging and the reading
   strategies used for by older and younger adults. Kintsch's cognitive
   model (1988) allows the identification of different levels of
   representation within text treatment (linguistic form, macrostructure,
   microstructure and situation model) and predicts the underlying
   cognitive components. Eye-tracking analyses during reading permit
   inference about the moments of reading treatment and detection of
   reading strategies. Sixty highly educated participants were assessed.
   They were divided in two age groups (20-40 and 60-80 years old).
   Participants were asked to read and understand three texts constructed
   to highlight the features of text comprehension within each one of the
   different levels of text representation. The amount of detail and the
   necessity of updating the situation model varied for each text. Eye
   movements were registered by an eyetracker (Cambridge research) during
   the reading process. Specific complementary tasks were administered to
   evaluate working memory, long-term memory, and executive functions.
   Variances analyses showed significantly lower performance by older
   adults regarding: 1) recall of the microstructure of the two texts with
   a high degree of detail, 2) macrostructure of the text with fewer
   details, and 3) performance on all tasks that evaluated cognitive
   components. Aging influenced treatment of levels of text representation
   depending on text characteristics. However, cluster analysis of the text
   comprehension and eye-tracker data revealed a group of older adults
   whose performance in reading comprehension was identical to the
   performance of younger adults, with the same reading profile. This
   result seems to show that use of compensation strategies by older adults
   at the onset of signs of cognitive deterioration is not necessary in
   reading.}},
Publisher = {{JOHN LIBBEY EUROTEXT LTD}},
Address = {{127 AVE DE LA REPUBLIQUE, 92120 MONTROUGE, FRANCE}},
Type = {{Article}},
Language = {{French}},
Affiliation = {{Chesneau, S (Reprint Author), Inst Univ Geriatr Montreal, Ctr Rech, Montreal, PQ, Canada.
   Chesneau, Sophie; Giroux, Francine; Ska, Bernadette, Inst Univ Geriatr Montreal, Ctr Rech, Montreal, PQ, Canada.
   Chesneau, Sophie; Ska, Bernadette, Univ Montreal, Fac Med, Ecole Orthophonie \& Audiol, Montreal, PQ H3C 3J7, Canada.
   Jbabdi, Saad, UPMC, INSERM, U678, Paris, France.
   Champagne-Lavau, Maud, Hop Sacre Coeur Montreal, Montreal, PQ, Canada.}},
DOI = {{10.1684/pnv.2007.0037}},
ISSN = {{1760-1703}},
Keywords = {{text comprehension; aging; compensation strategies; eye tracking}},
Keywords-Plus = {{WORKING-MEMORY; OLDER-ADULTS; LANGUAGE COMPREHENSION; AGE; YOUNGER;
   SPEECH; TASK; COMPENSATION; PERFORMANCE; INFORMATION}},
Research-Areas = {{Psychiatry; Psychology}},
Web-of-Science-Categories  = {{Psychiatry; Psychology}},
Author-Email = {{sophie.chesneau@umontreal.ca}},
ResearcherID-Numbers = {{Champagne-Lavau, Maud/F-9334-2010
   Jbabdi, Saad/A-2483-2012
   }},
ORCID-Numbers = {{Champagne-Lavau, Maud/0000-0003-4659-6957
   Jbabdi, Saad/0000-0003-3234-5639}},
Number-of-Cited-References = {{56}},
Times-Cited = {{6}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{14}},
Journal-ISO = {{Psychol. NeuroPsychiatr. Vieil.}},
Doc-Delivery-Number = {{317PB}},
Unique-ID = {{ISI:000257031000006}},
DA = {{2019-10-28}},
}

@article{ ISI:000245037000006,
Author = {Jansen, Anthony R. and Marriott, Kim and Yelland, Greg W.},
Title = {{Parsing of algebraic expressions by experienced users of mathematics}},
Journal = {{EUROPEAN JOURNAL OF COGNITIVE PSYCHOLOGY}},
Year = {{2007}},
Volume = {{19}},
Number = {{2}},
Pages = {{286-320}},
Abstract = {{The present study investigates how experienced users of mathematics
   parse algebraic expressions. The main issues examined are the order in
   which the symbols in an expression are scanned and the duration of
   fixation. Two experiments tracked the order in which the symbols of an
   expression were scanned. The results were analysed using Markov Chain
   models of the scanpath data and provided strong support for the
   hypothesised scanning order: a left-to-right, top-to-bottom syntax-based
   scanning order. Length of fixation was also analysed in the first
   experiment. When reading text, readers pause significantly longer at the
   end of clauses and sentences. A similar pattern was found for
   mathematical expressions: Symbols at the end of a phrasal constituent
   were fixated upon for significantly longer than symbols at the start or
   middle of the phrasal constituent. These results suggest that the
   parsing of algebraic expressions has marked similarities with the way in
   which sentences of natural language are processed and reinforces the
   importance of syntax in their comprehension.}},
Publisher = {{PSYCHOLOGY PRESS}},
Address = {{27 CHURCH RD, HOVE BN3 2FA, EAST SUSSEX, ENGLAND}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Marriott, K (Reprint Author), Monash Univ, Sch Comp Sci \& Software Engn, Clayton, Vic 3800, Australia.
   Monash Univ, Sch Comp Sci \& Software Engn, Clayton, Vic 3800, Australia.
   Monash Univ, Dept Psychol, Clayton, Vic 3800, Australia.}},
DOI = {{10.1080/09541440600709955}},
ISSN = {{0954-1446}},
Keywords-Plus = {{TRACKING VISUAL-ATTENTION; RESTRICTED FOCUS VIEWER; HIDDEN
   MARKOV-MODELS; EYE-MOVEMENTS; COMPREHENSION; RECOGNITION; INFORMATION;
   CONTEXT; SYNTAX; TOOL}},
Research-Areas = {{Psychology}},
Web-of-Science-Categories  = {{Psychology, Experimental}},
Author-Email = {{marriott@mail.csse.monash.edu.au}},
ORCID-Numbers = {{Yelland, Gregory/0000-0002-9439-253X}},
Number-of-Cited-References = {{34}},
Times-Cited = {{9}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{7}},
Journal-ISO = {{Eur. J. Cogn. Psychol.}},
Doc-Delivery-Number = {{147XB}},
Unique-ID = {{ISI:000245037000006}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000246502100059,
Author = {Krishna, Sreekar and Black, John A. and Braiman, Stuart and
   Panchanathan, Sethuraman},
Editor = {{Rogowitz, BE and Pappas, TN and Daly, SJ}},
Title = {{Temporal relation between bottom-up vs top-down strategies for gaze
   prediction}},
Booktitle = {{HUMAN VISION AND ELECTRONIC IMAGING XII}},
Series = {{Proceedings of SPIE}},
Year = {{2007}},
Volume = {{6492}},
Note = {{Conference on Human Vision and Electronic Imaging XII, San Jose, CA, JAN
   29-FEB 01, 2007}},
Organization = {{Soc Imaging Sci \& Technol; SPIE}},
Abstract = {{Much research has been focused on the study of bottom-up, feature-based
   visual perception, as a means to generate salience maps, and predict the
   distribution of fixations within images. However, it is plausible that
   the eventual perception of distinct objects within a 3D scene (and the
   subsequent top-down effects) would also have a significant effect on the
   distributions of fixations within that scene. This research is aimed at
   testing a hypothesis that there exists a switching from feature-based to
   object-based scanning of images, as the viewer gains a higher-level
   understanding of the image content, and that this switching can be
   detected by changes in the pattern of eye fixations within the image. An
   eye tracker is used to monitor the fixations of human participants over
   time, as they view images, in an effort to answer questions pertaining
   to (1) the nature of fixations during bottom-up and top-down scene scan
   scenarios (2) the ability of assessing whether the subject is perceiving
   the scene content based on low-level visual features or distinct
   objects, and (3) identification of the participant's transition from a
   bottom-up feature-based perception to a top-down object-based
   perception.}},
Publisher = {{SPIE-INT SOC OPTICAL ENGINEERING}},
Address = {{1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Krishna, S (Reprint Author), Arizona State Univ, Ctr Cognit Ubiquitous Comp CUbiC, 699 S Mill Ave, Tempe, AZ 85281 USA.
   Krishna, Sreekar; Black, John A.; Braiman, Stuart; Panchanathan, Sethuraman, Arizona State Univ, Ctr Cognit Ubiquitous Comp CUbiC, 699 S Mill Ave, Tempe, AZ 85281 USA.}},
DOI = {{10.1117/12.707574}},
Article-Number = {{64921Q}},
ISSN = {{0277-786X}},
ISBN = {{978-0-8194-6605-1}},
Keywords = {{eye-tracking; bottom-up and top-down scene scan; nature of saccades and
   fixations; object content}},
Research-Areas = {{Computer Science; Imaging Science \& Photographic Technology}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Imaging Science \&
   Photographic Technology}},
Number-of-Cited-References = {{5}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Doc-Delivery-Number = {{BGF81}},
Unique-ID = {{ISI:000246502100059}},
DA = {{2019-10-28}},
}

@article{ ISI:000240742700002,
Author = {Coulson, Seana and Urbach, Thomas P. and Kutas, Marta},
Title = {{Looking back: Joke comprehension and the space structuring model}},
Journal = {{HUMOR-INTERNATIONAL JOURNAL OF HUMOR RESEARCH}},
Year = {{2006}},
Volume = {{19}},
Number = {{3}},
Pages = {{229-250}},
Abstract = {{We describe the space structuring model, a model of language
   comprehension inspired by ideas in cognitive linguistics, focusing on
   its capacity to explain the sorts of inferences needed to understand
   one-line jokes. One process posited in the model is frame-shifting, the
   semantic and pragmatic reanalysis in which elements of the existing
   message-level representation are mapped into a new frame retrieved from
   long-term memory. To test this model, we recorded participants' eye
   movements with a headband-mounted eye-tracker while they read sentences
   that ended either as a joke, or as nonfunny controls ({''}She read so
   much about the bad effects of smoking she decided to give up the
   reading/habit.{''}). Only jokes required frame-shifting; nonjoke endings
   were consistent with the contextually evoked frame. Though initial gaze
   durations were the same for jokes and non-jokes, total viewing duration
   was longer for the jokes and participants were more likely to make
   regressive (leftward) eye movements after reading the ``punch word{''}
   of a joke. Results are consistent with the psychological reality of some
   process like frame-shifting, suggesting readers literally revisit
   aspects of the prior context while apprehending one-line jokes.}},
Publisher = {{MOUTON DE GRUYTER}},
Address = {{GENTHINER STRASSE 13, 10785 BERLIN, GERMANY}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Coulson, S (Reprint Author), Univ Calif San Diego, La Jolla, CA 92093 USA.
   Univ Calif San Diego, La Jolla, CA 92093 USA.}},
DOI = {{10.1515/HUMOR.2006.013}},
ISSN = {{0933-1719}},
Keywords = {{cognitive semantics; eye-tracking; frame-shifting; joke comprehension}},
Keywords-Plus = {{MEMORY}},
Research-Areas = {{Linguistics; Psychology}},
Web-of-Science-Categories  = {{Language \& Linguistics; Psychology, Multidisciplinary}},
Author-Email = {{coulson@cogsci.ucsd.edu}},
ORCID-Numbers = {{Coulson, Seana/0000-0003-1246-9394
   Urbach, Thomas/0000-0001-7993-142X}},
Number-of-Cited-References = {{29}},
Times-Cited = {{34}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{12}},
Journal-ISO = {{Humor-Int. J. Humor Res.}},
Doc-Delivery-Number = {{087LA}},
Unique-ID = {{ISI:000240742700002}},
DA = {{2019-10-28}},
}

@article{ ISI:000231783800012,
Author = {Falkmer, T and Gregersen, NP},
Title = {{A comparison of eye movement behavior of inexperienced and experienced
   drivers in real traffic environments}},
Journal = {{OPTOMETRY AND VISION SCIENCE}},
Year = {{2005}},
Volume = {{82}},
Number = {{8}},
Pages = {{732-739}},
Month = {{AUG}},
Abstract = {{Purpose. The importance of the visual system as the input channel for
   sensory information necessary when driving is often stated. There are
   several reports on differences in visual search strategies between
   experienced and inexperienced drivers, as well as in relation to the
   roadway. However, the results are ambiguous and are not sampled by
   similar procedures. Based on previous findings, the aim of the present
   study was to gain further knowledge on these differences by testing the
   hypotheses that inexperienced drivers, in comparison to experienced
   drivers, fixate closer to the vehicle, fixate more often on in-vehicle
   objects, spread their fixations less along the horizontal meridian,
   fixate more often on relevant traffic cues, and fixate more often on
   objects classified as potential hazards. Methods. Data from eye-tracker
   recordings of visual search strategies of the driver in real-world
   traffic were used for the analyses. Results. The results confirmed all
   stated hypotheses regarding differences between inexperienced and
   experienced drivers, with the exception of fixations closer to the
   vehicle, in which ambiguous results were found. Conclusions. The present
   study provides normative data for the understanding of the development
   of visual search strategies among drivers. The methodology used in the
   present study, i.e., to combine a quantitative analysis with a
   qualitative analysis proved, to be useful to compare visual search
   strategies among inexperienced and experienced drivers.}},
Publisher = {{LIPPINCOTT WILLIAMS \& WILKINS}},
Address = {{530 WALNUT ST, PHILADELPHIA, PA 19106-3621 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Falkmer, T (Reprint Author), Linkoping Univ, Fac Hlth Sci, Dept Neurosci \& Locomot, INR, SE-58185 Linkoping, Sweden.
   Linkoping Univ, Fac Hlth Sci, Dept Neurosci \& Locomot, INR, SE-58185 Linkoping, Sweden.
   VTI, Swedish Natl Rd \& Transport Res Inst, Linkoping, Sweden.
   Linkoping Univ, Dept Hlth \& Environm, Div Prevent \& Social Med \& Publ Hlth Sci, Linkoping, Sweden.}},
DOI = {{10.1097/01.opx.0000175560.45715.5b}},
ISSN = {{1040-5488}},
Keywords = {{visual search strategies; driver education; eye movements; fixations;
   driver experience}},
Keywords-Plus = {{VISUAL-SEARCH; OLDER DRIVERS; INFORMATION; SITUATIONS}},
Research-Areas = {{Ophthalmology}},
Web-of-Science-Categories  = {{Ophthalmology}},
Author-Email = {{torbjorn.falkmer@inr.liu.se}},
Number-of-Cited-References = {{25}},
Times-Cited = {{70}},
Usage-Count-Last-180-days = {{1}},
Usage-Count-Since-2013 = {{23}},
Journal-ISO = {{Optom. Vis. Sci.}},
Doc-Delivery-Number = {{963DP}},
Unique-ID = {{ISI:000231783800012}},
DA = {{2019-10-28}},
}

@article{ ISI:000181971400006,
Author = {Jansen, AR and Blackwell, AF and Marriott, K},
Title = {{A tool for tracking visual attention: The Restricted Focus Viewer}},
Journal = {{BEHAVIOR RESEARCH METHODS INSTRUMENTS \& COMPUTERS}},
Year = {{2003}},
Volume = {{35}},
Number = {{1}},
Pages = {{57-69}},
Month = {{FEB}},
Abstract = {{Eye-tracking equipment has proven useful in examining the cognitive
   processes people use when understanding and reasoning with visual
   stimuli. However, eye-tracking has several drawbacks: accurate
   eye-tracking equipment is expensive, it is often awkward for
   participants, it requires frequent recalibration, and the data can be
   difficult to interpret. We introduce an alternative tool: the Restricted
   Focus Viewer (RFV). This is a computer program that takes an image,
   blurs it, and displays it on a computer monitor, allowing the
   participant to see only a small region of the image in focus at any
   time. The region in focus can be moved using the computer mouse. The RFV
   records what the participant is focusing on at any point in time. It is
   cheap, nonintrusive, does not require calibration, and provides accurate
   data about which region is being focused on. We describe this tool and
   also provide experimental comparisons with eye-tracking. The RFV
   (Version 2.1) is freely available at http://www.csse.monash.edu.
   au/projects/RFV/.}},
Publisher = {{PSYCHONOMIC SOC INC}},
Address = {{1710 FORTVIEW RD, AUSTIN, TX 78704 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Jansen, AR (Reprint Author), Monash Univ, Sch Elect Engn \& Comp Sci, Clayton, Vic 3800, Australia.
   Monash Univ, Sch Elect Engn \& Comp Sci, Clayton, Vic 3800, Australia.
   Univ Cambridge, Cambridge CB2 1TN, England.}},
DOI = {{10.3758/BF03195497}},
ISSN = {{0743-3808}},
Keywords-Plus = {{EYE-MOVEMENTS}},
Research-Areas = {{Psychology}},
Web-of-Science-Categories  = {{Psychology, Mathematical; Psychology, Experimental}},
Number-of-Cited-References = {{20}},
Times-Cited = {{27}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{2}},
Journal-ISO = {{Behav. Res. Methods Instr. Comput.}},
Doc-Delivery-Number = {{662WE}},
Unique-ID = {{ISI:000181971400006}},
OA = {{Bronze}},
DA = {{2019-10-28}},
}

@article{ ISI:000180503000013,
Author = {Patla, AE and Vickers, JN},
Title = {{How far ahead do we look when required to step on specific locations in
   the travel path during locomotion?}},
Journal = {{EXPERIMENTAL BRAIN RESEARCH}},
Year = {{2003}},
Volume = {{148}},
Number = {{1}},
Pages = {{133-138}},
Month = {{JAN}},
Abstract = {{Spatial-temporal gaze behaviour patterns were analysed as normal
   participants wearing a mobile eye tracker were required to step on 17
   footprints, regularly or irregularly spaced over a 10-m distance, placed
   in their travel path. We examined the characteristics of two types of
   gaze fixation with respect to the participants' stepping patterns:
   footprint fixation; and travel fixation when the gaze is stable and
   travelling at the speed of whole body. The results showed that travel
   gaze fixation is a dominant gaze behaviour occupying over 50\% of the
   travel time. It is hypothesised that this gaze behaviour would
   facilitate acquisition of environmental and self-motion information from
   the optic flow that is generated during locomotion: this in turn would
   guide movements of the lower limbs to the appropriate landing targets.
   When participants did fixate on the landing target they did so on
   average two steps ahead, about 800-1,000 ms before the limb is placed on
   the target area. This would allow them sufficient time to successfully
   modify their gait patterns. None of the gaze behaviours was influenced
   by the placement (regularly versus irregularly spaced) of the footprints
   or repeated exposures to the travel path. Rather visual information
   acquired during each trial was used ``denovo{''} to modulate gait
   patterns. This study provides a clear temporal link between gaze and
   stepping pattern and adds to our understanding of how vision is used to
   regulate locomotion.}},
Publisher = {{SPRINGER-VERLAG}},
Address = {{175 FIFTH AVE, NEW YORK, NY 10010 USA}},
Type = {{Article}},
Language = {{English}},
Affiliation = {{Patla, AE (Reprint Author), Univ Waterloo, Gait \& Posture Lab, Dept Kinesiol, Waterloo, ON N2L 3G1, Canada.
   Univ Waterloo, Gait \& Posture Lab, Dept Kinesiol, Waterloo, ON N2L 3G1, Canada.
   Univ Calgary, Fac Kinesiol, Calgary, AB, Canada.}},
DOI = {{10.1007/s00221-002-1246-y}},
ISSN = {{0014-4819}},
Keywords = {{gaze behaviour; human locomotion; feedforward control; cluttered terrain}},
Keywords-Plus = {{VISION; HEAD}},
Research-Areas = {{Neurosciences \& Neurology}},
Web-of-Science-Categories  = {{Neurosciences}},
Number-of-Cited-References = {{20}},
Times-Cited = {{177}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{29}},
Journal-ISO = {{Exp. Brain Res.}},
Doc-Delivery-Number = {{637GJ}},
Unique-ID = {{ISI:000180503000013}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000181051900023,
Author = {Romero, P and Cox, R and du Boulay, B and Lutz, R},
Editor = {{Hegarty, M and Meyer, B and Narayanan, NH}},
Title = {{Visual attention and representation switching during Java program
   debugging: A study using the Restricted Focus Viewer}},
Booktitle = {{DIAGRAMMATIC REPRESENTATION AND INFERENCE}},
Series = {{LECTURE NOTES IN ARTIFICIAL INTELLIGENCE}},
Year = {{2002}},
Volume = {{2317}},
Pages = {{221-235}},
Note = {{2nd International Conference on Theory and Application of Diagrams,
   CALLAWAY GARDENS, GEORGIA, APR 18-20, 2002}},
Organization = {{USN, Off Res; Amer Assoc Aritficial Intelligence; Cognit Sci Soc;
   Japanese Cognit Sci Soc; Japanese Soc Artificial Intelligence}},
Abstract = {{Java program debugging was investigated in programmers who used a
   software debugging environment (SDE) that provided concurrently
   displayed, adjacent, multiple and linked representations consisting of
   the program code, a functional visualisation of the program, and its
   output.
   A modified version of the Restricted Focus Viewer (RFV){[}3] - a visual
   attention tracking system - was employed to measure the degree to which
   each of the representations was used, and to record switches between
   representations. Other measures included debugging performance (number
   of bugs identified, the order in which they were identified, bug
   discovery latencies, etc.).
   The aim of this investigation was to address questions such as `To what
   extent do programmers use each type of representation?' and `Are
   particular patterns of representational use associated with superior
   debugging performance?'.
   A within-subject design, and comparison of performance under (matched)
   RFV/no-RFV task conditions, allowed the use of the RFV as an
   attention-tracking tool to be validated in the programming domain. The
   results also provide tentative evidence that superior debugging using
   multiple-representation SDE's tends to be associated with a) the
   predominant use of the program code representation, and b) frequent
   switches between the code representation and the visualisation of the
   program execution.}},
Publisher = {{SPRINGER-VERLAG BERLIN}},
Address = {{HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY}},
Type = {{Article; Proceedings Paper}},
Language = {{English}},
Affiliation = {{Romero, P (Reprint Author), Univ Sussex, Sch Cognit \& Comp Sci, Human Centred Technol Grp, Brighton BN1 9QH, E Sussex, England.
   Univ Sussex, Sch Cognit \& Comp Sci, Human Centred Technol Grp, Brighton BN1 9QH, E Sussex, England.}},
ISSN = {{0302-9743}},
ISBN = {{3-540-43561-1}},
Keywords-Plus = {{MENTAL REPRESENTATIONS; COMPREHENSION; PROLOG; ALGORITHMS}},
Research-Areas = {{Computer Science}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence}},
Number-of-Cited-References = {{27}},
Times-Cited = {{4}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Doc-Delivery-Number = {{BW16H}},
Unique-ID = {{ISI:000181051900023}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000178108400040,
Author = {Romero, P and Lutz, R and Cox, R and du Boulay, B},
Book-Group-Author = {{IEEE COMPUTER SOCIETY
   IEEE COMPUTER SOCIETY}},
Title = {{Co-ordination of multiple external representations during Java program
   debugging}},
Booktitle = {{IEEE 2002 SYMPOSIA ON HUMAN CENTRIC COMPUTING LANGUAGES AND
   ENVIRONMENTS, PROCEEDINGS}},
Year = {{2002}},
Pages = {{207-214}},
Note = {{IEEE Symposia on Human Centric Computing Languages and Environments (HCC
   02), ARLINGTON, VA, SEP 03-06, 2002}},
Organization = {{IEEE Comp Soc, Tech Comm Multimedia Comp}},
Abstract = {{Java program debugging was investigated in computer science students who
   used a software debugging environment (SDE) that provided concurrently
   displayed, adjacent, multiple and linked representations consisting of
   the program code, a visualisation of the program, and its output.
   The aim of this investigation was to address questions such as `To what
   extent do programmers use each type of representation ?', `Are
   particular patterns of representation use associated with superior
   debugging performance ?', `Are graphical representations more helpful to
   Java programmers than textual ones ?' and `Are representations that
   highlight data structure more useful than those that highlight con trol
   -flow for Java debugging ?'
   A modified version of the Restricted Focus Viewer (RFV) - a visual
   attention tracking system - was employed to measure the degree to which
   each of the representations was used, and to record switches between
   representations. The experimental results are in agreement with research
   in the area that suggests that control-flow information is difficult to
   decode in an Object-Oriented language like Java. These results also
   suggest that graphical representations might be more useful than textual
   ones when the degree of difficulty of the debugging task poses a
   challenge to programmers. Additionally, the results link programming
   experience to switching behaviour suggesting that although switches
   between the code and the visualisation are the most common ones,
   programming experience might promote a more balanced switching behaviour
   between the main representation, the code, and the secondary ones.}},
Publisher = {{IEEE COMPUTER SOC}},
Address = {{10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Romero, P (Reprint Author), Univ Sussex, Human Centred Technol Grp, Sch Cognit \& Comp Sci, Brighton BN1 9QH, E Sussex, England.
   Univ Sussex, Human Centred Technol Grp, Sch Cognit \& Comp Sci, Brighton BN1 9QH, E Sussex, England.}},
DOI = {{10.1109/HCC.2002.1046373}},
ISBN = {{0-7695-1644-0}},
Keywords-Plus = {{COMPREHENSION; PROLOG}},
Research-Areas = {{Computer Science}},
Web-of-Science-Categories  = {{Computer Science, Software Engineering}},
Number-of-Cited-References = {{21}},
Times-Cited = {{3}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Doc-Delivery-Number = {{BV19K}},
Unique-ID = {{ISI:000178108400040}},
OA = {{Green Accepted}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000167102500002,
Author = {Snorrason, M and Hoffman, J and Ruda, H},
Editor = {{Watkins, WR and Clement, D and Reynolds, WR}},
Title = {{Analysis and modeling of fixation point selection for visual search in
   cluttered backgrounds}},
Booktitle = {{TARGETS AND BACKGROUNDS VI: CHARACTERIZATION, VISUALIZATION, AND THE
   DETECTION PROCESS}},
Series = {{PROCEEDINGS OF THE SOCIETY OF PHOTO-OPTICAL INSTRUMENTATION ENGINEERS
   (SPIE)}},
Year = {{2000}},
Volume = {{4029}},
Pages = {{10-20}},
Note = {{Conference on Targets and Backgrounds VI: Characterization,
   Visualization, and the Detection Process, ORLANDO, FL, APR 24-26, 2000}},
Organization = {{SPIE}},
Abstract = {{Hard-to-see targets are generally only detected by human observers once
   they have been fixated. Hence, understanding how the human visual system
   allocates fixation locations is necessary for predicting target
   detectability. Visual search experiments were conducted where observers
   searched for military vehicles in cluttered terrain. Instantaneous eye
   position measurements were collected using an eye tracker. The resulting
   data was partitioned into fixations and saccades, and analyzed for
   correlation with various image properties. The fixation data was used to
   validate our model for predicting fixation locations. This model
   generates a saliency map from bottom-up image features, such as local
   contrast. To account for top-down scene understanding effects, a
   separate cognitive bias map is generated. The combination of these two
   maps provides a fixation probability map, from which sequences of
   fixation points were generated.}},
Publisher = {{SPIE-INT SOC OPTICAL ENGINEERING}},
Address = {{1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Snorrason, M (Reprint Author), Charles River Analyt Inc, Cambridge, MA 02138 USA.
   Charles River Analyt Inc, Cambridge, MA 02138 USA.}},
DOI = {{10.1117/12.392532}},
ISSN = {{0277-786X}},
ISBN = {{0-8194-3655-0}},
Keywords = {{visual search; target detection; fixation; clutter; saliency; eye
   tracking}},
Keywords-Plus = {{ATTENTION}},
Research-Areas = {{Optics; Imaging Science \& Photographic Technology}},
Web-of-Science-Categories  = {{Optics; Imaging Science \& Photographic Technology}},
ResearcherID-Numbers = {{Ruda, Harry/G-5696-2017}},
Number-of-Cited-References = {{10}},
Times-Cited = {{0}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{1}},
Doc-Delivery-Number = {{BR65Y}},
Unique-ID = {{ISI:000167102500002}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000174950500011,
Author = {Blackwell, AF and Jansen, AR and Marriott, K},
Editor = {{Anderson, M and Cheng, P and Haarslev, V}},
Title = {{Restricted focus viewer: A tool for tracking visual attention}},
Booktitle = {{THEORY AND APPLICATION OF DIAGRAMS, PROCEEDINGS}},
Series = {{LECTURE NOTES IN ARTIFICIAL INTELLIGENCE}},
Year = {{2000}},
Volume = {{1889}},
Pages = {{162-177}},
Note = {{1st International Conference on Theory and Application of Diagrams
   (Diagrams 2000), UNIV EDINBURGH, EDINBURGH, SCOTLAND, SEP 01-03, 2000}},
Abstract = {{Eye-tracking equipment has proven useful in examining the cognitive
   processes people use when understanding and reasoning with diagrams.
   However, eye-tracking has several drawbacks: accurate eye-tracking
   equipment is expensive, often awkward for participants, requires
   frequent re-calibration and the data can be difficult to interpret. We
   introduce an alternative tool for diagram research: the Restricted Focus
   Viewer (RFV). This is a computer program which takes an image, blurs it
   and displays it on a computer monitor, allowing the participant to see
   only a small region of the image in focus at any time. The region in
   focus can be moved using the computer mouse. The RFV records what the
   participant is focusing on at any point in time. It is cheap,
   non-intrusive, does not require calibration and provides accurate data
   about which region is being focused upon. We describe this tool, and
   also provide an experimental comparison with eye-tracking. We show that
   the RFV gives similar results to those obtained by Hegarty (1992) when
   using eye-tracking equipment to investigate reasoning about mechanical
   diagrams.}},
Publisher = {{SPRINGER-VERLAG BERLIN}},
Address = {{HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY}},
Type = {{Article; Proceedings Paper}},
Language = {{English}},
Affiliation = {{Blackwell, AF (Reprint Author), Univ Cambridge, Comp Lab, Pembroke St, Cambridge CB2 3QG, England.
   Univ Cambridge, Comp Lab, Cambridge CB2 3QG, England.
   Monash Univ, Sch Comp Sci \& Software Engn, Clayton, Vic 3800, Australia.}},
ISSN = {{0302-9743}},
ISBN = {{3-540-67915-4}},
Keywords-Plus = {{EYE-MOVEMENTS}},
Research-Areas = {{Computer Science}},
Web-of-Science-Categories  = {{Computer Science, Artificial Intelligence; Computer Science, Theory \&
   Methods}},
Number-of-Cited-References = {{16}},
Times-Cited = {{13}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{0}},
Doc-Delivery-Number = {{BU08P}},
Unique-ID = {{ISI:000174950500011}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000171438400163,
Author = {Seagull, FJ and Xiao, Y and MacKenzie, CF and Jaberi, M and Dutton, RP},
Book-Group-Author = {{HFES
   HFES}},
Title = {{Monitoring behavior: A pilot study using an ambulatory eye-tracker in
   surgical operating rooms}},
Booktitle = {{PROCEEDINGS OF THE HUMAN FACTORS AND ERGONOMICS SOCIETY 43RD ANNUAL
   MEETING, VOLS 1 AND 2}},
Series = {{HUMAN FACTORS AND ERGONOMICS SOCIETY ANNUAL MEETING PROCEEDINGS}},
Year = {{1999}},
Pages = {{850-854}},
Note = {{43rd Annual Meeting of the Human-Factors-and-Ergonomics-Society,
   HOUSTON, TX, SEP 27-OCT 01, 1999}},
Organization = {{Human Factors \& Ergon Soc}},
Abstract = {{Information provision is an important function of most technology
   interventions. Understanding human operators' patterns in consulting
   various information sources could provide insights into how displays
   should be designed. We tested the utility of an ambulatory eye-tracker
   in an operational setting for collecting data of monitoring behavior.
   Temporal patterns of eye-tracking data, coupled with the data on
   observed activities and events led to a model of monitoring that treats
   knowledge about the monitored process as a resource. In a
   semi-self-paced work environment, the operator could deploy the strategy
   of building up knowledge about the monitored process before anticipated
   high workload periods and reestablishing such knowledge immediately
   after high workload periods to replenish lost resources. Through
   measures of monitoring frequency and time series analysis, supportive
   evidence for the resource model was obtained from the eye-tracking data
   of the pilot study. The study successfully demonstrated the utility of
   an ambulatory eye-tracking device in a real, dynamic environment.}},
Publisher = {{HUMAN FACTORS AND ERGONOMICS SOC}},
Address = {{PO BOX 1369, SANTA MONICA, CA 90406-1369 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Seagull, FJ (Reprint Author), Univ Illinois, Champaign, IL 61820 USA.
   Univ Illinois, Champaign, IL 61820 USA.}},
ISSN = {{1071-1813}},
ISBN = {{0-945289-12-X}},
Research-Areas = {{Computer Science; Engineering; Telecommunications; Transportation}},
Web-of-Science-Categories  = {{Computer Science, Cybernetics; Ergonomics; Telecommunications;
   Transportation Science \& Technology}},
Number-of-Cited-References = {{11}},
Times-Cited = {{2}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{1}},
Doc-Delivery-Number = {{BS93U}},
Unique-ID = {{ISI:000171438400163}},
DA = {{2019-10-28}},
}

@inproceedings{ ISI:000075850500020,
Author = {Piccione, D and Ferrett, D},
Editor = {{Verly, JG}},
Title = {{``Driving Miss Bradley{''}: Performance measurement to support thermal
   driving}},
Booktitle = {{ENHANCED AND SYNTHETIC VISION 1998}},
Series = {{PROCEEDINGS OF THE SOCIETY OF PHOTO-OPTICAL INSTRUMENTATION ENGINEERS
   (SPIE)}},
Year = {{1998}},
Volume = {{3364}},
Pages = {{200-207}},
Note = {{Conference on Enhanced and Synthetic Vision, ORLANDO, FL, APR 13-14,
   1998}},
Organization = {{SPIE}},
Abstract = {{The Driver's Vision Enhancer (DVE) program is providing a system to
   enlarge the driving envelope for the community of military wheeled and
   tracked vehicles. The DVE, an IR device, provides the driver with images
   of the forward scene under night and adverse day conditions. During the
   DVE development program, several questions emerged requiring
   performance-based data to resolve. A comprehensive program to provide
   the Project Manager, Night Vision/Reconnaissance, Surveillance and
   Target Acquisition with driver performance data that will aid in the
   decision-making process is described in this payer. The program involves
   several linked efforts including: the relative merits of the DVE and
   night vision goggles (NVG); drivers' ability to detect the presence of
   drop-offs when using the DVE and NVG; the effect on performance of
   various levers of nonuniformity and nonresponsiveness in the
   display/sensor system; the analysis of drivers' vision using an
   eye-tracker in a vehicle; and the evaluation of candidate symbology to
   enhance the DVE's utility in the M2 Bradley. The data collected will aid
   in making decisions on how to write a system specification to reduce
   cost without sacrificing driver performance, gain an understanding of
   how drivers use the DVE in operational settings, and determine where
   training is needed to enhance safety and reduce risk an the battlefield.}},
Publisher = {{SPIE-INT SOC OPTICAL ENGINEERING}},
Address = {{1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA}},
Type = {{Proceedings Paper}},
Language = {{English}},
Affiliation = {{Piccione, D (Reprint Author), DCS Corp, 1330 Braddock Pl, Alexandria, VA 22314 USA.
   DCS Corp, Alexandria, VA 22314 USA.}},
DOI = {{10.1117/12.317471}},
ISSN = {{0277-786X}},
ISBN = {{0-8194-2813-2}},
Keywords = {{Driver's Vision Enhancer; night vision devices; driver performance;
   safety; thermal imagery}},
Research-Areas = {{Automation \& Control Systems; Engineering; Computer Science; Optics;
   Transportation}},
Web-of-Science-Categories  = {{Automation \& Control Systems; Engineering, Aerospace; Computer Science,
   Artificial Intelligence; Computer Science, Cybernetics; Optics;
   Transportation}},
Number-of-Cited-References = {{0}},
Times-Cited = {{1}},
Usage-Count-Last-180-days = {{0}},
Usage-Count-Since-2013 = {{2}},
Doc-Delivery-Number = {{BL55E}},
Unique-ID = {{ISI:000075850500020}},
DA = {{2019-10-28}},
}
